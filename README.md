<div align="center">
  <h1>ğŸ—£ï¸ å¤§èªè¨€æ¨¡å‹èª²ç¨‹å¿ƒå¾—</h1>
  <p align="center">
    ğŸ¦ <a href="https://twitter.com/maximelabonne">Follow me on X</a> â€¢ 
    ğŸ¤— <a href="https://huggingface.co/mlabonne">Hugging Face</a> â€¢ 
    ğŸ’» <a href="https://mlabonne.github.io/blog">Blog</a> â€¢ 
    ğŸ“™ <a href="https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python">Hands-on GNN</a> â€¢ 
    ğŸ—£ï¸ <a href="https://chat.openai.com/g/g-yviLuLqvI-llm-course">Interactive GPT</a>
  </p>
</div>
<br/>

æ­¤ LLM èª²ç¨‹å¿ƒå¾—åˆ†æˆä¸‰å€‹éƒ¨åˆ†:

1. ğŸ§© **LLM åŸºç¤** æ¶µè“‹äº†æ•¸å­¸,Python å’Œç¥ç¶“ç¶²è·¯ç­‰åŸºç¤çŸ¥è­˜.
2. ğŸ§‘â€ğŸ”¬ **LLM æ¨¡å‹å·¥ç¨‹** å°ˆæ³¨åœ¨ä½¿ç”¨æœ€æ–°çš„æŠ€å·§,æŠ€è¡“æ­å»ºç¾éšæ®µå€‹äººå¯å¯¦ç¾æœ€å¥½çš„LLMs.
3. ğŸ‘· **LLM æ‡‰ç”¨å·¥ç¨‹** å°ˆæ³¨åœ¨å‰µå»º LLM é©…å‹•çš„æ‡‰ç”¨ä»¥åŠéƒ¨ç½².

## ğŸ“ Notebooks

èˆ‡å¤§å‹èªè¨€æ¨¡å‹ç›¸é—œçš„ colab notebook å’Œæ–‡ç« æ¸…å–®ã€‚

### Tools

| åç¨± | æ•˜è¿° | é€£çµ |
|----------|-------------|----------|
| ğŸ§ [LLM AutoEval](https://github.com/mlabonne/llm-autoeval) | ä½¿ç”¨ RunPod è‡ªå‹•è©•ä¼°æ‚¨çš„LLMs | <a href="https://colab.research.google.com/drive/1Igs3WZuXAIv9X0vwqiE90QlEPys8e8Oa?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |
| ğŸ¥± LazyMergekit | ä½¿ç”¨ mergekit ä¸€éµè¼•é¬†åˆä½µæ¨¡å‹. | <a href="https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |
| âš¡ AutoGGUF | ä¸€éµé‡åŒ– GGUF æ ¼å¼çš„ LLM. | <a href="https://colab.research.google.com/drive/1P646NEg33BZy4BfLDNpTz0V0lwIU3CHu?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |
| ğŸŒ³ Model Family Tree | å¯è¦–åŒ–åˆä½µæ¨¡å‹çš„æ¨¹ç‹€çµæ§‹åœ–. | <a href="https://colab.research.google.com/drive/1s2eQlolcI1VGgDhqWIANfkfKvcKrMyNr?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |

### Fine-tuning (å¾®èª¿)

| åç¨± | æ•˜è¿° | æ–‡ç«  | é€£çµ |
|---------------------------------------|-------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|
| Fine-tune Llama 2 in Google Colab | å¾®èª¿æ‚¨çš„ç¬¬ä¸€å€‹ Llama 2 æ¨¡å‹çš„é€æ­¥æŒ‡å—ã€‚ | [Article](https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html) | <a href="https://colab.research.google.com/drive/1PEQyJO1-f6j0S_XJ8DV50NkpzasXkrzd?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |
| Fine-tune LLMs with Axolotl | æœ€å…ˆé€²å¾®èª¿å·¥å…·çš„ç«¯åˆ°ç«¯æŒ‡å—ã€‚| [Article](https://mlabonne.github.io/blog/posts/A_Beginners_Guide_to_LLM_Finetuning.html) | <a href="https://colab.research.google.com/drive/1Xu0BrCB7IShwSWKVcfAfhehwjDrDMH5m?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |
| Fine-tune Mistral-7b with DPO | ä½¿ç”¨ DPO æå‡ç›£ç£å¾®èª¿æ¨¡å‹çš„æ€§èƒ½ | [Article](https://medium.com/towards-data-science/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac) | <a href="https://colab.research.google.com/drive/15iFBr1xWgztXvhrj5I9fBv20c7CFOPBE?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |

### Quantization(é‡åŒ–)

| åç¨± | æ•˜è¿° | æ–‡ç«  | é€£çµ |
|---------------------------------------|-------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|
| 1. Introduction to Quantization | ä½¿ç”¨ 8 ä½å…ƒé‡åŒ–çš„å¤§å‹èªè¨€æ¨¡å‹æœ€ä½³åŒ–ã€‚ | [Article](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html) | <a href="https://colab.research.google.com/drive/1DPr4mUQ92Cc-xf4GgAaB6dFcFnWIvqYi?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |
| 2. 4-bit Quantization using GPTQ | é‡åŒ–æ‚¨è‡ªå·±çš„é–‹æº LLM ä»¥åœ¨æ¶ˆè²»æ€§ç¡¬é«”ä¸Šé‹è¡Œå®ƒå€‘ã€‚ | [Article](https://mlabonne.github.io/blog/4bit_quantization/) | <a href="https://colab.research.google.com/drive/1lSvVDaRgqQp_mWK_jC9gydz6_-y6Aq4A?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |
| 3. Quantization with GGUF and llama.cpp | ä½¿ç”¨ llama.cpp é‡åŒ– Llama 2 æ¨¡å‹ä¸¦å°‡ GGUF ç‰ˆæœ¬ä¸Šå‚³åˆ° HF Hubã€‚ | [Article](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html) | <a href="https://colab.research.google.com/drive/1pL8k7m04mgE5jo2NrjGi8atB0j_37aDD?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |
| 4. ExLlamaV2: The Fastest Library to RunÂ LLMs | é‡åŒ–ä¸¦åŸ·è¡Œ EXL2 æ¨¡å‹ä¸¦å°‡å…¶ä¸Šå‚³è‡³ HF Hubã€‚| [Article](https://mlabonne.github.io/blog/posts/ExLlamaV2_The_Fastest_Library_to_Run%C2%A0LLMs.html) | <a href="https://colab.research.google.com/drive/1yrq4XBlxiA0fALtMoT2dwiACVc77PHou?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |

### å…¶ä»–

| åç¨± | æ•˜è¿° | æ–‡ç«  | é€£çµ |
|---------------------------------------|-------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|
| Decoding Strategies in Large Language Models | å¾æ³¢æŸæœå°‹(beam search)åˆ°æ ¸æ¡æ¨£(nucleus sampling)çš„æ–‡æœ¬ç”ŸæˆæŒ‡å—| [Article](https://mlabonne.github.io/blog/posts/2022-06-07-Decoding_strategies.html) | <a href="https://colab.research.google.com/drive/19CJlOS5lI29g-B3dziNn93Enez1yiHk2?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |
| Visualizing GPT-2's Loss Landscape | åŸºæ–¼æ¬Šé‡æ“¾å‹•çš„æå¤±æ™¯è§€ä¸‰ç¶­åœ–(3D plot of the loss landscape based on weight perturbations.)| [Tweet](https://twitter.com/maximelabonne/status/1667618081844219904) | <a href="https://colab.research.google.com/drive/1Fu1jikJzFxnSPzR_V2JJyDVWWJNXssaL?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |
| Improve ChatGPT with Knowledge Graphs | ç”¨çŸ¥è­˜åœ–è­œå¢å¼· ChatGPT çš„ç­”æ¡ˆ | [Article](https://mlabonne.github.io/blog/posts/Article_Improve_ChatGPT_with_Knowledge_Graphs.html) | <a href="https://colab.research.google.com/drive/1mwhOSw9Y9bgEaIFKT4CLi0n18pXRM4cj?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |
| Merge LLMs with mergekit | è¼•é¬†å‰µå»ºæ‚¨è‡ªå·±çš„æ¨¡å‹ï¼Œç„¡éœ€ GPUï¼| [Article](https://towardsdatascience.com/merge-large-language-models-with-mergekit-2118fb392b54) | <a href="https://colab.research.google.com/drive/1_JS7JKJAQozD48-LhYdegcuuZ2ddgXfr?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |


## ğŸ§© LLM åŸºç¤

![](img/roadmap_fundamentals.png)

### 1. æ©Ÿå™¨å­¸ç¿’æ•¸å­¸åŸºç¤

ä¸€èˆ¬ä¾†èªªå…¥é–€çš„è©±å…¶å¯¦åªè¦çœ‹å‹•æ‰‹æ·±åº¦å­¸ç¿’ä¸­é å‚™çŸ¥è­˜å…§ç« äº†è§£é‹ä½œä¸¦èƒ½ä½¿ç”¨pytorch ,tensorflow æˆ– numpy å¯¦ç¾åŸºæœ¬çš„åŠŸèƒ½å°±å¯ä»¥äº†ã€‚

æ¨è–¦é–±è®€ï¼š
- [å‹•æ‰‹æ·±åº¦å­¸ç¿’-ä¸­çš„é å‚™çŸ¥è­˜é‚£ç« ](https://zh-v2.d2l.ai/d2l-zh-pytorch.pdf)
- [å‹•æ‰‹æ·±åº¦å­¸ç¿’é€™æœ¬æ›¸ä¹Ÿèƒ½è®“ä¸€èˆ¬äººå¤§è‡´ä¸Šäº†è§£æ·±åº¦å­¸ç¿’çš„é‹ä½œ](http://zh.gluon.ai/chapter_introduction/deep-learning-intro.html)
- [Blog- æ·±åº¦å­¸ç¿’(Deep Learning)-æ•¸å­¸æ•´ç†](https://dysonma.github.io/2021/01/27/%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-Deep-Learning-%E6%95%B8%E5%AD%B8%E6%95%B4%E7%90%86/)
- [Blog- æ©Ÿå™¨/æ·±åº¦å­¸ç¿’-åŸºç¤æ•¸å­¸ç¯‡(ä¸€)(å…§å®¹è·Ÿä¸Šé¢é›·åŒ)](https://chih-sheng-huang821.medium.com/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E5%9F%BA%E7%A4%8E%E6%95%B8%E5%AD%B8%E7%AF%87-%E4%B8%80-1c8337179ad6)
- [æˆ‘æ˜¯å¦‚ä½•å­¸ç¿’æ·±åº¦å­¸ç¿’ä¸­çš„æ•¸å­¸çš„ï¼Ÿ(å¯åƒè€ƒæ–¹æ³•)](https://yanwei-liu.medium.com/%E6%88%91%E6%98%AF%E5%A6%82%E4%BD%95%E5%AD%B8%E7%BF%92%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92%E4%B8%AD%E7%9A%84%E6%95%B8%E5%AD%B8%E7%9A%84-a26eee623638)

- [æ·±åº¦å­¸ç¿’ç¶“å…¸(èŠ±æ›¸)](https://github.com/ytin16/awesome-machine-learning-1/blob/master/Deep-Learning%E8%8A%B1%E4%B9%A6-%E3%80%8A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B-%E4%B8%AD%E6%96%87%E7%89%88.pdf)

- [æ·±åº¦å­¸ç¿’ç¶“å…¸(èŠ±æ›¸)ä¸­çš„æ•¸å­¸æ¨å°](https://github.com/MingchaoZhu/DeepLearning)


é›»å­æ›¸ï¼š

- [æ·±åº¦å­¸ç¿’ä¸­çš„æ•¸å­¸](https://github.com/jash-git/Jash-good-idea-20200304-001/blob/master/CN%20AI%20book/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6.pdf)

- [æ·±åº¦å­¸ç¿’åŸºç¤ä»¥åŠæ•¸å­¸åŸç†](https://github.com/HaoMood/File/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E5%8F%8A%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86.pdf)

åœ¨æŒæ¡æ©Ÿå™¨å­¸ç¿’ä¹‹å‰ï¼Œäº†è§£æ”¯æ’äº†é€™äº›æ¼”ç®—æ³•çš„åŸºæœ¬æ•¸å­¸æ¦‚å¿µéå¸¸é‡è¦ã€‚

- **ç·šæ€§ä»£æ•¸**: é€™å°æ–¼ç†è§£è¨±å¤šæ¼”ç®—æ³•è‡³é—œé‡è¦ï¼Œå°¤å…¶æ˜¯æ·±åº¦å­¸ç¿’ä¸­ä½¿ç”¨çš„æ¼”ç®—æ³•ã€‚é—œéµæ¦‚å¿µåŒ…æ‹¬å‘é‡ã€çŸ©é™£ã€è¡Œåˆ—å¼ã€ç‰¹å¾µå€¼å’Œç‰¹å¾µå‘é‡ã€å‘é‡ç©ºé–“å’Œç·šæ€§è®Šæ›ã€‚
- **å¾®ç©åˆ†**: è¨±å¤šæ©Ÿå™¨å­¸ç¿’æ¼”ç®—æ³•æ¶‰åŠé€£çºŒå‡½æ•¸çš„æœ€ä½³åŒ–ï¼Œé€™éœ€è¦äº†è§£å°æ•¸ã€ç©åˆ†ã€æ¥µé™å’Œç´šæ•¸ã€‚å¦å¤–å¤šè®Šé‡å¾®ç©åˆ†å’Œæ¢¯åº¦çš„æ¦‚å¿µä¹Ÿå¾ˆé‡è¦ã€‚
- **æ©Ÿç‡å’Œçµ±è¨ˆ**: é€™äº›å°æ–¼ç†è§£æ¨¡å‹å¦‚ä½•å¾æ•¸æ“šä¸­å­¸ç¿’ä¸¦åšå‡ºé æ¸¬è‡³é—œé‡è¦ã€‚ é—œéµæ¦‚å¿µåŒ…æ‹¬æ©Ÿç‡è«–ã€éš¨æ©Ÿè®Šæ•¸ã€æ©Ÿç‡åˆ†ä½ˆã€æœŸæœ›ã€è®Šç•°æ•¸ã€å”æ–¹å·®ã€ç›¸é—œæ€§ã€å‡è¨­æª¢å®šã€ä¿¡è³´å€é–“ã€æœ€å¤§ä¼¼ç„¶ä¼°è¨ˆå’Œè²è‘‰æ–¯æ¨ç†ã€‚

ğŸ“š åƒè€ƒè³‡æº:
- [ç¹ä¸­çš„æ·±åº¦å­¸ç¿’ä¸­çš„æ•¸å­¸ç›¸é—œè³‡æ–™](https://hackmd.io/@changken/rkukooSGS#%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E5%8F%8A%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92%E7%9A%84%E8%B3%87%E6%96%99)
- [ç”¨ Python å­¸å¾®ç©åˆ†](https://ryancheunggit.gitbooks.io/calculus-with-python/content/01Functions.html)
- [æ©Ÿå™¨å­¸ç¿’çš„æ•¸å­¸åŸºç¤ï¼šçŸ©é™£ç¯‡](http://www.hahack.com/math/math-matrix/)
- [æ©Ÿå™¨å­¸ç¿’çš„æ•¸å­¸åŸºç¤ï¼šå‘é‡ç¯‡](http://www.hahack.com/math/math-vector/)
- [æ©Ÿå™¨å­¸ç¿’çš„æ•¸å­¸åŸºç¤ï¼šç·šæ€§ä»£æ•¸é€²éšç¯‡](http://www.hahack.com/math/math-linear-algebra-graded/)
- [Python for Probability, Statistics, and Machine Learning](https://github.com/unpingco/Python-for-Probability-Statistics-and-Machine-Learning)
- [Think Bayes](https://greenteapress.com/wp/think-bayes/)
- [çµ±è¨ˆåˆ†ä½ˆ æ–¹é–‹æ³°æ•™æˆ ç‹å…ƒæ•™æˆ](http://item.jd.com/12019664.html)
- [æ¦‚ç‡è«–èˆ‡æ•¸ç†çµ±è¨ˆ é™³å¸Œå­ºæ•™æˆ](https://www.amazon.cn/dp/B073LBYPZ4/ref=sr_1_1?ie=UTF8&qid=1546071311&sr=8-1&keywords=é™ˆå¸Œå„’)
- [Linear Regression in Python with Scikit-Learn](https://stackabuse.com/linear-regression-in-python-with-scikit-learn/)
- [ç·šæ€§ä»£æ•¸è‡ªå­¸èª²ç¨‹ï¼Œåœ‹å…§å¤–å­¸ç¿’è³‡æº](https://selflearningsuccess.com/linear-algebra-courses/#Mathematics_for_Machine_Learning_Linear_Algebra): æœ¬æ–‡å½™æ•´åœ‹å…§å¤–ç·šæ€§ä»£æ•¸è‡ªå­¸èª²ç¨‹ï¼Œæä¾›çµ¦è¦åŠƒå­¸ç¿’ç·šæ€§ä»£æ•¸çš„æœ‹å‹å€‘åƒè€ƒã€‚
- [3Blue1Brown - ç·šæ€§ä»£æ•¸çš„æœ¬è³ª](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab): æ­¤ç³»åˆ—çš„å½±ç‰‡ä»‹ç´¹å¹¾ä½•ç›¸é—œçš„æ¦‚å¿µ
- [StatQuest with Josh Starmer - çµ±è¨ˆåŸºç¤çŸ¥è­˜](https://www.youtube.com/watch?v=qBigTkBLU6g&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9): ç‚ºè¨±å¤šçµ±è¨ˆæ¦‚å¿µæä¾›ç°¡å–®æ˜äº†çš„è§£é‡‹ã€‚
- [Aerinå¥³å£«çš„APçµ±è¨ˆç›´è§€ç†è§£](https://automata88.medium.com/list/cacc224d5e7d): æä¾›æ¯å€‹æ©Ÿç‡åˆ†ä½ˆèƒŒå¾Œçš„Mediumæ–‡ç« æ¸…å–®ã€‚
- [æ²‰æµ¸å¼ç·šæ€§ä»£æ•¸](https://immersivemath.com/ila/learnmore.html): ç·šæ€§ä»£æ•¸çš„å¦ä¸€ç¨®åœ–åƒåŒ–è©®é‡‹.
- [Khan Academy - ç·šæ€§ä»£æ•¸](https://www.khanacademy.org/math/linear-algebra): éå¸¸é©åˆåˆå­¸è€…ï¼Œå› ç‚ºå®ƒä»¥éå¸¸ç›´è§€çš„æ–¹å¼è§£é‡‹äº†æ¦‚å¿µã€‚
- [Khan Academy - å¾®ç©åˆ†](https://www.khanacademy.org/math/calculus-1): ä¸€é–€æ¶µè“‹å¾®ç©åˆ†æ‰€æœ‰åŸºç¤çŸ¥è­˜çš„äº’å‹•èª²ç¨‹ã€‚
- [Khan Academy - æ©Ÿç‡èˆ‡çµ±è¨ˆ](https://www.khanacademy.org/math/statistics-probability): ä»¥æ˜“æ–¼ç†è§£çš„æ ¼å¼æä¾›ææ–™ã€‚
---

### 2. ç”¨æ–¼æ©Ÿå™¨å­¸ç¿’çš„Python

Python æ˜¯ä¸€ç¨®å¼·å¤§è€Œéˆæ´»çš„ç¨‹å¼èªè¨€ï¼Œç”±æ–¼å…¶å¯è®€æ€§ã€ä¸€è‡´æ€§å’Œå¼·å¤§çš„è³‡æ–™ç§‘å­¸åº«ç”Ÿæ…‹ç³»çµ±ï¼Œç‰¹åˆ¥é©åˆæ©Ÿå™¨å­¸ç¿’ã€‚


- **PythonåŸºç¤**: Pythonç¨‹å¼è¨­è¨ˆéœ€è¦å¾ˆå¥½åœ°ç†è§£åŸºæœ¬èªæ³•ã€è³‡æ–™é¡å‹ã€éŒ¯èª¤è™•ç†å’Œç‰©ä»¶å°å‘ç¨‹å¼è¨­è¨ˆã€‚
    -  æ¨è–¦é–±è®€,æ‡‰ç”¨-[Into Python in one video](https://www.giraffeacademy.com/programming-languages/python/in-one-video/)
- **è³‡æ–™ç§‘å­¸å‡½å¼åº«**: åŒ…æ‹¬ç†Ÿæ‚‰ç”¨æ–¼æ•¸å€¼é‹ç®—çš„ NumPyã€ç”¨æ–¼è³‡æ–™æ“ä½œå’Œåˆ†æçš„ Pandasã€ç”¨æ–¼è³‡æ–™è¦–è¦ºåŒ–çš„ Matplotlib å’Œ Seabornã€‚
    -  æ¨è–¦é–±è®€,æ‡‰ç”¨-[Visualization Curriculum](https://uwdata.github.io/visualization-curriculum/intro.html)
    -  æ¨è–¦é–±è®€,æ‡‰ç”¨-[Pythonè³‡æ–™ç§‘å­¸åˆ†äº«ï¼3.è³‡æ–™è¦–è¦ºåŒ–(1)](https://asyncfor.com/jupyter/python/data%20science/2020/05/22/data-viz-1.html)
    -  æ¨è–¦é–±è®€,æ‡‰ç”¨-[Pythonè³‡æ–™ç§‘å­¸åˆ†äº«ï¼3.è³‡æ–™è¦–è¦ºåŒ–(2)](https://asyncfor.com/jupyter/python/data%20science/2020/05/29/data-viz-2.html)
- **è³‡æ–™é è™•ç†**: é€™æ¶‰åŠç‰¹å¾µç¸®æ”¾å’Œæ¨™æº–åŒ–ã€è™•ç†ç¼ºå¤±è³‡æ–™ã€ç•°å¸¸å€¼æª¢æ¸¬ã€åˆ†é¡è³‡æ–™ç·¨ç¢¼ä»¥åŠå°‡è³‡æ–™æ‹†åˆ†ç‚ºè¨“ç·´é›†ã€é©—è­‰é›†å’Œæ¸¬è©¦é›†ã€‚
    -  æ¨è–¦é–±è®€,æ‡‰ç”¨- æ¦‚è¦½[[è³‡æ–™åˆ†æ&æ©Ÿå™¨å­¸ç¿’] ç¬¬2.4è¬›ï¼šè³‡æ–™å‰è™•ç†(Missing data, One-hot encoding, Feature Scaling)](https://medium.com/jameslearningnote/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%AC%AC2-4%E8%AC%9B-%E8%B3%87%E6%96%99%E5%89%8D%E8%99%95%E7%90%86-missing-data-one-hot-encoding-feature-scaling-3b70a7839b4a)
    -  æ¨è–¦é–±è®€,æ‡‰ç”¨-[Pythonè³‡æ–™ç§‘å­¸åˆ†äº«â€”2.è³‡æ–™è™•ç†](https://asyncfor.com/jupyter/python/data%20science/2020/05/15/data-etl.html)
    -  æ¨è–¦é–±è®€,æ‡‰ç”¨-[[æ©Ÿå™¨å­¸ç¿’ç­†è¨˜]æ•¸æ“šé è™•ç†](https://doremi31618.medium.com/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E7%AD%86%E8%A8%98-%E6%95%B8%E6%93%9A%E9%A0%90%E8%99%95%E7%90%8601-ae90853978da)
- **æ©Ÿå™¨å­¸ç¿’å‡½å¼åº«**: ç†Ÿç·´ä½¿ç”¨ Scikit-learnï¼ˆä¸€å€‹æä¾›å¤šç¨®ç›£ç£å’Œéç›£ç£å­¸ç¿’æ¼”ç®—æ³•çš„å‡½å¼åº«ï¼‰è‡³é—œé‡è¦ã€‚äº†è§£å¦‚ä½•å¯¦ç¾ç·šæ€§è¿´æ­¸ã€é‚è¼¯è¿´æ­¸ã€æ±ºç­–æ¨¹ã€éš¨æ©Ÿæ£®æ—ã€k æœ€è¿‘é„° (K-NN) å’Œ K å‡å€¼èšé¡ç­‰æ¼”ç®—æ³•éå¸¸é‡è¦ã€‚PCA å’Œ t-SNE ç­‰é™ç¶­æŠ€è¡“ä¹Ÿæœ‰åŠ©æ–¼è¦–è¦ºåŒ–é«˜ç¶­åº¦è³‡æ–™ã€‚
    -  æ¨è–¦é–±è®€,æ‡‰ç”¨(è£¡é¢æœ‰ipynb)-[Introducing Scikit-Learn](https://doremi31618.medium.com/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E7%AD%86%E8%A8%98-%E6%95%B8%E6%93%9A%E9%A0%90%E8%99%95%E7%90%8601-ae90853978da)
    - æ¨è–¦é–±è®€,æ‡‰ç”¨(è£¡é¢æœ‰ipynb)-[Python è³‡æ–™ç§‘å­¸æ‰‹å†Š(è£¡é¢æœ‰ipynb ,colab)](https://jakevdp.github.io/PythonDataScienceHandbook/):
      å…è²»çš„æ•¸ä½æ›¸ç±ï¼Œæ˜¯å­¸ç¿’ pandasã€NumPyã€Matplotlib å’Œ Seaborn çš„çµ•ä½³è³‡æºã€‚    

ğŸ“š è³‡æºï¼š

- [Real Python](https://realpython.com/): ç¶œåˆè³‡æºï¼ŒåŒ…å«åˆå­¸è€…å’Œé€²éš Python æ¦‚å¿µçš„æ–‡ç« å’Œæ•™å­¸ã€‚
- [freeCodeCamp - å­¸ç¿’ Python](https://www.youtube.com/watch?v=rfscVS0vtbw): é•·å½±ç‰‡ï¼Œå®Œæ•´ä»‹ç´¹äº† Python ä¸­çš„æ‰€æœ‰æ ¸å¿ƒæ¦‚å¿µã€‚
- [Python è³‡æ–™ç§‘å­¸æ‰‹å†Š](https://jakevdp.github.io/PythonDataScienceHandbook/): å…è²»çš„æ•¸ä½æ›¸ç±ï¼Œæ˜¯å­¸ç¿’ pandasã€NumPyã€Matplotlib å’Œ Seaborn çš„çµ•ä½³è³‡æºã€‚
- [freeCodeCamp - é©åˆæ‰€æœ‰äººçš„æ©Ÿå™¨å­¸ç¿’](https://youtu.be/i_LwzRVP7bg): ç‚ºåˆå­¸è€…ä»‹ç´¹ä¸åŒçš„æ©Ÿå™¨å­¸ç¿’æ¼”ç®—æ³•ã€‚
- [Udacity - æ©Ÿå™¨å­¸ç¿’ç°¡ä»‹](https://www.udacity.com/course/intro-to-machine-learning--ud120): å…è²»èª²ç¨‹ï¼Œæ¶µè“‹ PCA å’Œå…¶ä»–å¹¾å€‹æ©Ÿå™¨å­¸ç¿’æ¦‚å¿µã€‚

---

### 3. ç¥ç¶“ç¶²çµ¡è·Ÿæ·±åº¦å­¸ç¿’

- **åŸºç¤çŸ¥è­˜**: é€™åŒ…æ‹¬ç†è§£ç¥ç¶“ç¶²è·¯çš„çµæ§‹ï¼Œä¾‹å¦‚å±¤ã€æ¬Šé‡ã€åå·®å’Œæ¿€æ´»å‡½æ•¸ï¼ˆsigmoidã€tanhã€ReLU ç­‰ï¼‰
    - [3Blue1Brown - ä»€éº¼æ˜¯ç¥ç¶“ç¶²è·¯ï¼Ÿ](https://www.youtube.com/watch?v=aircAruvnKk): è©²å½±ç‰‡ç›´è§€åœ°è§£é‡‹äº†ç¥ç¶“ç¶²è·¯åŠå…¶å…§éƒ¨é‹ä½œåŸç†ã€‚
- **æ·±åº¦å­¸ç¿’æ¡†æ¶**: ç›®å‰æ˜¯åœ¨æ·±åº¦å­¸ç¿’æ¡†æ¶æ–¹é¢é‚„æ˜¯ Pytorch æœ€ç†±é–€ï¼Œä½†æ˜¯æœ‰äº›è€æ‡‰ç”¨è·ŸæŸäº› Google ç›¸é—œçš„æ‡‰ç”¨ä»é‚„æ˜¯ä½¿ç”¨ Tensorflow ã€‚å‡å¦‚è¦å…¥é–€çš„è©±å»ºè­°ä¸‹é¢å››å€‹é€£çµæ‰¾ä¸€å€‹å…¥é–€ä¸¦å¯¦ä½œä¸€å€‹æ‡‰ç”¨å³å¯ã€‚
    - [freeCodeCamp - æ·±åº¦å­¸ç¿’é€Ÿæˆèª²ç¨‹](https://www.youtube.com/watch?v=VyWAvY2CF9c): æ­¤å½±ç‰‡ç°¡æ½”åœ°ä»‹ç´¹äº†æ·±åº¦å­¸ç¿’ä¸­æ‰€æœ‰æœ€é‡è¦çš„æ¦‚å¿µã€‚
    - [å‹•æ‰‹æ·±åº¦å­¸ç¿’å®˜ç¶²](https://zh.d2l.ai/chapter_linear-networks/index.html)
    - [å‹•æ‰‹æ·±åº¦å­¸ç¿’åŒ…å« tensorflow,pytorch ç¨‹å¼ç¢¼çš„æ•™å­¸ï¼Œä¸éè¦è‡ªå·±debug](https://zh-v2.d2l.ai/d2l-zh.zip)
    - [pytorchå®˜ç¶²æ•™å­¸](https://pytorch.org/tutorials/beginner/basics/intro.html):å»ºè­°å­¸ç¿’è·¯ç·š: Introduction to PyTorch ->Image and Video ,Audio ,Text æŒ‰éœ€å­¸ç¿’ï¼Œåªå­¸éœ€è¦çš„å°±å¥½(å»ºè­°åªå…ˆé¸ä¸€å€‹)ã€‚
    - [tensorflow å®˜ç¶²æ•™å­¸](https://www.tensorflow.org/tutorials?hl=zh-tw): Begginner -> Adanced(ä¹Ÿæ˜¯å»ºè­°æŒ‰éœ€å­¸ç¿’)
- **è¨“ç·´èˆ‡æœ€ä½³åŒ–**: ç†Ÿæ‚‰åå‘å‚³æ’­å’Œä¸åŒé¡å‹çš„æå¤±å‡½æ•¸ï¼Œä¾‹å¦‚å‡æ–¹èª¤å·® (MSE) å’Œäº¤å‰ç†µã€‚äº†è§£å„ç¨®æœ€ä½³åŒ–æ¼”ç®—æ³•ï¼Œä¾‹å¦‚æ¢¯åº¦ä¸‹é™ã€éš¨æ©Ÿæ¢¯åº¦ä¸‹é™ã€RMSprop å’Œ Adamã€‚
ç¥ç¶“ç¶²è·¯æ˜¯è¨±å¤šæ©Ÿå™¨å­¸ç¿’æ¨¡å‹çš„åŸºæœ¬çµ„æˆéƒ¨åˆ†ï¼Œç‰¹åˆ¥æ˜¯åœ¨æ·±åº¦å­¸ç¿’é ˜åŸŸã€‚ç‚ºäº†æœ‰æ•ˆåœ°åˆ©ç”¨å®ƒå€‘ï¼Œå…¨é¢äº†è§£å®ƒå€‘çš„è¨­è¨ˆå’Œæ©Ÿåˆ¶è‡³é—œé‡è¦ã€‚
- **éåº¦æ“¬åˆ**: äº†è§£éåº¦æ“¬åˆçš„æ¦‚å¿µï¼ˆæ¨¡å‹åœ¨è¨“ç·´è³‡æ–™ä¸Šè¡¨ç¾è‰¯å¥½ï¼Œä½†åœ¨æœªè¦‹éçš„è³‡æ–™ä¸Šè¡¨ç¾ä¸ä½³ï¼‰ä¸¦å­¸ç¿’å„ç¨®æ­£å‰‡åŒ–æŠ€è¡“ï¼ˆdropoutã€L1/L2 æ­£å‰‡åŒ–ã€æå‰åœæ­¢ã€è³‡æ–™å¢å¼·ï¼‰ä¾†é˜²æ­¢éåº¦æ“¬åˆã€‚
- **å¯¦ä½œå¤šå±¤æ„ŸçŸ¥å™¨ (MLP)**: ä½¿ç”¨ PyTorch å»ºæ§‹ MLPï¼Œä¹Ÿç¨±ç‚ºå…¨é€£æ¥ç¶²è·¯ã€‚
  
ğŸ“š å…¶ä»–è³‡æº:
- [Patrick Loeber - PyTorch æ•™å­¸](https://www.youtube.com/playlist?list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4): ç‚ºåˆå­¸è€…å­¸ç¿’ PyTorch çš„ç³»åˆ—å½±ç‰‡ã€‚

---

### 4. è‡ªç„¶èªè¨€è™•ç†(NLP)

NLP æ˜¯äººå·¥æ™ºæ…§çš„ä¸€å€‹ä»¤äººè‘—è¿·çš„åˆ†æ”¯ï¼Œå®ƒå½Œåˆäº†äººé¡èªè¨€å’Œæ©Ÿå™¨ç†è§£ä¹‹é–“çš„å·®è·ã€‚å¾ç°¡å–®çš„æ–‡å­—è™•ç†åˆ°ç†è§£èªè¨€çš„ç´°å¾®å·®åˆ¥ï¼ŒNLP åœ¨ç¿»è­¯ã€æƒ…ç·’åˆ†æã€èŠå¤©æ©Ÿå™¨äººç­‰è¨±å¤šæ‡‰ç”¨ä¸­ç™¼æ®è‘—è‡³é—œé‡è¦çš„ä½œç”¨ã€‚

- **æ–‡å­—é è™•ç†**: å­¸ç¿’å„ç¨®æ–‡å­—é è™•ç†æ­¥é©Ÿï¼Œä¾‹å¦‚åˆ†è©ï¼ˆå°‡æ–‡å­—åˆ†å‰²æˆå–®å­—æˆ–å¥å­ï¼‰ã€è©å¹¹æ“·å–ï¼ˆå°‡å–®å­—é‚„åŸç‚ºå…¶è©æ ¹å½¢å¼ï¼‰ã€è©å½¢é‚„åŸï¼ˆèˆ‡è©å¹¹æ“·å–é¡ä¼¼ï¼Œä½†è€ƒæ…®ä¸Šä¸‹æ–‡ï¼‰ã€åœç”¨è©åˆªé™¤ç­‰ã€‚
- **ç‰¹å¾µæå–æŠ€è¡“**: ç†Ÿæ‚‰å°‡æ–‡å­—è³‡æ–™è½‰æ›ç‚ºæ©Ÿå™¨å­¸ç¿’æ¼”ç®—æ³•å¯ä»¥ç†è§£çš„æ ¼å¼çš„æŠ€è¡“ã€‚ä¸»è¦æ–¹æ³•åŒ…æ‹¬è©è¢‹ (BoW)ã€è©é »-é€†æ–‡æª”é »ç‡ (TF-IDF) å’Œ n-gramã€‚
- **è©åµŒå…¥**: è©åµŒå…¥æ˜¯ä¸€ç¨®è©è¡¨ç¤ºå½¢å¼ï¼Œå…è¨±å…·æœ‰ç›¸ä¼¼æ„ç¾©çš„è©å…·æœ‰ç›¸ä¼¼çš„è¡¨ç¤ºå½¢å¼ã€‚ä¸»è¦æ–¹æ³•åŒ…æ‹¬ Word2Vecã€GloVe å’Œ FastTextã€‚
    - [Jay Alammar - The Illustration Word2Vec](https://jalammar.github.io/illustrated-word2vec/):äº†è§£è‘—å Word2Vec æ¶æ§‹çš„ä¸€å€‹å¥½ææ–™ã€‚
- **éæ­¸ç¥ç¶“ç¶²è·¯ (RNN)**: äº†è§£ RNN çš„å·¥ä½œåŸç†ï¼ŒRNN æ˜¯ä¸€ç¨®è¨­è¨ˆç”¨æ–¼è™•ç†åºåˆ—è³‡æ–™çš„ç¥ç¶“ç¶²è·¯ã€‚æ¢ç´¢ LSTM å’Œ GRUï¼Œé€™å…©ç¨®èƒ½å¤ å­¸ç¿’é•·æœŸä¾è³´é—œä¿‚çš„ RNN è®Šé«”ã€‚
    - [Jake Tae - PyTorch RNN from Scratch](https://jaketae.github.io/study/pytorch-rnn/): åœ¨ PyTorch ä¸­å¯¦ç”¨ä¸”ç°¡å–®åœ°å¯¦ä½œ RNNã€LSTM å’Œ GRU æ¨¡å‹ã€‚
    - [colah's blog - Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/): ä¸€ç¯‡æ›´ç†è«–æ€§çš„ LSTM ç¶²è·¯æ–‡ç« ã€‚
- **åŸºæ–¼ Transformer è·Ÿé è¨“ç·´æ¨¡å‹çš„ NLP**: ç”±æ–¼åŸºæ–¼é¡ Transformer çš„æ¨¡å‹èƒ½è™•ç†ä¹‹å‰å…¶ä»– ç¶“å…¸ NLP æ¨¡å‹è™•ç†çš„ä»»å‹™ï¼Œä¸¦ä¸”å¤§éƒ¨åˆ†ä»»å‹™èƒ½åšå¾—æ›´å¥½ï¼Œæ‰€ä»¥æˆ‘è¦ºå¾—é€™æ˜¯å¿…å­¸çš„ä¸€éƒ¨åˆ†ã€‚
    - [NLP course from huggingface](https://huggingface.co/learn/nlp-course/chapter1/1)

ğŸ“š Resources:
- [RealPython - NLP with spaCy in Python](https://realpython.com/natural-language-processing-spacy-python/): æœ‰é—œ Python ä¸­ç”¨æ–¼ NLP ä»»å‹™çš„ spaCy å‡½å¼åº«çš„è©³ç´°æŒ‡å—ã€‚
- [Kaggle - NLP Guide](https://www.kaggle.com/learn-guide/natural-language-processing):ä¸€äº› notebooks å’Œè³‡æºï¼Œç”¨æ–¼ Python ä¸­ NLP çš„å¯¦è¸è§£é‡‹ã€‚ 


## ğŸ§‘â€ğŸ”¬ LLM æ¨¡å‹å·¥ç¨‹

æœ¬èª²ç¨‹çš„é€™ä¸€éƒ¨åˆ†é‡é»åœ¨æ–¼å­¸ç¿’å¦‚ä½•ä½¿ç”¨æœ€æ–°æŠ€è¡“ä¾†å»ºç«‹æœ€å¥½çš„ LLMsã€‚

![](img/roadmap_scientist.png)

### 1. The LLM æ¶æ§‹

é›–ç„¶ä¸éœ€è¦æ·±å…¥äº†è§£ Transformer æ¶æ§‹ï¼Œä½†æ·±å…¥äº†è§£å…¶è¼¸å…¥ï¼ˆtokens ä»¤ç‰Œï¼‰å’Œè¼¸å‡ºï¼ˆlogitsï¼‰éå¸¸é‡è¦ã€‚æ™®é€šçš„æ³¨æ„åŠ›æ©Ÿåˆ¶æ˜¯å¦ä¸€å€‹éœ€è¦æŒæ¡çš„é—œéµçµ„æˆéƒ¨åˆ†ï¼Œç¨å¾Œæœƒä»‹ç´¹å®ƒçš„æ”¹é€²ç‰ˆæœ¬ã€‚

* **é«˜éšè¦–åœ– High-level view**: é‡æ–°å¯©è¦–ç·¨ç¢¼å™¨-è§£ç¢¼å™¨ Transformer æ¶æ§‹ï¼Œæ›´å…·é«”åœ°èªªï¼Œæ˜¯åƒ…è§£ç¢¼å™¨çš„ GPT æ¶æ§‹ï¼Œè©²æ¶æ§‹åœ¨æ¯å€‹æœ€è¿‘çš„ LLM ä¸­åŸºæœ¬éƒ½æœ‰ä½¿ç”¨ã€‚

  * [LLM Foundations](https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/llm-foundations/): å½±ç‰‡æ­é…æ•™æçš„è¬›è§£ã€‚

  * [Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY): å½±ç‰‡æ­é…æ•™æçš„è¬›è§£ã€‚

  * [LLM-from-scratch.ipynb](https://colab.research.google.com/gist/iamaziz/171170dce60d9cd07fab221507fd1d52): å¯¦éš›ç°¡å–®ç‰ˆçš„ç¨‹å¼ç¢¼ã€‚

  * [[1hr Talk] Intro to Large Language Models](https://youtu.be/zjkBMFhNj_g?si=VnNOE1gggtAhxTDn): å½±ç‰‡æ­é…æ•™æçš„è¬›è§£ã€‚


* **æ¨™è¨˜åŒ– Tokenization**: äº†è§£å¦‚ä½•å°‡åŸå§‹æ–‡å­—è³‡æ–™è½‰æ›ç‚ºæ¨¡å‹å¯ä»¥ç†è§£çš„æ ¼å¼ï¼Œé€™æ¶‰åŠå°‡æ–‡å­—æ‹†åˆ†ç‚ºæ¨™è¨˜ï¼ˆé€šå¸¸æ˜¯å–®å­—æˆ–å­å–®å­—ï¼‰ã€‚
    * [Let's build the GPT Tokenizer](https://youtu.be/zjkBMFhNj_g?si=VnNOE1gggtAhxTDn): å½±ç‰‡æ­é…æ•™æçš„è¬›è§£ã€‚

* **æ³¨æ„åŠ›æ©Ÿåˆ¶**: æŒæ¡æ³¨æ„åŠ›æ©Ÿåˆ¶èƒŒå¾Œçš„ç†è«–ï¼ŒåŒ…æ‹¬è‡ªè¨»æ„åŠ›å’Œç¸®æ”¾é»ç©æ³¨æ„åŠ›ï¼Œé€™ä½¿å¾—æ¨¡å‹åœ¨ç”¢ç”Ÿè¼¸å‡ºæ™‚èƒ½å¤ å°ˆæ³¨æ–¼è¼¸å…¥çš„ä¸åŒéƒ¨åˆ†ã€‚
    * [å‹•æ‰‹æ·±åº¦å­¸ç¿’-æ³¨æ„åŠ›æ©Ÿåˆ¶](https://zh.d2l.ai/chapter_attention-mechanisms/index.html) ( å…¶å¯¦ä¸Šé¢çš„å½±ç‰‡æœ‰è¦†è“‹åˆ° )
* **æ–‡å­—ç”Ÿæˆ**: äº†è§£æ¨¡å‹ç”¢ç”Ÿè¼¸å‡ºåºåˆ—çš„ä¸åŒæ–¹å¼ã€‚å¸¸è¦‹çš„ç­–ç•¥åŒ…æ‹¬è²ªå©ªè§£ç¢¼(greedy decoding)ã€æ³¢æŸæœå°‹(beam searc)ã€top-k æ¡æ¨£(top-k sampling)å’Œæ ¸æ¡æ¨£(nucleus sampling)ã€‚
    * [å¦‚ä½•ç”Ÿæˆæ–‡æœ¬: é€šè¿‡ Transformers ç”¨ä¸åŒçš„è§£ç æ–¹æ³•ç”Ÿæˆæ–‡æœ¬](https://huggingface.co/blog/zh/how-to-generate)

- [HuggingFaceçš„ Transformeræ•™å­¸](https://huggingface.co/docs/transformers/quicktour):å»ºè­°ä»¥å»ºç«‹æ‡‰ç”¨ç‚ºç›®æ¨™ï¼Œå¾å°å…¥,å¾®èª¿,éƒ¨ç½²æ•´å€‹æµç¨‹è·‘ä¸€éã€‚
- å²ä¸¹ä½›çš„Transformersèª²ï¼Œå¾æ¶æ§‹åˆ°æ‡‰ç”¨éƒ½æœ‰ [CS25: Transformers United V3](https://web.stanford.edu/class/cs25/)
- Youtube Transformers United ä¸Šèª²éŒ„å½±[Stanford CS25: V1 I Transformers United: DL Models that have revolutionized NLP, CV, RL](https://www.youtube.com/watch?v=P127jhj-8-Y&list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM)

ğŸ“š **åƒè€ƒè³‡æ–™**:
- [Building LLMs from Scratch](https://youtu.be/UU1WVnMk4E8?si=Vn1IbHE5p5LUQmKi) å¾é›¶é–‹å§‹ build LLMsã€‚
- [Transformer æ’åœ–](https://jalammar.github.io/illustrated-transformer/) by Jay Alammar: Transformer æ¨¡å‹çš„ç›´è§€è§£é‡‹ã€‚
- [GPT-2åœ–è§£](https://jalammar.github.io/illustrated-gpt2/) by Jay Alammar: æ­¤æ–‡æ¯”ä¸Šä¸€ç¯‡æ–‡ç« æƒ³å°æ›´é‡è¦äº›ï¼Œå®ƒå°ˆæ³¨æ–¼å’Œ Llama éå¸¸ç›¸ä¼¼çš„ GPT æ¶æ§‹ã€‚
- [LLM Visualization](https://bbycroft.net/llm) by Brendan Bycroft: ä»¥ 3D è¦–è¦ºåŒ–æ–¹å¼å‘ˆç¾ LLM å…§éƒ¨ç™¼ç”Ÿçš„æƒ…æ³ã€‚
* [nanoGPT](https://www.youtube.com/watch?v=kCc8FmEb1nY) by Andrej Karpathy:ä¸€æ®µ 2 å°æ™‚é•·çš„ YouTube å½±ç‰‡ï¼Œç”¨æ–¼å¾é ­é–‹å§‹é‡æ–°å¯¦ç¾ GPTï¼ˆä»¥ç¨‹å¼è¨­è¨ˆå¸«çš„è¦–è§’ï¼‰ã€‚
* [Attention? Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/) by Lilian Weng: ä»¥æ›´æ­£å¼çš„æ–¹å¼ä»‹ç´¹æ³¨æ„åŠ›çš„å¿…è¦æ€§ã€‚
* [Decoding Strategies in LLMs](https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html): æä¾›å°ç”Ÿæˆæ–‡æœ¬çš„ä¸åŒè§£ç¢¼ç­–ç•¥çš„åœ–åƒåŒ–ä»‹ç´¹ä»¥åŠç¨‹å¼ç¢¼ã€‚

---

### 2. æ§‹å»ºä¸€å€‹ç¯„ä¾‹(æˆ–æŒ‡ä»¤)è³‡æ–™é›† instruction dataset

é›–ç„¶å¾ç¶­åŸºç™¾ç§‘å’Œå…¶ä»–ç¶²ç«™æ‰¾åˆ°åŸå§‹è³‡æ–™å¾ˆå®¹æ˜“ï¼Œä½†åœ¨å¾ˆå¤šçš„ç’°å¢ƒä¸­æ”¶é›†æˆå°çš„æŒ‡ç¤º,ç¯„ä¾‹å’Œç­”æ¡ˆå»å¾ˆå›°é›£ã€‚èˆ‡å‚³çµ±æ©Ÿå™¨å­¸ç¿’ä¸€æ¨£ï¼Œè³‡æ–™é›†çš„å“è³ªå°‡ç›´æ¥å½±éŸ¿æ¨¡å‹çš„å“è³ªï¼Œé€™å°±æ˜¯ç‚ºä»€éº¼å®ƒå¯èƒ½æ˜¯å¾®èª¿éç¨‹ä¸­æœ€é‡è¦çš„çµ„æˆéƒ¨åˆ†ã€‚

* **[é¡ä¼¼ Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html)-çš„è³‡æ–™é›†**: ä½¿ç”¨ OpenAI API (GPT) å¾é ­é–‹å§‹ç”¢ç”Ÿåˆæˆè³‡æ–™ã€‚æ‚¨å¯ä»¥æŒ‡å®šç¨®å­å’Œç³»çµ±æç¤ºä¾†å»ºç«‹å¤šæ¨£åŒ–çš„è³‡æ–™é›†ã€‚
    * ç°¡å–®ä»»å‹™çš„ç¯„ä¾‹1: [How To Create Datasets for Finetuning From Multiple Sources! Improving Finetunes With Embeddings.](https://youtu.be/fYyZiRi6yNE?si=mYgoyAeCMkkKQUr1)   
    * ç°¡å–®ä»»å‹™çš„ç¯„ä¾‹2: [How I created an instruction dataset using GPT 3.5 to fine-tune Llama 2 for news classification](https://medium.com/@kshitiz.sahay26/how-i-created-an-instruction-dataset-using-gpt-3-5-to-fine-tune-llama-2-for-news-classification-ed02fe41c81f)
* **é€²éšæŠ€å·§**: äº†è§£å¦‚ä½•ä½¿ç”¨[Evol-Instruct](https://arxiv.org/abs/2304.12244)æ”¹é€²ç¾æœ‰è³‡æ–™é›†ï¼Œå¦‚ä½•ç”¢ç”Ÿå’Œ[Orca](https://arxiv.org/abs/2306.02707) å’Œ [phi-1](https://arxiv.org/abs/2306.11644) è«–æ–‡ä¸­é¡ä¼¼çš„é«˜å“è³ªåˆæˆè³‡æ–™.
    * [Evol-Instructä¸­æ–‡æ¦‚ç•¥çš„è§£é‡‹](https://zhuanlan.zhihu.com/p/668755469)
* **è³‡æ–™éæ¿¾**: æ¶‰åŠæ­£è¦è¡¨ç¤ºå¼ã€åˆªé™¤è¿‘ä¼¼é‡è¤‡é …ã€é—œæ³¨å…·æœ‰å¤§é‡æ¨™è¨˜çš„ç­”æ¡ˆç­‰çš„å‚³çµ±æŠ€å·§.
    * ç›¸é—œå…§å®¹å¦‚åƒè€ƒè³‡æ–™
* **æç¤ºè©æ¨¡æ¿**: ç›®å‰é‚„æ²’æœ‰çœŸæ­£çš„æ¨™æº–æ–¹æ³•ä¾†æ ¼å¼ä¾†æ¨™æº–åŒ–èªªæ˜ç¯„æœ¬å’Œç­”æ¡ˆï¼Œé€™å°±æ˜¯ç‚ºä»€éº¼äº†è§£ä¸åŒçš„èŠå¤©ç¯„æœ¬å¾ˆé‡è¦, åƒæ˜¯ [ChatML](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/chatgpt?tabs=python&pivots=programming-language-chat-ml), [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) ç­‰.
    * ç›¸é—œå…§å®¹å¦‚åƒè€ƒè³‡æ–™

ğŸ“š **åƒè€ƒè³‡æ–™**:
* [Preparing a Dataset for Instruction tuning](https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-1-Preparing-a-Dataset-for-Instruction-Tuning--Vmlldzo1NTcxNzE2) by Thomas Capelle: Alpaca å’Œ Alpaca-GPT4 è³‡æ–™é›†çš„æ¢ç´¢ä»¥åŠå¦‚ä½•æ¨™æº–åŒ–è³‡æ–™.
* [Generating a Clinical Instruction Dataset](https://medium.com/mlearning-ai/generating-a-clinical-instruction-dataset-in-portuguese-with-langchain-and-gpt-4-6ee9abfa41ae) by Solano Todeschini: æœ‰é—œå¦‚ä½•ä½¿ç”¨ GPT-4 å»ºç«‹ç¶œåˆæŒ‡å°è³‡æ–™é›†çš„æ•™å­¸. 
* [GPT 3.5 for news classification](https://medium.com/@kshitiz.sahay26/how-i-created-an-instruction-dataset-using-gpt-3-5-to-fine-tune-llama-2-for-news-classification-ed02fe41c81f) by Kshitiz Sahay: ä½¿ç”¨ GPT 3.5 å»ºç«‹ç¯„ä¾‹è³‡æ–™é›†ä¾†å¾®èª¿ Llama 2 çš„æ–°èåˆ†é¡.
* [Dataset creation for fine-tuning LLM](https://colab.research.google.com/drive/1GH8PW9-zAe4cXEZyOIE-T9uHXblIldAg?usp=sharing): åŒ…å«ä¸€äº›éæ¿¾è³‡æ–™é›†å’Œä¸Šå‚³çµæœæŠ€è¡“çš„ Notebook .
* [Chat Template](https://huggingface.co/blog/chat-templates) by Matthew Carrigan: é—œæ–¼æç¤ºæ¨¡æ¿çš„ Hugging Face é é¢

---
### 3. é è¨“ç·´æ¨¡å‹

é è¨“ç·´æ˜¯ä¸€å€‹éå¸¸æ¼«é•·ä¸”æˆæœ¬é«˜æ˜‚çš„éç¨‹ï¼Œé€™å°±æ˜¯ç‚ºä»€éº¼é€™ä¸æ˜¯æœ¬èª²ç¨‹çš„é‡é»ã€‚ä½†æ˜¯å°é è¨“ç·´æœŸé–“ç™¼ç”Ÿçš„æƒ…æ³æœ‰ä¸€å®šç¨‹åº¦çš„äº†è§£æ˜¯å¾ˆå¥½çš„ã€‚ç°¡å–®ä¾†èªªï¼Œäº†è§£å¯ä»¥ï¼Œå¯¦ç¾æ²’å››åƒè¬åˆ¥åšï¼Œç›®å‰å·²çŸ¥èŠ±æœ€å°‘éŒ¢è¨“ç·´çš„æ˜¯[10è¬ç¾å…ƒ](https://www.ithome.com.tw/news/158779)ï¼Œå¦å¤–ï¼Œ[æˆ‘å¾æ–°èå¾—çŸ¥é€šå¸¸çš„è¨“ç·´æˆæœ¬æ˜¯1200è¬ç¾å…ƒå·¦å³](https://tw.news.yahoo.com/chatgpt%E6%9C%89%E5%A4%9A%E7%87%92%E9%8C%A2-%E5%88%9D%E5%A7%8B%E6%99%B6%E7%89%87%E9%9C%808%E5%84%84-%E6%AC%A1%E8%A8%93%E7%B7%B4%E6%88%90%E6%9C%AC%E4%B8%8A%E7%9C%8B1200%E8%90%AC%E7%BE%8E%E5%85%83-014000496.html)ã€‚

* ç›®å‰è™Ÿç¨±å®Œå…¨é–‹æºçš„é è¨“ç·´æ¨¡å‹ï¼š
    * 1.[olmo](https://allenai.org/olmo):[è™Ÿç¨±çœŸæ­£é–‹æºï¼AI2é‡‹å‡ºOLMoèªè¨€æ¨¡å‹å’Œæ‰€æœ‰ç›¸é—œè³‡æ–™](https://www.ithome.com.tw/news/161199)
        * [å²ä¸Šé¦–ä¸ª100%å¼€æºå¤§æ¨¡å‹é‡ç£…ç™»åœºï¼Œç ´çºªå½•å…¬å¼€ä»£ç /æƒé‡/æ•°æ®é›†/è®­ç»ƒå…¨è¿‡ç¨‹ï¼ŒAMDéƒ½èƒ½è®­](https://36kr.com/p/2632336993616134)
    * 2.[LLM 360](https://www.llm360.ai/): é–‹æºå¤§èªè¨€æ¨¡å‹æ¡†æ¶ï¼ŒåŒ…å«è¨“ç·´å’Œè³‡æ–™æº–å‚™ä»£ç¢¼ã€è³‡æ–™ã€æŒ‡æ¨™å’Œæ¨¡å‹ã€‚
    * 3.[å…¨çƒé¦–å€‹å®Œå…¨é–‹æºçš„å¤§èªè¨€æ¨¡å‹Dollyï¼Œæ€§èƒ½å ªæ¯” GPT3.5ï¼](https://github.com/databrickslabs/dolly)

* **è³‡æ–™è™•ç†æµç¨‹**: é è¨“ç·´éœ€è¦é¾å¤§çš„è³‡æ–™é›† (ä¾‹å¦‚ï¼š [Llama 2](https://arxiv.org/abs/2307.09288) ä½¿ç”¨ 2 å…†å€‹tokensé€²è¡Œè¨“ç·´) ï¼Œéœ€è¦å°‡é€™äº›è³‡æ–™é›†éæ¿¾ã€æ¨™è¨˜åŒ–ä¸¦èˆ‡é å…ˆå®šç¾©çš„è©å½™é€²è¡Œæ•´ç†ã€‚
    * [LLMDataHub](https://github.com/Zjh-819/LLMDataHub) by Junhao Zhao: ç”¨æ–¼é è¨“ç·´ã€å¾®èª¿å’Œ RLHF çš„ç²¾é¸è³‡æ–™é›†æ¸…å–®ã€‚
    * [Training a causal language model from scratch](https://huggingface.co/learn/nlp-course/chapter7/6?fw=pt) by Hugging Face: ä½¿ç”¨ Transformers åº«å¾é ­é–‹å§‹é å…ˆè¨“ç·´ GPT-2 æ¨¡å‹ã€‚
    * [TinyLlama](https://github.com/jzhang38/TinyLlama) by Zhang et al.: å¯åœ¨æ­¤é …ç›®å¾ˆå¥½åœ°äº†è§£ Llama æ¨¡å‹æ˜¯å¦‚ä½•å¾é ­é–‹å§‹è¨“ç·´çš„ã€‚
    * [ä½¿ç”¨pytorchï¼Œç”¨æ­ç§¯æœ¨çš„æ–¹å¼å®ç°å®Œæ•´çš„Transformeræ¨¡å‹](https://zhuanlan.zhihu.com/p/682451065) 
    
* **å› æœèªè¨€å»ºæ¨¡(Causal language modeling)**: äº†è§£å› æœèªè¨€å»ºæ¨¡å’Œæ©ç¢¼èªè¨€å»ºæ¨¡(causal and masked language modeling)ä¹‹é–“çš„å€åˆ¥,ä»¥åŠæœ¬ä¾‹ä¸­ä½¿ç”¨çš„æå¤±å‡½æ•¸ã€‚æ›´å¤šé«˜æ•ˆç‡çš„é è¨“ç·´çŸ¥è­˜å¯å‰å¾€ [Megatron-LM](https://github.com/NVIDIA/Megatron-LM) æˆ–[gpt-neox](https://github.com/EleutherAI/gpt-neox)äº†è§£ã€‚
    * [Causal language modeling](https://huggingface.co/docs/transformers/tasks/language_modeling) by Hugging Face: è§£é‡‹å› æœèªè¨€å»ºæ¨¡å’Œå±è”½èªè¨€å»ºæ¨¡ä¹‹é–“çš„å·®ç•°ä»¥åŠå¦‚ä½•å¿«é€Ÿå¾®èª¿ DistilGPT-2 æ¨¡å‹ã€‚
* **ç¸®æ”¾çš„è¦å¾‹**: [ç¸®æ”¾çš„è¦å¾‹](https://arxiv.org/pdf/2001.08361.pdf) æ ¹æ“šæ¨¡å‹å¤§å°ã€è³‡æ–™é›†å¤§å°å’Œç”¨æ–¼è¨“ç·´çš„è¨ˆç®—é‡æè¿°é æœŸçš„æ¨¡å‹æ€§èƒ½ã€‚
    * [Chinchilla's wild implications](https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications) by nostalgebraist: è¨è«–ç¸®æ”¾å®šå¾‹ä¸¦è§£é‡‹å®ƒå€‘å°å¤§èªè¨€æ¨¡å‹çš„æ„ç¾©ã€‚
* **é«˜æ•ˆèƒ½é‹ç®—**: é€™æœ‰é»è¶…å‡ºäº†æœ¬æ–‡çš„ç¯„åœï¼Œä½†å¦‚æœæ‚¨æ‰“ç®—å¾é ­é–‹å§‹å‰µå»ºè‡ªå·±çš„LLMs(å¤§èªè¨€æ¨¡å‹)ï¼ˆç¡¬é«”ã€åˆ†æ•£å¼å·¥ä½œè² è¼‰ç­‰ï¼‰ï¼Œé‚£éº¼æ›´å¤šæœ‰é—œ HPC çš„çŸ¥è­˜æ˜¯å°ä½ è€Œè¨€æ˜¯å¿…è¦çš„ã€‚
  
ğŸ“š **åƒè€ƒè³‡æ–™**:
* [BLOOM](https://bigscience.notion.site/BLOOM-BigScience-176B-Model-ad073ca07cdf479398d5f95d88e218c4) by BigScience: æè¿°å¦‚ä½•å»ºç«‹ BLOOM æ¨¡å‹çš„ Notion é é¢ï¼Œå…¶ä¸­åŒ…å«å¤§é‡æœ‰é—œå·¥ç¨‹éƒ¨åˆ†å’Œé‡åˆ°å•é¡Œçš„æœ‰ç”¨è³‡è¨Šã€‚
* [OPT-175 Logbook](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/OPT175B_Logbook.pdf) by Meta: ç ”ç©¶æ—¥èªŒé¡¯ç¤ºå‡ºäº†ä»€éº¼éŒ¯çš„ä»¥åŠä»€éº¼æ˜¯æ­£ç¢ºçš„ã€‚å¦‚æœæ‚¨è¨ˆåŠƒé å…ˆè¨“ç·´éå¸¸å¤§çš„èªè¨€æ¨¡å‹ï¼ˆåœ¨æœ¬ä¾‹ä¸­ç‚º 175B åƒæ•¸ï¼‰ï¼Œå‰‡éå¸¸æœ‰ç”¨ã€‚

---
### 4. ç›£ç£å¾®èª¿ (Supervised Fine-Tuning)

é è¨“ç·´æ¨¡å‹åƒ…é‡å°ä¸‹ä¸€å€‹æ¨™è¨˜(next-token)é æ¸¬ä»»å‹™é€²è¡Œè¨“ç·´ï¼Œé€™å°±æ˜¯ç‚ºä»€éº¼å®ƒå€‘ä¸æ˜¯æœ‰ç”¨çš„åŠ©æ‰‹ã€‚SFT å…è¨±æ‚¨èª¿æ•´å®ƒå€‘ä»¥å›æ‡‰æŒ‡ä»¤ã€‚æ­¤å¤–ï¼Œå®ƒå…è¨±æ‚¨æ ¹æ“šä»»ä½•è³‡æ–™ï¼ˆç§äººè³‡æ–™ã€GPT-4 ç„¡æ³•çœ‹åˆ°çš„è³‡æ–™ç­‰ï¼‰å¾®èª¿æ‚¨çš„æ¨¡å‹ä¸¦ä½¿ç”¨å®ƒï¼Œè€Œç„¡éœ€æ”¯ä»˜ OpenAI ç­‰ API çš„è²»ç”¨ã€‚
* ç°¡ä»‹è·Ÿæ•™å­¸ï¼š
    * [ç”¨äººè©±è¬›è§£å¾®èª¿æŠ€è¡“](https://www.zhihu.com/zvideo/1723348624463994881)ï¼šè«‹åªå°ˆæ³¨æŠ€è¡“ã€‚
    * [Finetuning Large Language Models](https://www.deeplearning.ai/short-courses/finetuning-large-language-models/): Deeplearning.ai çš„å¾®èª¿æŠ€è¡“çŸ­èª²ç¨‹ï¼Œé©åˆå¿«é€Ÿå…¥é–€ã€‚
* **å…¨å¾®èª¿**: å…¨å¾®èª¿æ˜¯æŒ‡è¨“ç·´æ¨¡å‹ä¸­çš„æ‰€æœ‰åƒæ•¸ ( å°±æ˜¯æ¨¡å‹è¨“ç·´ï¼Œåªæ˜¯è³‡æ–™é‡ä¸å¤šï¼Œä¸¦ä¸”è³‡æ–™é€šå¸¸æ˜¯ç‰¹å®šä»»å‹™æˆ–å­é ˜åŸŸä¸Šçš„ )ã€‚é€™ä¸æ˜¯ä¸€ç¨®æœ‰æ•ˆçš„æŠ€è¡“ï¼Œä½†å®ƒæœƒç”¢ç”Ÿç¨å¾®å¥½ä¸€é»çš„çµæœ.
    * [The Novice's LLM Training Guide](https://rentry.org/llm-training) by Alpin: æ¦‚è¿°å¾®èª¿ LLM æ™‚è¦è€ƒæ…®çš„ä¸»è¦æ¦‚å¿µå’Œåƒæ•¸.
* [**LoRA**](https://arxiv.org/abs/2106.09685): ä¸€ç¨®åŸºæ–¼ä½éšé©é…å™¨(low-rank adapters)çš„é«˜æ•ˆåƒæ•¸å¾®èª¿æŠ€è¡“ï¼ˆPEFTï¼‰ã€‚æˆ‘å€‘ä¸è¨“ç·´æ‰€æœ‰åƒæ•¸ï¼Œè€Œæ˜¯åªè¨“ç·´é€™äº›é©é…å™¨(adapters)ã€‚
    * [LoRA insights](https://lightning.ai/pages/community/lora-insights/) by Sebastian Raschka: æœ‰é—œ LoRA ä»¥åŠå¦‚ä½•é¸æ“‡æœ€ä½³åƒæ•¸çš„å¯¦ç”¨è¦‹è§£.
    * [Fine-Tune Your Own Llama 2 Model](https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html): æœ‰é—œå¦‚ä½•ä½¿ç”¨ Hugging Face åº«å¾®èª¿ Llama 2 æ¨¡å‹çš„å¯¦ä½œæ•™å­¸.
    * [Padding Large Language Models](https://towardsdatascience.com/padding-large-language-models-examples-with-llama-2-199fb10df8ff) by Benjamin Marie: ç‚ºå› æœLLMs(causal LLMs)å¡«å……è¨“ç·´ç¯„ä¾‹çš„æœ€ä½³å¯¦è¸ 
* [**QLoRA**](https://arxiv.org/abs/2305.14314): å¦ä¸€å€‹åŸºæ–¼ LoRA çš„ PEFTï¼Œå®ƒé‚„å°‡æ¨¡å‹çš„æ¬Šé‡é‡åŒ–ç‚º 4 bitsï¼Œä¸¦å¼•å…¥åˆ†é å„ªåŒ–å™¨ä¾†ç®¡ç†è¨˜æ†¶é«”å³°å€¼ã€‚å°‡å…¶èˆ‡[Unsloth](https://github.com/unslothai/unsloth)çµåˆä½¿ç”¨ï¼Œå¯ä»¥åœ¨å…è²»çš„ Colab ç­†è¨˜æœ¬ä¸Šé‹è¡Œã€‚
* **[Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)**: ä¸€ç¨®ç”¨æˆ¶å‹å¥½ä¸”åŠŸèƒ½å¼·å¤§çš„å¾®èª¿å·¥å…·ï¼Œç”¨æ–¼è¨±å¤šæœ€å…ˆé€²çš„é–‹æºæ¨¡å‹ã€‚
    * [A Beginner's Guide to LLM Fine-Tuning](https://mlabonne.github.io/blog/posts/A_Beginners_Guide_to_LLM_Finetuning.html): æœ‰é—œå¦‚ä½•ä½¿ç”¨ Axolotl å¾®èª¿ CodeLlama æ¨¡å‹çš„æ•™å­¸.
* [**DeepSpeed**](https://www.deepspeed.ai/): é‡å°å¤š GPU å’Œå¤šç¯€é»è¨­å®šçš„ LLM çš„é«˜æ•ˆé è¨“ç·´å’Œå¾®èª¿ï¼ˆåœ¨ Axolotl ä¸­å¯¦ç¾ï¼‰ã€‚
    
ğŸ“š **åƒè€ƒè³‡æ–™**:
* [ä¸‡å­—é•¿æ–‡ä¹‹æç¤ºå­¦ä¹ å’Œå¾®è°ƒå¤§æ¨¡å‹ï¼ˆPrompt Learning & Prompt Tuningï¼‰](https://zhuanlan.zhihu.com/p/670039833)
* [å¤§æ¨¡å‹å¾®è°ƒæ€»ç»“](https://www.zhihu.com/tardis/zm/art/627642632?source_id=1003)
* [Finetuning Large Language Models çš„èª²ç¨‹ç­†è¨˜](https://hackmd.io/@YungHuiHsu/HJ6AT8XG6)

---
### 5. Reinforcement Learning from Human Feedback RLHF (æ ¹æ“šäººé¡å›é¥‹é€²è¡Œå¼·åŒ–å­¸ç¿’)

ç¶“éç›£ç£å¾®èª¿å¾Œï¼ŒRLHF æ˜¯ç”¨ä¾†ä½¿ LLM çš„ç­”æ¡ˆèˆ‡äººé¡æœŸæœ›ä¿æŒä¸€è‡´çš„ä¸€å€‹æ­¥é©Ÿã€‚é€™å€‹æƒ³æ³•æ˜¯å¾äººé¡ï¼ˆæˆ–äººå·¥ï¼‰å›é¥‹ä¸­å­¸ç¿’åå¥½ï¼Œé€™å¯ç”¨æ–¼æ¸›å°‘åè¦‹ã€å¯©æŸ¥æ¨¡å‹æˆ–ä½¿å®ƒå€‘ä»¥æ›´æœ‰ç”¨çš„æ–¹å¼è¡Œäº‹ã€‚å®ƒæ¯” SFT æ›´è¤‡é›œï¼Œä¸¦ä¸”é€šå¸¸è¢«è¦–ç‚ºå¯é¸é …ä¹‹ä¸€ã€‚
* ç°¡ä»‹èˆ‡å…¥é–€ï¼š
    * [An Introduction to Training LLMs using RLHF](https://wandb.ai/ayush-thakur/Intro-RLAIF/reports/An-Introduction-to-Training-LLMs-Using-Reinforcement-Learning-From-Human-Feedback-RLHF---VmlldzozMzYyNjcy) by Ayush Thakur: é€™è§£é‡‹äº†ç‚ºä»€éº¼ RLHF å°æ–¼æ¸›å°‘å¤§èªè¨€æ¨¡å‹çš„åè¦‹å’Œæé«˜ç¸¾æ•ˆæ˜¯å¯å–çš„ã€‚
    * [Illustration RLHF](https://huggingface.co/blog/rlhf) by Hugging Face: RLHF ç°¡ä»‹ï¼ŒåŒ…æ‹¬çå‹µæ¨¡å‹è¨“ç·´å’Œå¼·åŒ–å­¸ç¿’å¾®èª¿.
    * [RLHF from Deeplearning.ai](https://www.deeplearning.ai/short-courses/finetuning-large-language-models/):  Deeplearning.ai çš„RLHFçŸ­èª²ç¨‹ï¼Œé©åˆå¿«é€Ÿå…¥é–€ã€‚

* **åå¥½è³‡æ–™é›†**: é€™äº›è³‡æ–™é›†é€šå¸¸åŒ…å«å…·æœ‰æŸç¨®æ’åçš„å¤šå€‹ç­”æ¡ˆï¼Œé€™ä½¿å¾—å®ƒå€‘æ¯”æŒ‡ä»¤è³‡æ–™é›†æ›´é›£ç”¢ç”Ÿ.
    * [StackLLaMA](https://huggingface.co/blog/stackllama) by Hugging Face: ä½¿ç”¨ Transformer å‡½å¼åº«æœ‰æ•ˆåœ°å°‡ LLaMA æ¨¡å‹èˆ‡ RLHF å°é½Šçš„æ•™å­¸.
* [**è¿‘ç«¯ç­–ç•¥æœ€ä½³åŒ–**](https://arxiv.org/abs/1707.06347): æ­¤æ¼”ç®—æ³•åˆ©ç”¨çå‹µæ¨¡å‹ä¾†é æ¸¬çµ¦å®šæ–‡å­—æ˜¯å¦è¢«äººé¡æ’åè¼ƒé«˜ã€‚ç„¶å¾Œä½¿ç”¨è©²é æ¸¬ä¾†æœ€ä½³åŒ– SFT æ¨¡å‹ï¼Œä¸¦æ ¹æ“š KL æ•£åº¦é€²è¡Œçæ‡²ã€‚
* **[ç›´æ¥åå¥½å„ªåŒ–](https://arxiv.org/abs/2305.18290)**: DPO é€éå°‡å…¶é‡æ–°å®šç¾©ç‚ºåˆ†é¡å•é¡Œä¾†ç°¡åŒ–æµç¨‹ã€‚å®ƒä½¿ç”¨åƒè€ƒæ¨¡å‹è€Œä¸æ˜¯çå‹µæ¨¡å‹ï¼ˆç„¡éœ€è¨“ç·´ï¼‰ï¼Œä¸¦ä¸”åªéœ€è¦ä¸€å€‹è¶…åƒæ•¸ï¼Œä½¿å…¶æ›´åŠ ç©©å®šå’Œé«˜æ•ˆã€‚

ğŸ“š **åƒè€ƒè³‡æ–™**:
* [LLM Training: RLHF and Its Alternatives](https://substack.com/profile/27393275-sebastian-raschka-phd) by Sebastian Rashcka: RLHF æµç¨‹å’Œ RLAIF ç­‰æ›¿ä»£æ–¹æ¡ˆçš„æ¦‚è¿°.
* [Fine-tune Mistral-7b with DPO](https://huggingface.co/blog/dpo-trl):ä½¿ç”¨ DPO å¾®èª¿ Mistral-7b æ¨¡å‹ä¸¦é‡ç¾[NeuralHermes-2.5](https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B) çš„æ•™å­¸.
* [[RL] Fine-Tuning Language Models from Human Preferences (RLHF) è«–æ–‡ç­†è¨˜-ChatGPTéŠæˆè¡“](https://hackmd.io/@YungHuiHsu/Sy5Ug7iV6)
* [è¯¦è§£å¤§æ¨¡å‹RLHFè¿‡ç¨‹ï¼ˆé…ä»£ç è§£è¯»ï¼‰](https://zhuanlan.zhihu.com/p/624589622)
* [LLMåŸºçŸ³ï¼šRLHFåŠå…¶æ›¿ä»£æŠ€æœ¯](https://zhuanlan.zhihu.com/p/682683518)
* [å›¾è§£å¤§æ¨¡å‹RLHFç³»åˆ—ä¹‹ï¼šäººäººéƒ½èƒ½çœ‹æ‡‚çš„PPOåŸç†ä¸æºç è§£è¯»](https://zhuanlan.zhihu.com/p/677607581)
* [RLAIFç»†èŠ‚åˆ†äº«&ä¸ªäººæƒ³æ³•](https://zhuanlan.zhihu.com/p/657436655)
---
### 6. è©•ä¼° Evaluation

è©•ä¼°å¤§å‹èªè¨€æ¨¡å‹(LLMs)æ˜¯æµç¨‹ä¸­ä¸€å€‹è¢«ä½ä¼°çš„éƒ¨åˆ†ï¼Œå› ç‚ºè©•ä¼°é€™ä¸€å€‹éç¨‹è€—æ™‚ä¸”ç›¸å°å¯é æ€§è¼ƒä½ã€‚ä½ çš„ä¸‹æ¸¸ä»»å‹™æ‡‰è©²æŒ‡æ˜ä½ æƒ³è¦è©•ä¼°çš„å…§å®¹ï¼Œä½†è¨˜å¾—å¤å¾·å“ˆç‰¹å®šå¾‹(Goodhart's law)æåˆ°çš„ï¼šâ€œç•¶ä¸€å€‹è¡¡é‡æŒ‡æ¨™è®Šæˆäº†ç›®æ¨™ï¼Œå®ƒå°±ä¸å†æ˜¯ä¸€å€‹å¥½çš„è¡¡é‡æŒ‡æ¨™ã€‚â€
* ç°¡ä»‹èˆ‡å…¥é–€ï¼š
    * [Evaluating and Debugging Generative AI Models Using Weights and Biases](https://www.deeplearning.ai/short-courses/evaluating-debugging-generative-ai//): Deeplearning AI çš„çŸ­èª²ç¨‹ï¼Œé©åˆå¿«é€Ÿå…¥é–€ã€‚
    * [LLM Evaluation å¦‚ä½•è¯„ä¼°ä¸€ä¸ªå¤§æ¨¡å‹ï¼Ÿ](https://zhuanlan.zhihu.com/p/644373658): åˆ¥äººçš„å¿ƒå¾—ï¼Œå¯ä»¥åƒè€ƒä¸‹ã€‚

* **å‚³çµ±æŒ‡æ¨™ Traditional metrics**: åƒå›°æƒ‘åº¦(perplexity)å’ŒBLEUåˆ†æ•¸é€™æ¨£çš„æŒ‡æ¨™ä¸å†åƒä»¥å‰é‚£æ¨£å—æ­¡è¿ï¼Œå› ç‚ºåœ¨å¤§å¤šæ•¸æƒ…æ³ä¸‹å®ƒå€‘æ˜¯æœ‰ç¼ºé™·çš„ã€‚ä½†äº†è§£å®ƒå€‘ä»¥åŠå®ƒå€‘é©ç”¨çš„æƒ…å¢ƒä»ç„¶å¾ˆé‡è¦ã€‚
    * [å›ºå®šé•·åº¦è¼¸å…¥(æœ‰æœ€å¤§è¼¸å…¥é™åˆ¶)æ¨¡å‹çš„å›°æƒ‘åº¦](https://huggingface.co/docs/transformers/perplexity) by Hugging Face: å›°æƒ‘åº¦(perplexity)çš„æ¦‚è¿°ï¼Œä¸¦ä½¿ç”¨ Transformer åº«å¯¦ç¾äº†å®ƒçš„ç¨‹å¼ç¢¼ã€‚
    * [BLEU ä½¿ç”¨é¢¨éšª](https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213) by Rachael Tatman: BLEU åˆ†æ•¸åŠå…¶è¨±å¤šå•é¡Œçš„æ¦‚è¿°ï¼Œä¸¦æä¾›äº†ç¤ºä¾‹ã€‚
* **é€šç”¨åŸºæº– General benchmarks**: åŸºæ–¼èªè¨€æ¨¡å‹è©•ä¼°å·¥å…·ç®± [Language Model Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness)ï¼ŒOpen LLMæ’è¡Œæ¦œ [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) æ˜¯ç”¨æ–¼é€šç”¨å¤§å‹èªè¨€æ¨¡å‹ï¼ˆå¦‚ChatGPTï¼‰çš„ä¸»è¦åŸºæº–ã€‚é‚„æœ‰å…¶ä»–å—æ­¡è¿çš„åŸºæº–ï¼Œå¦‚[BigBench](https://github.com/google/BIG-bench), [MT-Bench](https://arxiv.org/abs/2306.05685)ç­‰ã€‚
    * [å¤§å‹èªè¨€æ¨¡å‹è©•ä¼°èª¿æŸ¥](https://arxiv.org/abs/2307.03109) by Chang et al.: é—œæ–¼è©•ä¼°ä»€éº¼ã€åœ¨å“ªè£¡è©•ä¼°ä»¥åŠå¦‚ä½•è©•ä¼°çš„ç¶œåˆæ€§è«–æ–‡ã€‚
* **ä»»å‹™ç‰¹å®šåŸºæº– Task-specific benchmarks**: å¦‚æ‘˜è¦ã€ç¿»è­¯å’Œå•ç­”ç­‰ä»»å‹™æœ‰å°ˆé–€çš„åŸºæº–ã€æŒ‡æ¨™ç”šè‡³å­é ˜åŸŸï¼ˆé†«ç™‚ã€é‡‘èç­‰ï¼‰ï¼Œä¾‹å¦‚ç”¨æ–¼ç”Ÿç‰©é†«å­¸å•ç­” [PubMedQA](https://pubmedqa.github.io/)ã€‚
* **äººé¡è©•ä¼° Human evaluation**: æœ€å¯é çš„è©•ä¼°æ˜¯ç”¨æˆ¶çš„æ¥å—åº¦æˆ–ç”±äººé¡æ‰€åšçš„æ¯”è¼ƒã€‚å¦‚æœä½ æƒ³çŸ¥é“ä¸€å€‹æ¨¡å‹è¡¨ç¾å¾—å¦‚ä½•ï¼Œæœ€ç°¡å–®ä½†æœ€ç¢ºå®šçš„æ–¹å¼å°±æ˜¯è‡ªå·±ä½¿ç”¨å®ƒã€‚

ğŸ“š **åƒè€ƒæ–‡ç»**:
* [èŠå¤©æ©Ÿå™¨äººæ’è¡Œæ¦œ](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard) by lmsys: åŸºæ–¼äººé¡æ¯”è¼ƒçš„é€šç”¨å¤§å‹èªè¨€æ¨¡å‹çš„Eloè©•åˆ†ã€‚

---
### 7. é‡åŒ–

é‡åŒ–æ˜¯å°‡æ¨¡å‹çš„æ¬Šé‡ï¼ˆå’Œå•Ÿå‹•å€¼ï¼‰è½‰æ›æˆæ›´ä½ç²¾åº¦è¡¨ç¤ºçš„éç¨‹ã€‚ä¾‹å¦‚ï¼ŒåŸæœ¬ä½¿ç”¨16ä½å…ƒå„²å­˜çš„æ¬Šé‡å¯ä»¥è½‰æ›æˆ4ä½å…ƒè¡¨ç¤ºã€‚é€™ç¨®æŠ€è¡“æ„ˆä¾†æ„ˆé‡è¦ï¼Œç”¨ä¾†æ¸›å°‘èˆ‡å¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç›¸é—œçš„è¨ˆç®—èˆ‡è¨˜æ†¶é«”æˆæœ¬ã€‚

* **åŸºç¤æŠ€è¡“**: ç­è§£ä¸åŒçš„ç²¾ç¢ºåº¦å±¤ç´šï¼ˆFP32ã€FP16ã€INT8ç­‰ï¼‰ä»¥åŠå¦‚ä½•ä½¿ç”¨absmaxèˆ‡é›¶é»æŠ€è¡“(zero-point techniques)é€²è¡Œç°¡å–®çš„é‡åŒ–ã€‚
* **GGUFå’Œllama.cpp**: æœ€åˆè¨­è¨ˆç”¨æ–¼åœ¨CPUä¸Šé‹è¡Œï¼Œ[llama.cpp](https://github.com/ggerganov/llama.cpp) å’ŒGGUFæ ¼å¼å·²æˆç‚ºåœ¨æ¶ˆè²»ç´šç¡¬é«”ä¸Šé‹è¡ŒLLMsçš„æœ€å—æ­¡è¿çš„å·¥å…·ã€‚
* **GPTQå’ŒEXL2**: [GPTQ](https://arxiv.org/abs/2210.17323) ï¼Œç‰¹åˆ¥æ˜¯ [EXL2](https://github.com/turboderp/exllamav2) ï¼Œæä¾›äº†è¼ƒå¿«çš„é€Ÿåº¦ï¼Œä½†åªèƒ½åœ¨GPUä¸Šé‹è¡Œã€‚æ¨¡å‹é‡åŒ–ä¹Ÿéœ€è¦å¾ˆé•·æ™‚é–“ã€‚
* **AWQ**: é€™ç¨®æ–°æ ¼å¼æ¯”GPTQæ›´æº–ç¢ºï¼ˆå›°æƒ‘åº¦æ›´ä½ï¼‰ï¼Œä½†ä½¿ç”¨çš„é¡¯å­˜æ›´å¤šï¼Œé€Ÿåº¦ä¹Ÿä¸ä¸€å®šæ›´å¿«ã€‚

ğŸ“š **åƒè€ƒæ–‡ç»**:
* [é‡åŒ–ç°¡ä»‹](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html): é‡åŒ–æ¦‚è¿°ï¼Œabsmaxèˆ‡é›¶é»é‡åŒ–ï¼Œä»¥åŠä½¿ç”¨ LLM.int8()åœ¨ç¨‹å¼ç¢¼ä¸Šã€‚
* [ä½¿ç”¨llama.cppé‡åŒ–Llamaæ¨¡å‹](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html): é—œæ–¼å¦‚ä½•ä½¿ç”¨llama.cppå’ŒGGUFæ ¼å¼é‡åŒ–Llama 2æ¨¡å‹çš„æ•™å­¸ã€‚
* [ä½¿ç”¨GPTQé€²è¡Œ4ä½å…ƒLLMé‡åŒ–](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html):é—œæ–¼å¦‚ä½•ä½¿ç”¨GPTQæ¼”ç®—æ³•å’ŒAutoGPTQé‡åŒ–LLMçš„æ•™å­¸ã€‚
* [ExLlamaV2: é‹è¡ŒLLMsçš„æœ€å¿«ç¨‹å¼åº«](https://mlabonne.github.io/blog/posts/ExLlamaV2_The_Fastest_Library_to_Run%C2%A0LLMs.html): æŒ‡å—ï¼›é—œæ–¼å¦‚ä½•ä½¿ç”¨EXL2æ ¼å¼é‡åŒ–Mistralæ¨¡å‹ï¼Œä¸¦ä½¿ç”¨ExLlamaV2ç¨‹å¼åº«é‹è¡Œã€‚
* [äº†è§£å•Ÿå‹•æ„ŸçŸ¥æ¬Šé‡é‡åŒ–](https://medium.com/friendliai/understanding-activation-aware-weight-quantization-awq-boosting-inference-serving-efficiency-in-10bb0faf63a8) by FriendliAI: AWQæŠ€è¡“åŠå…¶å„ªå‹¢çš„æ¦‚è¿°ã€‚

---
### 8. æ–°è¶¨å‹¢

* **ä½ç½®åµŒå…¥ Positional embeddings**: äº†è§£å¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ä½•ç·¨ç¢¼ä½ç½®ï¼Œç‰¹åˆ¥æ˜¯åƒ [RoPE](https://arxiv.org/abs/2104.09864) é€™æ¨£çš„ç›¸å°ä½ç½®ç·¨ç¢¼æ–¹æ¡ˆã€‚å¯¦ç¾ [YaRN](https://arxiv.org/abs/2309.00071) (é€šéæº«åº¦å› å­ä¹˜ä»¥æ³¨æ„åŠ›çŸ©é™£) or [ALiBi](https://arxiv.org/abs/2108.12409) (åŸºæ–¼tokenè·é›¢çš„æ³¨æ„åŠ›çæ‡²) ä¾†æ“´å±•ä¸Šä¸‹æ–‡é•·åº¦ã€‚
* **æ¨¡å‹èåˆ Model merging**: å°‡è¨“ç·´å¥½çš„æ¨¡å‹åˆä½µå·²æˆç‚ºä¸€ç¨®æµè¡Œçš„æ–¹å¼ï¼Œç”¨æ–¼å‰µå»ºè¡¨ç¾å„ªç•°çš„æ¨¡å‹ï¼Œç„¡éœ€ä»»ä½•å¾®èª¿ã€‚æµè¡Œçš„ [mergekit](https://github.com/cg123/mergekit) åº«å¯¦ç¾äº†æœ€å—æ­¡è¿çš„èåˆæ–¹æ³•ï¼Œå¦‚e SLERP, [DARE](https://arxiv.org/abs/2311.03099), å’Œ [TIES](https://arxiv.org/abs/2311.03099)ã€‚æ¨¡å‹èåˆé€šå¸¸æŒ‡çš„æ˜¯å°‡å¤šå€‹å·²è¨“ç·´çš„æ¨¡å‹åˆä½µæˆä¸€å€‹å–®ä¸€æ¨¡å‹çš„éç¨‹ã€‚é€™ä¸åƒ…åƒ…æ˜¯å¹³å‡æˆ–æŠ•ç¥¨æ±ºå®šè¼¸å‡ºï¼Œè€Œæ˜¯åœ¨æ¨¡å‹çš„æ¬Šé‡å’Œçµæ§‹å±¤é¢ä¸Šé€²è¡Œåˆä½µã€‚é€™å€‹éç¨‹ä¸éœ€è¦å†æ¬¡è¨“ç·´ï¼Œå¯ä»¥é€šéæ•¸å­¸æ“ä½œï¼ˆå¦‚çƒé¢ç·šæ€§å…§æ’ï¼ˆSLERPï¼‰æˆ–å…¶ä»–èåˆæŠ€è¡“ï¼‰å°‡ä¸åŒæ¨¡å‹çš„çŸ¥è­˜æ•´åˆèµ·ä¾†ã€‚æ¨¡å‹èåˆç”¨æ–¼å‰µå»ºä¸€å€‹è¡¨ç¾æ›´ä½³ã€æ›´å¼·å¤§çš„æ¨¡å‹ï¼Œé€šå¸¸æ˜¯å°‡å¤šå€‹æ¨¡å‹åœ¨ç‰¹å®šä»»å‹™ä¸Šçš„å„ªå‹¢çµåˆèµ·ä¾†ã€‚
* **å°ˆå®¶æ··åˆ Mixture of Experts**: [Mixtral](https://arxiv.org/abs/2401.04088) å› å…¶å“è¶Šçš„æ€§èƒ½è€Œé‡æ–°ä½¿MoEæ¶æ§‹æµè¡Œèµ·ä¾†ã€‚ èˆ‡æ­¤åŒæ™‚ï¼ŒOSSç¤¾å€å‡ºç¾äº†ä¸€ç¨®frankenMoEï¼Œé€šéèåˆåƒ [Phixtral](https://huggingface.co/mlabonne/phixtral-2x2_8)é€™æ¨£çš„æ¨¡å‹ï¼Œé€™æ˜¯ä¸€å€‹æ›´ç¶“æ¿Ÿä¸”æ€§èƒ½è‰¯å¥½çš„é¸é …ã€‚MoEæ˜¯ä¸€ç¨®çµæ§‹ï¼Œå®ƒåŒ…å«å¤šå€‹å­æ¨¡å‹æˆ–â€œå°ˆå®¶â€ï¼Œæ¯å€‹å°ˆå®¶å°ˆé–€è™•ç†ä¸åŒçš„ä»»å‹™æˆ–æ•¸æ“šå­é›†ã€‚åœ¨MoEæ¶æ§‹ä¸­ï¼Œä¸€å€‹â€œgateâ€æˆ–èª¿åº¦å™¨æ±ºå®šå°æ–¼çµ¦å®šçš„è¼¸å…¥ï¼Œå“ªå€‹å°ˆå®¶è¢«ä½¿ç”¨ã€‚é€™æ˜¯ä¸€ç¨®ç¨€ç–å•Ÿå‹•æ–¹æ³•ï¼Œå¯ä»¥å¤§å¹…æå‡æ¨¡å‹çš„å®¹é‡å’Œæ•ˆç‡ï¼Œå› ç‚ºä¸æ˜¯æ‰€æœ‰çš„å°ˆå®¶éƒ½æœƒå°æ¯å€‹è¼¸å…¥é€²è¡ŒéŸ¿æ‡‰ã€‚
* **å¤šæ¨¡æ…‹æ¨¡å‹ Multimodal models**: é€™äº›æ¨¡å‹ï¼ˆ [CLIP](https://openai.com/research/clip), [Stable Diffusion](https://stability.ai/stable-image), æˆ– [LLaVA](https://llava-vl.github.io/)) è™•ç†å¤šç¨®é¡å‹çš„è¼¸å…¥ï¼ˆæ–‡æœ¬ã€åœ–åƒã€éŸ³é »ç­‰ï¼‰èˆ‡çµ±ä¸€çš„åµŒå…¥ç©ºé–“ï¼Œå¾è€Œè§£é–äº†å¼·å¤§çš„æ‡‰ç”¨ï¼Œå¦‚æ–‡æœ¬åˆ°åœ–åƒã€‚

ğŸ“š **åƒè€ƒæ–‡ç»**:
* [Extending the RoPE](https://blog.eleuther.ai/yarn/) by EleutherAI: ç¸½çµä¸åŒä½ç½®ç·¨ç¢¼æŠ€è¡“çš„æ–‡ç« .
* [Understanding YaRN](https://medium.com/@rcrajatchawla/understanding-yarn-extending-context-window-of-llms-3f21e3522465) by Rajat Chawla: å°YaRNçš„ä»‹ç´¹.
* [Merge LLMs with mergekit](https://mlabonne.github.io/blog/posts/2024-01-08_Merge_LLMs_with_mergekit.html): é—œæ–¼ä½¿ç”¨mergekité€²è¡Œæ¨¡å‹èåˆçš„æ•™ç¨‹.
* [Mixture of Experts Explained](https://huggingface.co/blog/moe) by Hugging Face: é—œæ–¼MoEåŠå…¶å·¥ä½œæ–¹å¼çš„è©³ç›¡æŒ‡å—.
* [Large Multimodal Models](https://huyenchip.com/2023/10/10/multimodal.html) by Chip Huyen: å°å¤šæ¨¡æ…‹ç³»çµ±åŠå…¶è¿‘æœŸç™¼å±•æ­·å²çš„æ¦‚è¿°.

## ğŸ‘· LLM æ‡‰ç”¨å·¥ç¨‹

æœ¬èª²ç¨‹çš„é€™ä¸€éƒ¨åˆ†é‡é»æ˜¯å­¸ç¿’å¦‚ä½•å»ºç«‹å¯åœ¨ç”Ÿç”¢ä¸­ä½¿ç”¨çš„ç”± LLM æ”¯æ´çš„æ‡‰ç”¨ç¨‹åºï¼Œé‡é»ä¸»è¦åœ¨å¢å¼·æ¨¡å‹å’Œéƒ¨ç½²å®ƒå€‘ã€‚

![](img/roadmap_engineer.png)


### 1. é‹è¡Œ LLMs


ç”±æ–¼ç¡¬é«”è¦æ±‚é«˜ï¼Œé‹è¡Œå¤§å‹èªè¨€æ¨¡å‹å¯èƒ½æœƒæœ‰å›°é›£ã€‚æ ¹æ“šæ‚¨çš„ä½¿ç”¨æ¡ˆä¾‹ï¼Œæ‚¨ä¹Ÿå¯èƒ½åªæƒ³é€šéAPIï¼ˆå¦‚GPT-4ï¼‰ç°¡å–®åœ°ä½¿ç”¨æ¨¡å‹ï¼Œæˆ–è€…åœ¨æœ¬åœ°é‹è¡Œå®ƒã€‚ç„¡è«–å“ªç¨®æƒ…æ³ï¼Œé¡å¤–çš„æç¤ºå’ŒæŒ‡å°æŠ€è¡“éƒ½å¯ä»¥æ”¹å–„æˆ–é™åˆ¶æ‚¨çš„æ‡‰ç”¨ç¨‹åºçš„è¼¸å‡ºã€‚

* **LLM APIs**: APIsæ˜¯éƒ¨ç½²LLMsçš„ä¾¿æ·æ–¹å¼ã€‚é€™å€‹é ˜åŸŸåˆ†åˆ¥æœ‰ç§æœ‰LLMs ([OpenAI](https://platform.openai.com/), [Google](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview), [Anthropic](https://docs.anthropic.com/claude/reference/getting-started-with-the-api), [Cohere](https://docs.cohere.com/docs), ç­‰.) å’Œé–‹æºLLMs ([OpenRouter](https://openrouter.ai/), [Hugging Face](https://huggingface.co/inference-api), [Together AI](https://www.together.ai/), ç­‰.).
* **é–‹æº LLMs**: [Hugging Face Hub](https://huggingface.co/models) æ˜¯å°‹æ‰¾LLMsçš„å¥½åœ°æ–¹ã€‚ æ‚¨å¯ä»¥ç›´æ¥åœ¨ [Hugging Face Spaces](https://huggingface.co/spaces)ä¸­é‹è¡Œä¸€äº›æ¨¡å‹ï¼Œ æˆ–è€…ä¸‹è¼‰ä¸¦åœ¨åƒ [LM Studio](https://lmstudio.ai/) é€™æ¨£çš„æ‡‰ç”¨ç¨‹åºä¸­æœ¬åœ°é‹è¡Œä»¥åŠé€šéCLIä½¿ç”¨ [llama.cpp](https://github.com/ggerganov/llama.cpp) æˆ– [Ollama](https://ollama.ai/)ã€‚
* **æç¤ºå·¥ç¨‹ Prompt engineering**: å¸¸è¦‹æŠ€è¡“åŒ…æ‹¬é›¶æç¤ºè©ã€å°‘é‡æç¤ºè©ã€æ€ç¶­éˆèˆ‡ReActã€‚å®ƒå€‘åœ¨æ›´å¤§çš„æ¨¡å‹ä¸Šå·¥ä½œå¾—æ›´å¥½ï¼Œä½†ä¹Ÿå¯ä»¥é©æ‡‰è¼ƒå°çš„æ¨¡å‹ã€‚
* **çµæ§‹åŒ–è¼¸å‡º Structuring outputs**: è¨±å¤šä»»å‹™éœ€è¦çµæ§‹åŒ–çš„è¼¸å‡ºï¼Œå¦‚åš´æ ¼çš„æ¨¡æ¿æˆ–JSONæ ¼å¼ã€‚åƒ [LMQL](https://lmql.ai/), [Outlines](https://github.com/outlines-dev/outlines), [Guidance](https://github.com/guidance-ai/guidance), ç­‰åº«å¯ä»¥ç”¨ä¾†æŒ‡å°ç”Ÿæˆä¸¦éµå¾ªçµ¦å®šçš„çµæ§‹ã€‚

ğŸ“š **åƒè€ƒè³‡æ–™**:
* [Run an LLM locally with LM Studio](https://www.kdnuggets.com/run-an-llm-locally-with-lm-studio) by Nisha Arya: é—œæ–¼å¦‚ä½•ä½¿ç”¨LM Studioçš„ç°¡çŸ­æŒ‡å—ã€‚
* [Prompt engineering guide](https://www.promptingguide.ai/) by DAIR.AI: å¸¶æœ‰ç¤ºä¾‹çš„æç¤ºæŠ€è¡“çš„è©³ç›¡åˆ—è¡¨ã€‚
* [Outlines - Quickstart](https://outlines-dev.github.io/outlines/quickstart/): Outlineså•Ÿç”¨çš„æŒ‡å°ç”ŸæˆæŠ€è¡“çš„åˆ—è¡¨ã€‚
* [LMQL - Overview](https://lmql.ai/docs/language/overview.html): å°LMQLèªè¨€çš„ä»‹ç´¹ã€‚
---
### 2. å»ºç«‹å‘é‡å„²å­˜

å‰µå»ºå‘é‡å„²å­˜æ˜¯å»ºç«‹æª¢ç´¢å¢å¼·ç”Ÿæˆï¼ˆRetrieval Augmented Generationï¼Œç°¡ç¨±RAGï¼‰æµç¨‹çš„ç¬¬ä¸€æ­¥ã€‚æ–‡ä»¶è¢«åŠ è¼‰ã€æ‹†åˆ†ï¼Œä¸¦ä½¿ç”¨ç›¸é—œçš„ç‰‡æ®µä¾†ç”¢ç”Ÿå‘é‡è¡¨ç¤ºï¼ˆåµŒå…¥ï¼‰ï¼Œé€™äº›å‘é‡è¡¨ç¤ºå°‡è¢«å­˜å„²ä»¥ä¾¿åœ¨æ¨ç†éç¨‹ä¸­ä½¿ç”¨ã€‚

* **æ–‡æª”å°å…¥ Ingesting documents**: æ–‡æª”åŠ è¼‰å™¨æ˜¯æ–¹ä¾¿çš„åŒ…è£å™¨ï¼Œå¯ä»¥è™•ç†å¤šç¨®æ ¼å¼ï¼š PDF, JSON, HTML, Markdown, ç­‰ã€‚ å®ƒå€‘é‚„å¯ä»¥ç›´æ¥å¾ä¸€äº›æ•¸æ“šåº«å’ŒAPIï¼ˆGitHub, Reddit, Google Drive, ç­‰ï¼‰æª¢ç´¢æ•¸æ“šã€‚
* **æ–‡æª”æ‹†åˆ† Splitting documents**: æ–‡æœ¬æ‹†åˆ†å™¨å°‡æ–‡æª”æ‹†åˆ†æˆè¼ƒå°çš„ã€èªç¾©ä¸Šæœ‰æ„ç¾©çš„ç‰‡æ®µã€‚é€šå¸¸æœ€å¥½ä¸è¦åœ¨nå€‹å­—ç¬¦å¾Œæ‹†åˆ†æ–‡æœ¬ï¼Œè€Œæ˜¯æŒ‰ç…§æ¨™é¡Œæˆ–éè¿´åœ°æ‹†åˆ†ï¼Œä¸¦é™„å¸¶ä¸€äº›é¡å¤–çš„å…ƒæ•¸æ“šã€‚
* **åµŒå…¥æ¨¡å‹ Embedding models**: åµŒå…¥æ¨¡å‹å°‡æ–‡æœ¬è½‰æ›ç‚ºå‘é‡è¡¨ç¤ºã€‚é€™å…è¨±å°èªè¨€é€²è¡Œæ›´æ·±å…¥ã€æ›´ç´°è†©çš„ç†è§£ï¼Œé€™å°æ–¼é€²è¡Œèªç¾©æœç´¢è‡³é—œé‡è¦ã€‚
* **å‘é‡æ•¸æ“šåº« Vector databases**: å‘é‡æ•¸æ“šåº«ï¼ˆå¦‚ [Chroma](https://www.trychroma.com/), [Pinecone](https://www.pinecone.io/), [Milvus](https://milvus.io/), [FAISS](https://faiss.ai/), [Annoy](https://github.com/spotify/annoy), ç­‰ï¼‰å°ˆç‚ºå„²å­˜åµŒå…¥å‘é‡è€Œè¨­è¨ˆã€‚å®ƒå€‘æ”¯æ´åŸºæ–¼å‘é‡ç›¸ä¼¼æ€§æœ‰æ•ˆæª¢ç´¢èˆ‡æŸ¥è©¢æœ€ç›¸ä¼¼çš„æ•¸æ“šã€‚

ğŸ“š ** åƒè€ƒè³‡æ–™**:
* [LangChain - Text splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/): LangChainå¯¦ç¾çš„ä¸åŒæ–‡æœ¬æ‹†åˆ†å™¨åˆ—è¡¨ã€‚
* [Sentence Transformers library](https://www.sbert.net/): æµè¡Œçš„åµŒå…¥æ¨¡å‹åº«ã€‚
* [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard): åµŒå…¥æ¨¡å‹çš„æ’è¡Œæ¦œã€‚
* [The Top 5 Vector Databases](https://www.datacamp.com/blog/the-top-5-vector-databases) by Moez Ali: æœ€ä½³å’Œæœ€æµè¡Œçš„å‘é‡æ•¸æ“šåº«æ¯”è¼ƒã€‚

---
### 3. Retrieval Augmented Generation æª¢ç´¢å¢å¼·ç”Ÿæˆ

å€ŸåŠ© RAGï¼ŒLLMs å¯ä»¥å¾è³‡æ–™åº«ä¸­æª¢ç´¢ä¸Šä¸‹æ–‡æ–‡æª”ï¼Œä»¥æé«˜ç­”æ¡ˆçš„æº–ç¢ºæ€§ã€‚RAG æ˜¯ä¸€ç¨®ç„¡éœ€ä»»ä½•å¾®èª¿å³å¯å¢å¼·æ¨¡å‹çŸ¥è­˜çš„æµè¡Œæ–¹æ³•ã€‚

* **Orchestrators å”ä½œå™¨**: Orchestrators å”ä½œå™¨ (å¦‚ [LangChain](https://python.langchain.com/docs/get_started/introduction), [LlamaIndex](https://docs.llamaindex.ai/en/stable/), [FastRAG](https://github.com/IntelLabs/fastRAG), ç­‰ï¼‰æ˜¯æµè¡Œçš„æ¡†æ¶ï¼Œç”¨æ–¼å°‡æ‚¨çš„ LLM èˆ‡å·¥å…·ã€è³‡æ–™åº«ã€è¨˜æ†¶é«”ç­‰é€£æ¥èµ·ä¾†ä¸¦å¢å¼·ä»–å€‘çš„èƒ½åŠ›ã€‚
* **Retrievers æª¢ç´¢å™¨**: ä½¿ç”¨è€…æŒ‡ä»¤æœªé‡å°æª¢ç´¢é€²è¡Œæœ€ä½³åŒ–ã€‚å¯ä»¥æ‡‰ç”¨ä¸åŒçš„æŠ€è¡“ï¼ˆä¾‹å¦‚ï¼Œå¤šæŸ¥è©¢æª¢ç´¢å™¨ã€ [HyDE](https://arxiv.org/abs/2212.10496), ç­‰ï¼‰ä¾†é‡æ–°è¡¨è¿°/æ“´å±•å®ƒå€‘ä¸¦æé«˜æ•ˆèƒ½ã€‚
* **è¨˜æ†¶**: ç‚ºäº†è¨˜ä½å…ˆå‰çš„èªªæ˜å’Œç­”æ¡ˆï¼ŒLLM å’Œ ChatGPT ç­‰èŠå¤©æ©Ÿå™¨äººæœƒå°‡æ­¤æ­·å²è¨˜éŒ„æ·»åŠ åˆ°å…¶ä¸Šä¸‹æ–‡è¦–çª—ä¸­ã€‚æ­¤ç·©è¡å€å¯ä»¥é€éåŒ¯ç¸½ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨è¼ƒå°çš„ LLMï¼‰ã€å‘é‡å„²å­˜ + RAG ç­‰ä¾†æ”¹é€²ã€‚
* **è©•ä¼°**: æˆ‘å€‘éœ€è¦è©•ä¼°æ–‡ä»¶æª¢ç´¢ï¼ˆä¸Šä¸‹æ–‡ç²¾ç¢ºåº¦å’Œå¬å›ç‡ï¼‰å’Œç”Ÿæˆéšæ®µï¼ˆå¯ä¿¡åº¦å’Œç­”æ¡ˆç›¸é—œæ€§ï¼‰ã€‚å¯ä»¥ä½¿ç”¨ [Ragas](https://github.com/explodinggradients/ragas/tree/main) å’Œ [DeepEval](https://github.com/confident-ai/deepeval)å·¥å…·é€²è¡Œç°¡åŒ–ã€‚

ğŸ“š **åƒè€ƒè³‡æ–™**:
* [Llamaindex - High-level concepts](https://docs.llamaindex.ai/en/stable/getting_started/concepts.html): å»ºé€  RAG ç®¡é“æ™‚éœ€è¦äº†è§£çš„ä¸»è¦æ¦‚å¿µã€‚
* [Pinecone - Retrieval Augmentation](https://www.pinecone.io/learn/series/langchain/langchain-retrieval-augmentation/): æª¢ç´¢å¢å¼·æµç¨‹æ¦‚è¿°ã€‚
* [LangChain - Q&A with RAG](https://python.langchain.com/docs/use_cases/question_answering/quickstart): å»ºç«‹å…¸å‹ RAG ç®¡é“çš„é€æ­¥æ•™å­¸ã€‚
* [LangChain - Memory types](https://python.langchain.com/docs/modules/memory/types/): ä¸åŒé¡å‹è¨˜æ†¶é«”åŠå…¶ç›¸é—œç”¨é€”çš„æ¸…å–®ã€‚
* [RAG pipeline - Metrics](https://docs.ragas.io/en/stable/concepts/metrics/index.html): ç”¨æ–¼è©•ä¼° RAG ç®¡é“çš„ä¸»è¦æŒ‡æ¨™çš„æ¦‚è¿°ã€‚
---
### 4. é€²éš RAG

ç¾å¯¦æ‡‰ç”¨ç¨‹å¼å¯èƒ½éœ€è¦è¤‡é›œçš„ç®¡é“ï¼ŒåŒ…æ‹¬ SQL æˆ–åœ–å½¢è³‡æ–™åº«ï¼Œä»¥åŠè‡ªå‹•é¸æ“‡ç›¸é—œå·¥å…·å’Œ APIã€‚é€™äº›å…ˆé€²æŠ€è¡“å¯ä»¥æ”¹é€²åŸºæº–è§£æ±ºæ–¹æ¡ˆä¸¦æä¾›é™„åŠ åŠŸèƒ½ã€‚

* **æŸ¥è©¢å»ºæ§‹**: å„²å­˜åœ¨å‚³çµ±æ•¸æ“šåº«ä¸­çš„çµæ§‹åŒ–æ•¸æ“šéœ€è¦ç‰¹å®šçš„æŸ¥è©¢èªè¨€ï¼Œå¦‚SQLã€Cypherã€å…ƒæ•¸æ“šç­‰ã€‚æˆ‘å€‘å¯ä»¥ç›´æ¥å°‡ç”¨æˆ¶æŒ‡ä»¤ç¿»è­¯æˆæŸ¥è©¢ï¼Œé€šéæŸ¥è©¢å»ºæ§‹ä¾†å­˜å–æ•¸æ“šã€‚
* **ä»£ç†èˆ‡å·¥å…·**: ä»£ç†é€éè‡ªå‹•é¸æ“‡æœ€ç›¸é—œçš„å·¥å…·ä¾†å¢å¼·LLMsçš„å›ç­”èƒ½åŠ›ã€‚é€™äº›å·¥å…·å¯ä»¥åƒä½¿ç”¨Googleæˆ–Wikipediaé‚£éº¼ç°¡å–®ï¼Œæˆ–è€…åƒPythonè§£é‡‹å™¨æˆ–Jiraé€™æ¨£è¤‡é›œã€‚
* **å¾Œè™•ç†**: è™•ç†è¼¸å…¥åˆ°LLMçš„æœ€å¾Œä¸€æ­¥ã€‚å®ƒé€šéé‡æ–°æ’åºã€[RAGèåˆ](https://github.com/Raudaschl/rag-fusion)å’Œåˆ†é¡ä¾†å¢å¼·æª¢ç´¢æ–‡æª”çš„ç›¸é—œæ€§å’Œå¤šæ¨£æ€§ã€‚

ğŸ“š **åƒè€ƒè³‡æ–™**:
* [LangChain - Query ConstructionæŸ¥è©¢å»ºæ§‹](https://blog.langchain.dev/query-construction/): é—œæ–¼ä¸åŒé¡å‹æŸ¥è©¢å»ºæ§‹çš„åšå®¢æ–‡ç« .
* [LangChain - SQL](https://python.langchain.com/docs/use_cases/qa_structured/sql): æ•™ç¨‹ï¼Œä»‹ç´¹å¦‚ä½•åˆ©ç”¨LLMsèˆ‡SQLæ•¸æ“šåº«äº’å‹•ï¼ŒåŒ…æ‹¬Text-to-SQLå’Œå¯é¸çš„SQLä»£ç†ã€‚
* [Pinecone - LLM agents(ä»£ç†)](https://www.pinecone.io/learn/series/langchain/langchain-agents/): ä»‹ç´¹ä¸åŒé¡å‹çš„ä»£ç†å’Œå·¥å…·ã€‚
* [LLM Powered Autonomous Agents(ä»£ç†)](https://lilianweng.github.io/posts/2023-06-23-agent/) by Lilian Weng: é—œæ–¼LLMä»£ç†çš„æ›´ç†è«–æ€§æ–‡ç« ã€‚
* [LangChain - OpenAI's RAG](https://blog.langchain.dev/applying-openai-rag/): æ¦‚è¿°OpenAIä½¿ç”¨çš„RAGç­–ç•¥ï¼ŒåŒ…æ‹¬å¾Œè™•ç†ã€‚

---
### 5. Inference optimization æ¨ç†å„ªåŒ–

æ–‡æœ¬ç”Ÿæˆæ˜¯ä¸€å€‹æˆæœ¬é«˜æ˜‚çš„éç¨‹ï¼Œéœ€è¦æ˜‚è²´çš„ç¡¬é«”è¨­å‚™ã€‚é™¤äº†é‡åŒ–ä¹‹å¤–ï¼Œé‚„æœ‰å„ç¨®æŠ€è¡“è¢«æå‡ºä»¥æœ€å¤§åŒ–ååé‡ä¸¦é™ä½æ¨è«–æˆæœ¬ã€‚

* **Flash Attention é–ƒå­˜æ³¨æ„åŠ›**: å„ªåŒ–æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼Œå°‡å…¶è¤‡é›œæ€§å¾äºŒæ¬¡æ–¹è®Šæˆç·šæ€§ä»¥åŠ å¿«è¨“ç·´å’Œæ¨è«–é€Ÿåº¦ã€‚
* **Key-value cache éµå€¼å¿«å–**: è«‹å¤šäº†è§£éµå€¼å¿«å–ä»¥åŠå¤šæŸ¥è©¢æ³¨æ„åŠ›ï¼ˆ[Multi-Query Attention](https://arxiv.org/abs/1911.02150) (MQA)ï¼‰å’Œ(åˆ†çµ„æŸ¥è©¢æ³¨æ„åŠ›[Grouped-Query Attention](https://arxiv.org/abs/2305.13245) (GQA)ï¼‰å¸¶ä¾†çš„æ”¹é€²ã€‚
* **Speculative decoding æŠ•æ©Ÿè§£ç¢¼**: ä½¿ç”¨å°å‹æ¨¡å‹ç”¢ç”Ÿè‰ç¨¿ï¼Œç„¶å¾Œç”±æ›´å¤§çš„æ¨¡å‹å¯©æ ¸ï¼Œä»¥åŠ å¿«æ–‡æœ¬ç”Ÿæˆé€Ÿåº¦ã€‚

ğŸ“š **åƒè€ƒè³‡æ–™**:
* [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one) by Hugging Face: è§£é‡‹å¦‚ä½•åœ¨GPUä¸Šå„ªåŒ–æ¨è«–.
* [LLM Inference](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices) by Databricks: å¯¦éš›é‹ä½œä¸­å„ªåŒ–LLMæ¨è«–çš„æœ€ä½³å¯¦è¸ã€‚
* [Optimizing LLMs for Speed and Memory](https://huggingface.co/docs/transformers/main/en/llm_tutorial_optimization) by Hugging Face: è§£é‡‹ä¸‰ç¨®ä¸»è¦çš„é€Ÿåº¦å’Œè¨˜æ†¶é«”å„ªåŒ–æŠ€è¡“ï¼Œå³é‡åŒ–ã€é–ƒå­˜æ³¨æ„åŠ›å’Œæ¶æ§‹å‰µæ–°ã€‚
* [Assisted Generation](https://huggingface.co/blog/assisted-generation) by Hugging Face: HFç‰ˆæœ¬çš„æŠ•æ©Ÿè§£ç¢¼ï¼Œé€™æ˜¯ä¸€ç¯‡æœ‰è¶£çš„åšå®¢æ–‡ç« ï¼Œä»‹ç´¹äº†å®ƒçš„å·¥ä½œåŸç†åŠå…¶å¯¦ç¾ä»£ç¢¼ã€‚

---
### 6. Deploying LLMs éƒ¨ç½²å¤§å‹èªè¨€æ¨¡å‹

åœ¨å¤§è¦æ¨¡éƒ¨ç½²å¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ˜¯ä¸€é …å·¥ç¨‹å£¯èˆ‰ï¼Œå¯èƒ½éœ€è¦å¤šå€‹GPUé›†ç¾¤ã€‚åœ¨å…¶ä»–æƒ…æ™¯ä¸‹ï¼Œæ¼”ç¤ºå’Œæœ¬åœ°æ‡‰ç”¨å¯ä»¥æ›´ç°¡å–®çš„å¯¦ç¾é‹ä½œã€‚

* **Local deployment æœ¬åœ°éƒ¨ç½²**: éš±ç§æ˜¯é–‹æºLLMsç›¸å°æ–¼ç§æœ‰LLMsçš„ä¸€å€‹é‡è¦å„ªå‹¢ã€‚ æœ¬åœ°LLMæœå‹™å™¨ ([LM Studio](https://lmstudio.ai/), [Ollama](https://ollama.ai/), [oobabooga](https://github.com/oobabooga/text-generation-webui), [kobold.cpp](https://github.com/LostRuins/koboldcpp), ç­‰ï¼‰åˆ©ç”¨é€™ä¸€å„ªå‹¢ç‚ºæœ¬åœ°æ‡‰ç”¨æä¾›å‹•åŠ›ã€‚ 
* **Demo deployment æ¼”ç¤ºéƒ¨ç½²**:åƒ [Gradio](https://www.gradio.app/) å’Œ [Streamlit](https://docs.streamlit.io/) é€™æ¨£çš„æ¡†æ¶æœ‰åŠ©æ–¼åŸå‹æ‡‰ç”¨çš„é–‹ç™¼å’Œæ¼”ç¤ºçš„åˆ†äº«ã€‚æ‚¨ä¹Ÿå¯ä»¥è¼•é¬†åœ°åœ¨ç·šä¸Šéƒ¨ç½²ï¼Œä¾‹å¦‚ä½¿ç”¨ [Hugging Face Spaces](https://huggingface.co/spaces)ã€‚ 
* **Server deployment æœå‹™å™¨éƒ¨ç½²**: å¤§è¦æ¨¡éƒ¨ç½²LLMséœ€è¦é›²ç«¯ (è©³è¦‹ [SkyPilot](https://skypilot.readthedocs.io/en/latest/)) æˆ–å…§éƒ¨éƒ¨ç½²çš„åŸºç¤è¨­æ–½ï¼Œä¸¦ç¶“å¸¸åˆ©ç”¨å„ªåŒ–çš„æ–‡æœ¬ç”Ÿæˆæ¡†æ¶ï¼Œå¦‚ [TGI](https://github.com/huggingface/text-generation-inference), [vLLM](https://github.com/vllm-project/vllm/tree/main)ç­‰ã€‚
* **Edge deployment é‚Šç·£(ä¸­ä½ç®—åŠ›)éƒ¨ç½²**: åœ¨å—é™ç’°å¢ƒä¸­ï¼Œé«˜æ€§èƒ½æ¡†æ¶å¦‚ [MLC LLM](https://github.com/mlc-ai/mlc-llm) å’Œ [mnn-llm](https://github.com/wangzhaode/mnn-llm/blob/master/README_en.md) å¯ä»¥åœ¨ç¶²é ç€è¦½å™¨ã€Androidå’ŒiOSä¸­éƒ¨ç½²LLMã€‚

ğŸ“š **åƒè€ƒè³‡æ–™**:
* [Streamlit - Build a basic LLM app](https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps): ä½¿ç”¨Streamlitå‰µå»ºé¡ä¼¼ChatGPTçš„åŸºç¤æ‡‰ç”¨çš„æ•™å­¸ã€‚
* [HF LLM Inference Container](https://huggingface.co/blog/sagemaker-huggingface-llm): Dä½¿ç”¨Hugging Faceçš„æ¨è«–å®¹å™¨åœ¨Amazon SageMakerä¸Šéƒ¨ç½²LLMsã€‚
* [PhilschmidÂ blog](https://www.philschmid.de/) by Philipp Schmid: é—œæ–¼ä½¿ç”¨Amazon SageMakeréƒ¨ç½²LLMçš„é«˜è³ªé‡æ–‡ç« é›†ã€‚
* [Optimizing latence å„ªåŒ–å»¶é²](https://hamel.dev/notes/llm/inference/03_inference.html) by Hamel Husain:æ¯”è¼ƒTGIã€vLLMã€CTranslate2å’Œmlcåœ¨ååé‡å’Œå»¶é²æ–¹é¢çš„æ€§èƒ½ã€‚

---
### 7. Securing LLMs 

é™¤äº†èˆ‡è»Ÿé«”ç›¸é—œçš„å‚³çµ±å®‰å…¨å•é¡Œå¤–ï¼Œç”±æ–¼è¨“ç·´å’Œæç¤ºçš„æ–¹å¼ï¼Œå¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMsï¼‰é‚„æœ‰ç‰¹å®šçš„å¼±é»ã€‚

* **Prompt hacking æç¤ºè©æ”»æ“Š**: èˆ‡æç¤ºå·¥ç¨‹ç›¸é—œçš„æŠ€è¡“ç•¥æœ‰ä¸åŒï¼Œæç¤ºè©æ³¨å…¥ï¼ˆä½¿ç”¨é¡å¤–æŒ‡ä»¤ä»¥åŠ«æŒæ¨¡å‹çš„ç­”æ¡ˆï¼‰ã€æ•¸æ“š/æç¤ºæ´©æ¼ï¼ˆæª¢ç´¢å…¶åŸå§‹æ•¸æ“š/æç¤ºï¼‰å’Œè¶Šç„ï¼ˆè£½ä½œæç¤ºè©ä»¥ç¹éå®‰å…¨ç‰¹æ€§ï¼‰éƒ½ç®—åœ¨æ­¤ç¯„åœå…§ã€‚
* **Backdoors å¾Œé–€**:  æ”»æ“Šå‘é‡å¯ä»¥é‡å°è¨“ç·´æ•¸æ“šæœ¬èº«ï¼Œé€šéæ±¡æŸ“è¨“ç·´æ•¸æ“šï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨éŒ¯èª¤ä¿¡æ¯ï¼‰æˆ–å‰µå»ºå¾Œé–€ï¼ˆè§¸ç™¼å™¨åœ¨æ¨è«–æœŸé–“ç§˜å¯†çš„æ”¹è®Šæ¨¡å‹è¡Œç‚ºï¼‰ã€‚
* **Defensive measures é˜²ç¦¦æªæ–½**: ä¿è­·æ‚¨çš„LLMæ‡‰ç”¨ç¨‹åºçš„æœ€ä½³æ–¹å¼æ˜¯å°é€™äº›æ¼æ´é€²è¡Œæ¸¬è©¦ (e.g., ä¾‹å¦‚ï¼Œä½¿ç”¨ç´…éšŠæ¸¬è©¦å’Œåƒ[garak](https://github.com/leondz/garak/)é€™æ¨£çš„æª¢æŸ¥ ) ä¸¦åœ¨å¯¦éš›çš„ç’°å¢ƒä¸­è§€å¯Ÿå®ƒå€‘ï¼ˆä½¿ç”¨åƒ[langfuse](https://github.com/langfuse/langfuse)é€™æ¨£çš„æ¡†æ¶ï¼‰ã€‚

ğŸ“š **References**:
* [OWASP LLM Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/) by HEGO Wiki: LLMæ‡‰ç”¨ç¨‹åºä¸­10å€‹æœ€åš´é‡çš„æ¼æ´æ¸…å–®ã€‚
* [Prompt Injection Primer](https://github.com/jthack/PIPE) by Joseph Thacker: å°ˆé–€é‡å°å·¥ç¨‹å¸«çš„æç¤ºæ³¨å…¥çŸ­æŒ‡å—ã€‚
* [LLM Security](https://llmsecurity.net/) by [@llm_sec](https://twitter.com/llm_sec): èˆ‡LLMå®‰å…¨ç›¸é—œçš„å»£æ³›è³‡æºåˆ—è¡¨ã€‚
* [Red teaming LLMs](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/red-teaming) by Microsoft: é—œæ–¼å¦‚ä½•åŸ·è¡ŒLLMç´…éšŠæ¸¬è©¦çš„æŒ‡å—ã€‚
## Acknowledgements

This roadmap was inspired by the excellent [DevOps Roadmap](https://github.com/milanm/DevOps-Roadmap) from Milan MilanoviÄ‡ and Romano Roth.

Special thanks to:

* Thomas Thelen for motivating me to create a roadmap
* AndrÃ© Frade for his input and review of the first draft
* Dino Dunn for providing resources about LLM security

*Disclaimer: I am not affiliated with any sources listed here.*

---
<p align="center">
  <a href="https://star-history.com/#mlabonne/llm-course&Date">
    <img src="https://api.star-history.com/svg?repos=mlabonne/llm-course&type=Date" alt="Star History Chart">
  </a>
</p>

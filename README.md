<div align="center">
  <h1>ğŸ—£ï¸ å¤§èªè¨€æ¨¡å‹èª²ç¨‹å¿ƒå¾—</h1>
  <p align="center">
    ğŸ¦ <a href="https://twitter.com/maximelabonne">Follow me on X</a> â€¢ 
    ğŸ¤— <a href="https://huggingface.co/mlabonne">Hugging Face</a> â€¢ 
    ğŸ’» <a href="https://mlabonne.github.io/blog">Blog</a> â€¢ 
    ğŸ“™ <a href="https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python">Hands-on GNN</a> â€¢ 
    ğŸ—£ï¸ <a href="https://chat.openai.com/g/g-yviLuLqvI-llm-course">Interactive GPT</a>
  </p>
</div>
<br/>

æ­¤ LLM èª²ç¨‹å¿ƒå¾—åˆ†æˆä¸‰å€‹éƒ¨åˆ†:

1. ğŸ§© **LLM åŸºç¤** æ¶µè“‹äº†æ•¸å­¸,Python å’Œç¥ç¶“ç¶²è·¯ç­‰åŸºç¤çŸ¥è­˜.
2. ğŸ§‘â€ğŸ”¬ **LLM æ¨¡å‹å·¥ç¨‹** å°ˆæ³¨åœ¨ä½¿ç”¨æœ€æ–°çš„æŠ€å·§,æŠ€è¡“æ­å»ºç¾éšæ®µå€‹äººå¯å¯¦ç¾æœ€å¥½çš„LLMs.
3. ğŸ‘· **LLM æ‡‰ç”¨å·¥ç¨‹** å°ˆæ³¨åœ¨å‰µå»º LLM é©…å‹•çš„æ‡‰ç”¨ä»¥åŠéƒ¨ç½².

## ğŸ“ Notebooks

èˆ‡å¤§å‹èªè¨€æ¨¡å‹ç›¸é—œçš„ç­†è¨˜æœ¬å’Œæ–‡ç« æ¸…å–®ã€‚

### Tools

| åç¨± | æ•˜è¿° | é€£çµ |
|----------|-------------|----------|
| ğŸ§ [LLM AutoEval](https://github.com/mlabonne/llm-autoeval) | ä½¿ç”¨ RunPod è‡ªå‹•è©•ä¼°æ‚¨çš„LLMs | <a href="https://colab.research.google.com/drive/1Igs3WZuXAIv9X0vwqiE90QlEPys8e8Oa?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |
| ğŸ¥± LazyMergekit | ä½¿ç”¨ mergekit ä¸€éµè¼•é¬†åˆä½µæ¨¡å‹. | <a href="https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |
| âš¡ AutoGGUF | ä¸€éµé‡åŒ– GGUF æ ¼å¼çš„ LLM. | <a href="https://colab.research.google.com/drive/1P646NEg33BZy4BfLDNpTz0V0lwIU3CHu?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |
| ğŸŒ³ Model Family Tree | å¯è¦–åŒ–åˆä½µæ¨¡å‹çš„æ¨¹ç‹€çµæ§‹åœ–. | <a href="https://colab.research.google.com/drive/1s2eQlolcI1VGgDhqWIANfkfKvcKrMyNr?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |

### Fine-tuning (å¾®èª¿)

| åç¨± | æ•˜è¿° | æ–‡ç«  | é€£çµ |
|---------------------------------------|-------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|
| Fine-tune Llama 2 in Google Colab | å¾®èª¿æ‚¨çš„ç¬¬ä¸€å€‹ Llama 2 æ¨¡å‹çš„é€æ­¥æŒ‡å—ã€‚ | [Article](https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html) | <a href="https://colab.research.google.com/drive/1PEQyJO1-f6j0S_XJ8DV50NkpzasXkrzd?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |
| Fine-tune LLMs with Axolotl | æœ€å…ˆé€²å¾®èª¿å·¥å…·çš„ç«¯åˆ°ç«¯æŒ‡å—ã€‚| [Article](https://mlabonne.github.io/blog/posts/A_Beginners_Guide_to_LLM_Finetuning.html) | <a href="https://colab.research.google.com/drive/1Xu0BrCB7IShwSWKVcfAfhehwjDrDMH5m?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |
| Fine-tune Mistral-7b with DPO | ä½¿ç”¨ DPO æå‡ç›£ç£å¾®èª¿æ¨¡å‹çš„æ€§èƒ½ | [Article](https://medium.com/towards-data-science/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac) | <a href="https://colab.research.google.com/drive/15iFBr1xWgztXvhrj5I9fBv20c7CFOPBE?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |

### Quantization(é‡åŒ–)

| åç¨± | æ•˜è¿° | æ–‡ç«  | é€£çµ |
|---------------------------------------|-------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|
| 1. Introduction to Quantization | ä½¿ç”¨ 8 ä½å…ƒé‡åŒ–çš„å¤§å‹èªè¨€æ¨¡å‹æœ€ä½³åŒ–ã€‚ | [Article](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html) | <a href="https://colab.research.google.com/drive/1DPr4mUQ92Cc-xf4GgAaB6dFcFnWIvqYi?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |
| 2. 4-bit Quantization using GPTQ | é‡åŒ–æ‚¨è‡ªå·±çš„é–‹æº LLM ä»¥åœ¨æ¶ˆè²»æ€§ç¡¬é«”ä¸Šé‹è¡Œå®ƒå€‘ã€‚ | [Article](https://mlabonne.github.io/blog/4bit_quantization/) | <a href="https://colab.research.google.com/drive/1lSvVDaRgqQp_mWK_jC9gydz6_-y6Aq4A?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |
| 3. Quantization with GGUF and llama.cpp | ä½¿ç”¨ llama.cpp é‡åŒ– Llama 2 æ¨¡å‹ä¸¦å°‡ GGUF ç‰ˆæœ¬ä¸Šå‚³åˆ° HF Hubã€‚ | [Article](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html) | <a href="https://colab.research.google.com/drive/1pL8k7m04mgE5jo2NrjGi8atB0j_37aDD?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |
| 4. ExLlamaV2: The Fastest Library to RunÂ LLMs | é‡åŒ–ä¸¦åŸ·è¡Œ EXL2 æ¨¡å‹ä¸¦å°‡å…¶ä¸Šå‚³è‡³ HF Hubã€‚| [Article](https://mlabonne.github.io/blog/posts/ExLlamaV2_The_Fastest_Library_to_Run%C2%A0LLMs.html) | <a href="https://colab.research.google.com/drive/1yrq4XBlxiA0fALtMoT2dwiACVc77PHou?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |

### å…¶ä»–

| åç¨± | æ•˜è¿° | æ–‡ç«  | é€£çµ |
|---------------------------------------|-------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|
| Decoding Strategies in Large Language Models | å¾æ³¢æŸæœå°‹(beam search)åˆ°æ ¸æ¡æ¨£(nucleus sampling)çš„æ–‡æœ¬ç”ŸæˆæŒ‡å—| [Article](https://mlabonne.github.io/blog/posts/2022-06-07-Decoding_strategies.html) | <a href="https://colab.research.google.com/drive/19CJlOS5lI29g-B3dziNn93Enez1yiHk2?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |
| Visualizing GPT-2's Loss Landscape | åŸºæ–¼æ¬Šé‡æ“¾å‹•çš„æå¤±æ™¯è§€ä¸‰ç¶­åœ–(3D plot of the loss landscape based on weight perturbations.)| [Tweet](https://twitter.com/maximelabonne/status/1667618081844219904) | <a href="https://colab.research.google.com/drive/1Fu1jikJzFxnSPzR_V2JJyDVWWJNXssaL?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |
| Improve ChatGPT with Knowledge Graphs | ç”¨çŸ¥è­˜åœ–è­œå¢å¼· ChatGPT çš„ç­”æ¡ˆ | [Article](https://mlabonne.github.io/blog/posts/Article_Improve_ChatGPT_with_Knowledge_Graphs.html) | <a href="https://colab.research.google.com/drive/1mwhOSw9Y9bgEaIFKT4CLi0n18pXRM4cj?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |
| Merge LLMs with mergekit | è¼•é¬†å‰µå»ºæ‚¨è‡ªå·±çš„æ¨¡å‹ï¼Œç„¡éœ€ GPUï¼| [Article](https://towardsdatascience.com/merge-large-language-models-with-mergekit-2118fb392b54) | <a href="https://colab.research.google.com/drive/1_JS7JKJAQozD48-LhYdegcuuZ2ddgXfr?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |


## ğŸ§© LLM åŸºç¤

![](img/roadmap_fundamentals.png)

### 1. æ©Ÿå™¨å­¸ç¿’æ•¸å­¸

åœ¨æŒæ¡æ©Ÿå™¨å­¸ç¿’ä¹‹å‰ï¼Œäº†è§£æ”¯æ’äº†é€™äº›æ¼”ç®—æ³•çš„åŸºæœ¬æ•¸å­¸æ¦‚å¿µéå¸¸é‡è¦ã€‚

- **ç·šæ€§ä»£æ•¸**: é€™å°æ–¼ç†è§£è¨±å¤šæ¼”ç®—æ³•è‡³é—œé‡è¦ï¼Œå°¤å…¶æ˜¯æ·±åº¦å­¸ç¿’ä¸­ä½¿ç”¨çš„æ¼”ç®—æ³•ã€‚é—œéµæ¦‚å¿µåŒ…æ‹¬å‘é‡ã€çŸ©é™£ã€è¡Œåˆ—å¼ã€ç‰¹å¾µå€¼å’Œç‰¹å¾µå‘é‡ã€å‘é‡ç©ºé–“å’Œç·šæ€§è®Šæ›ã€‚
- **å¾®ç©åˆ†**: è¨±å¤šæ©Ÿå™¨å­¸ç¿’æ¼”ç®—æ³•æ¶‰åŠé€£çºŒå‡½æ•¸çš„æœ€ä½³åŒ–ï¼Œé€™éœ€è¦äº†è§£å°æ•¸ã€ç©åˆ†ã€æ¥µé™å’Œç´šæ•¸ã€‚å¦å¤–å¤šè®Šé‡å¾®ç©åˆ†å’Œæ¢¯åº¦çš„æ¦‚å¿µä¹Ÿå¾ˆé‡è¦ã€‚
- **æ©Ÿç‡å’Œçµ±è¨ˆ**: é€™äº›å°æ–¼ç†è§£æ¨¡å‹å¦‚ä½•å¾æ•¸æ“šä¸­å­¸ç¿’ä¸¦åšå‡ºé æ¸¬è‡³é—œé‡è¦ã€‚ é—œéµæ¦‚å¿µåŒ…æ‹¬æ©Ÿç‡è«–ã€éš¨æ©Ÿè®Šæ•¸ã€æ©Ÿç‡åˆ†ä½ˆã€æœŸæœ›ã€è®Šç•°æ•¸ã€å”æ–¹å·®ã€ç›¸é—œæ€§ã€å‡è¨­æª¢å®šã€ä¿¡è³´å€é–“ã€æœ€å¤§ä¼¼ç„¶ä¼°è¨ˆå’Œè²è‘‰æ–¯æ¨ç†ã€‚

ğŸ“š è³‡æº:

- [3Blue1Brown - ç·šæ€§ä»£æ•¸çš„æœ¬è³ª](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab): æ­¤ç³»åˆ—çš„å½±ç‰‡ä»‹ç´¹å¹¾ä½•ç›¸é—œçš„æ¦‚å¿µ
- [StatQuest with Josh Starmer - çµ±è¨ˆåŸºç¤çŸ¥è­˜](https://www.youtube.com/watch?v=qBigTkBLU6g&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9): ç‚ºè¨±å¤šçµ±è¨ˆæ¦‚å¿µæä¾›ç°¡å–®æ˜äº†çš„è§£é‡‹ã€‚
- [Aerinå¥³å£«çš„APçµ±è¨ˆç›´è§€ç†è§£](https://automata88.medium.com/list/cacc224d5e7d): æä¾›æ¯å€‹æ©Ÿç‡åˆ†ä½ˆèƒŒå¾Œçš„Mediumæ–‡ç« æ¸…å–®ã€‚
- [æ²‰æµ¸å¼ç·šæ€§ä»£æ•¸](https://immersivemath.com/ila/learnmore.html): ç·šæ€§ä»£æ•¸çš„å¦ä¸€ç¨®åœ–åƒåŒ–è©®é‡‹.
- [Khan Academy - ç·šæ€§ä»£æ•¸](https://www.khanacademy.org/math/linear-algebra): éå¸¸é©åˆåˆå­¸è€…ï¼Œå› ç‚ºå®ƒä»¥éå¸¸ç›´è§€çš„æ–¹å¼è§£é‡‹äº†æ¦‚å¿µã€‚
- [Khan Academy - å¾®ç©åˆ†](https://www.khanacademy.org/math/calculus-1): ä¸€é–€æ¶µè“‹å¾®ç©åˆ†æ‰€æœ‰åŸºç¤çŸ¥è­˜çš„äº’å‹•èª²ç¨‹ã€‚
- [Khan Academy - æ©Ÿç‡èˆ‡çµ±è¨ˆ](https://www.khanacademy.org/math/statistics-probability): ä»¥æ˜“æ–¼ç†è§£çš„æ ¼å¼æä¾›ææ–™ã€‚
---

### 2. ç”¨æ–¼æ©Ÿå™¨å­¸ç¿’çš„Python

Python æ˜¯ä¸€ç¨®å¼·å¤§è€Œéˆæ´»çš„ç¨‹å¼èªè¨€ï¼Œç”±æ–¼å…¶å¯è®€æ€§ã€ä¸€è‡´æ€§å’Œå¼·å¤§çš„è³‡æ–™ç§‘å­¸åº«ç”Ÿæ…‹ç³»çµ±ï¼Œç‰¹åˆ¥é©åˆæ©Ÿå™¨å­¸ç¿’ã€‚


- **PythonåŸºç¤**: Pythonç¨‹å¼è¨­è¨ˆéœ€è¦å¾ˆå¥½åœ°ç†è§£åŸºæœ¬èªæ³•ã€è³‡æ–™é¡å‹ã€éŒ¯èª¤è™•ç†å’Œç‰©ä»¶å°å‘ç¨‹å¼è¨­è¨ˆã€‚
- **è³‡æ–™ç§‘å­¸å‡½å¼åº«**: åŒ…æ‹¬ç†Ÿæ‚‰ç”¨æ–¼æ•¸å€¼é‹ç®—çš„ NumPyã€ç”¨æ–¼è³‡æ–™æ“ä½œå’Œåˆ†æçš„ Pandasã€ç”¨æ–¼è³‡æ–™è¦–è¦ºåŒ–çš„ Matplotlib å’Œ Seabornã€‚
- **è³‡æ–™é è™•ç†**: é€™æ¶‰åŠç‰¹å¾µç¸®æ”¾å’Œæ¨™æº–åŒ–ã€è™•ç†ç¼ºå¤±è³‡æ–™ã€ç•°å¸¸å€¼æª¢æ¸¬ã€åˆ†é¡è³‡æ–™ç·¨ç¢¼ä»¥åŠå°‡è³‡æ–™æ‹†åˆ†ç‚ºè¨“ç·´é›†ã€é©—è­‰é›†å’Œæ¸¬è©¦é›†ã€‚
- **æ©Ÿå™¨å­¸ç¿’å‡½å¼åº«**: ç†Ÿç·´ä½¿ç”¨ Scikit-learnï¼ˆä¸€å€‹æä¾›å¤šç¨®ç›£ç£å’Œéç›£ç£å­¸ç¿’æ¼”ç®—æ³•çš„å‡½å¼åº«ï¼‰è‡³é—œé‡è¦ã€‚äº†è§£å¦‚ä½•å¯¦ç¾ç·šæ€§è¿´æ­¸ã€é‚è¼¯è¿´æ­¸ã€æ±ºç­–æ¨¹ã€éš¨æ©Ÿæ£®æ—ã€k æœ€è¿‘é„° (K-NN) å’Œ K å‡å€¼èšé¡ç­‰æ¼”ç®—æ³•éå¸¸é‡è¦ã€‚PCA å’Œ t-SNE ç­‰é™ç¶­æŠ€è¡“ä¹Ÿæœ‰åŠ©æ–¼è¦–è¦ºåŒ–é«˜ç¶­åº¦è³‡æ–™ã€‚

ğŸ“š è³‡æºï¼š

- [Real Python](https://realpython.com/): ç¶œåˆè³‡æºï¼ŒåŒ…å«åˆå­¸è€…å’Œé€²éš Python æ¦‚å¿µçš„æ–‡ç« å’Œæ•™å­¸ã€‚

- [freeCodeCamp - å­¸ç¿’ Python](https://www.youtube.com/watch?v=rfscVS0vtbw): é•·å½±ç‰‡ï¼Œå®Œæ•´ä»‹ç´¹äº† Python ä¸­çš„æ‰€æœ‰æ ¸å¿ƒæ¦‚å¿µã€‚
- [Python è³‡æ–™ç§‘å­¸æ‰‹å†Š](https://jakevdp.github.io/PythonDataScienceHandbook/): å…è²»çš„æ•¸ä½æ›¸ç±ï¼Œæ˜¯å­¸ç¿’ pandasã€NumPyã€Matplotlib å’Œ Seaborn çš„çµ•ä½³è³‡æºã€‚
- [freeCodeCamp - é©åˆæ‰€æœ‰äººçš„æ©Ÿå™¨å­¸ç¿’](https://youtu.be/i_LwzRVP7bg): ç‚ºåˆå­¸è€…ä»‹ç´¹ä¸åŒçš„æ©Ÿå™¨å­¸ç¿’æ¼”ç®—æ³•ã€‚
- [Udacity - æ©Ÿå™¨å­¸ç¿’ç°¡ä»‹](https://www.udacity.com/course/intro-to-machine-learning--ud120): å…è²»èª²ç¨‹ï¼Œæ¶µè“‹ PCA å’Œå…¶ä»–å¹¾å€‹æ©Ÿå™¨å­¸ç¿’æ¦‚å¿µã€‚

---

### 3. ç¥ç¶“ç¶²çµ¡

ç¥ç¶“ç¶²è·¯æ˜¯è¨±å¤šæ©Ÿå™¨å­¸ç¿’æ¨¡å‹çš„åŸºæœ¬çµ„æˆéƒ¨åˆ†ï¼Œç‰¹åˆ¥æ˜¯åœ¨æ·±åº¦å­¸ç¿’é ˜åŸŸã€‚ç‚ºäº†æœ‰æ•ˆåœ°åˆ©ç”¨å®ƒå€‘ï¼Œå…¨é¢äº†è§£å®ƒå€‘çš„è¨­è¨ˆå’Œæ©Ÿåˆ¶è‡³é—œé‡è¦ã€‚

- **åŸºç¤çŸ¥è­˜**: é€™åŒ…æ‹¬ç†è§£ç¥ç¶“ç¶²è·¯çš„çµæ§‹ï¼Œä¾‹å¦‚å±¤ã€æ¬Šé‡ã€åå·®å’Œæ¿€æ´»å‡½æ•¸ï¼ˆsigmoidã€tanhã€ReLU ç­‰ï¼‰
- **è¨“ç·´èˆ‡æœ€ä½³åŒ–**: ç†Ÿæ‚‰åå‘å‚³æ’­å’Œä¸åŒé¡å‹çš„æå¤±å‡½æ•¸ï¼Œä¾‹å¦‚å‡æ–¹èª¤å·® (MSE) å’Œäº¤å‰ç†µã€‚äº†è§£å„ç¨®æœ€ä½³åŒ–æ¼”ç®—æ³•ï¼Œä¾‹å¦‚æ¢¯åº¦ä¸‹é™ã€éš¨æ©Ÿæ¢¯åº¦ä¸‹é™ã€RMSprop å’Œ Adamã€‚
- **éåº¦æ“¬åˆ**: äº†è§£éåº¦æ“¬åˆçš„æ¦‚å¿µï¼ˆæ¨¡å‹åœ¨è¨“ç·´è³‡æ–™ä¸Šè¡¨ç¾è‰¯å¥½ï¼Œä½†åœ¨æœªè¦‹éçš„è³‡æ–™ä¸Šè¡¨ç¾ä¸ä½³ï¼‰ä¸¦å­¸ç¿’å„ç¨®æ­£å‰‡åŒ–æŠ€è¡“ï¼ˆdropoutã€L1/L2 æ­£å‰‡åŒ–ã€æå‰åœæ­¢ã€è³‡æ–™å¢å¼·ï¼‰ä¾†é˜²æ­¢éåº¦æ“¬åˆã€‚
- **å¯¦ä½œå¤šå±¤æ„ŸçŸ¥å™¨ (MLP)**: ä½¿ç”¨ PyTorch å»ºæ§‹ MLPï¼Œä¹Ÿç¨±ç‚ºå…¨é€£æ¥ç¶²è·¯ã€‚
  
ğŸ“š è³‡æº:

- [3Blue1Brown - ä»€éº¼æ˜¯ç¥ç¶“ç¶²è·¯ï¼Ÿ](https://www.youtube.com/watch?v=aircAruvnKk): è©²å½±ç‰‡ç›´è§€åœ°è§£é‡‹äº†ç¥ç¶“ç¶²è·¯åŠå…¶å…§éƒ¨é‹ä½œåŸç†ã€‚
- [freeCodeCamp - æ·±åº¦å­¸ç¿’é€Ÿæˆèª²ç¨‹](https://www.youtube.com/watch?v=VyWAvY2CF9c): æ­¤å½±ç‰‡ç°¡æ½”åœ°ä»‹ç´¹äº†æ·±åº¦å­¸ç¿’ä¸­æ‰€æœ‰æœ€é‡è¦çš„æ¦‚å¿µã€‚
- [Fast.ai - å¯¦ç”¨æ·±åº¦å­¸ç¿’](https://course.fast.ai/): ç‚ºå…·æœ‰ç¨‹å¼è¨­è¨ˆç¶“é©—ã€æƒ³è¦äº†è§£æ·±åº¦å­¸ç¿’çš„äººè¨­è¨ˆçš„å…è²»èª²ç¨‹ã€‚
- [Patrick Loeber - PyTorch æ•™å­¸](https://www.youtube.com/playlist?list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4): ç‚ºåˆå­¸è€…å­¸ç¿’ PyTorch çš„ç³»åˆ—å½±ç‰‡ã€‚

---

### 4. è‡ªç„¶èªè¨€è™•ç†(NLP)

NLP æ˜¯äººå·¥æ™ºæ…§çš„ä¸€å€‹ä»¤äººè‘—è¿·çš„åˆ†æ”¯ï¼Œå®ƒå½Œåˆäº†äººé¡èªè¨€å’Œæ©Ÿå™¨ç†è§£ä¹‹é–“çš„å·®è·ã€‚å¾ç°¡å–®çš„æ–‡å­—è™•ç†åˆ°ç†è§£èªè¨€çš„ç´°å¾®å·®åˆ¥ï¼ŒNLP åœ¨ç¿»è­¯ã€æƒ…ç·’åˆ†æã€èŠå¤©æ©Ÿå™¨äººç­‰è¨±å¤šæ‡‰ç”¨ä¸­ç™¼æ®è‘—è‡³é—œé‡è¦çš„ä½œç”¨ã€‚

- **æ–‡å­—é è™•ç†**: å­¸ç¿’å„ç¨®æ–‡å­—é è™•ç†æ­¥é©Ÿï¼Œä¾‹å¦‚åˆ†è©ï¼ˆå°‡æ–‡å­—åˆ†å‰²æˆå–®å­—æˆ–å¥å­ï¼‰ã€è©å¹¹æ“·å–ï¼ˆå°‡å–®å­—é‚„åŸç‚ºå…¶è©æ ¹å½¢å¼ï¼‰ã€è©å½¢é‚„åŸï¼ˆèˆ‡è©å¹¹æ“·å–é¡ä¼¼ï¼Œä½†è€ƒæ…®ä¸Šä¸‹æ–‡ï¼‰ã€åœç”¨è©åˆªé™¤ç­‰ã€‚
- **ç‰¹å¾µæå–æŠ€è¡“**: ç†Ÿæ‚‰å°‡æ–‡å­—è³‡æ–™è½‰æ›ç‚ºæ©Ÿå™¨å­¸ç¿’æ¼”ç®—æ³•å¯ä»¥ç†è§£çš„æ ¼å¼çš„æŠ€è¡“ã€‚ä¸»è¦æ–¹æ³•åŒ…æ‹¬è©è¢‹ (BoW)ã€è©é »-é€†æ–‡æª”é »ç‡ (TF-IDF) å’Œ n-gramã€‚
- **è©åµŒå…¥**: è©åµŒå…¥æ˜¯ä¸€ç¨®è©è¡¨ç¤ºå½¢å¼ï¼Œå…è¨±å…·æœ‰ç›¸ä¼¼æ„ç¾©çš„è©å…·æœ‰ç›¸ä¼¼çš„è¡¨ç¤ºå½¢å¼ã€‚ä¸»è¦æ–¹æ³•åŒ…æ‹¬ Word2Vecã€GloVe å’Œ FastTextã€‚
- **éæ­¸ç¥ç¶“ç¶²è·¯ (RNN)**: äº†è§£ RNN çš„å·¥ä½œåŸç†ï¼ŒRNN æ˜¯ä¸€ç¨®è¨­è¨ˆç”¨æ–¼è™•ç†åºåˆ—è³‡æ–™çš„ç¥ç¶“ç¶²è·¯ã€‚æ¢ç´¢ LSTM å’Œ GRUï¼Œé€™å…©ç¨®èƒ½å¤ å­¸ç¿’é•·æœŸä¾è³´é—œä¿‚çš„ RNN è®Šé«”ã€‚
ğŸ“š Resources:

- [RealPython - NLP with spaCy in Python](https://realpython.com/natural-language-processing-spacy-python/): æœ‰é—œ Python ä¸­ç”¨æ–¼ NLP ä»»å‹™çš„ spaCy å‡½å¼åº«çš„è©³ç´°æŒ‡å—ã€‚
- [Kaggle - NLP Guide](https://www.kaggle.com/learn-guide/natural-language-processing):ä¸€äº› notebooks å’Œè³‡æºï¼Œç”¨æ–¼ Python ä¸­ NLP çš„å¯¦è¸è§£é‡‹ã€‚ 
- [Jay Alammar - The Illustration Word2Vec](https://jalammar.github.io/illustrated-word2vec/):äº†è§£è‘—å Word2Vec æ¶æ§‹çš„ä¸€å€‹å¥½ææ–™ã€‚
- [Jake Tae - PyTorch RNN from Scratch](https://jaketae.github.io/study/pytorch-rnn/): åœ¨ PyTorch ä¸­å¯¦ç”¨ä¸”ç°¡å–®åœ°å¯¦ä½œ RNNã€LSTM å’Œ GRU æ¨¡å‹ã€‚
- [colah's blog - Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/): ä¸€ç¯‡æ›´ç†è«–æ€§çš„ LSTM ç¶²è·¯æ–‡ç« ã€‚

## ğŸ§‘â€ğŸ”¬ LLM æ¨¡å‹å·¥ç¨‹

æœ¬èª²ç¨‹çš„é€™ä¸€éƒ¨åˆ†é‡é»åœ¨æ–¼å­¸ç¿’å¦‚ä½•ä½¿ç”¨æœ€æ–°æŠ€è¡“ä¾†å»ºç«‹æœ€å¥½çš„ LLMsã€‚

![](img/roadmap_scientist.png)

### 1. The LLM æ¶æ§‹

é›–ç„¶ä¸éœ€è¦æ·±å…¥äº†è§£ Transformer æ¶æ§‹ï¼Œä½†æ·±å…¥äº†è§£å…¶è¼¸å…¥ï¼ˆtokens ä»¤ç‰Œï¼‰å’Œè¼¸å‡ºï¼ˆlogitsï¼‰éå¸¸é‡è¦ã€‚æ™®é€šçš„æ³¨æ„åŠ›æ©Ÿåˆ¶æ˜¯å¦ä¸€å€‹éœ€è¦æŒæ¡çš„é—œéµçµ„æˆéƒ¨åˆ†ï¼Œç¨å¾Œæœƒä»‹ç´¹å®ƒçš„æ”¹é€²ç‰ˆæœ¬ã€‚

* **é«˜éšè¦–åœ– High-level view**: é‡æ–°å¯©è¦–ç·¨ç¢¼å™¨-è§£ç¢¼å™¨ Transformer æ¶æ§‹ï¼Œæ›´å…·é«”åœ°èªªï¼Œæ˜¯åƒ…è§£ç¢¼å™¨çš„ GPT æ¶æ§‹ï¼Œè©²æ¶æ§‹åœ¨æ¯å€‹æœ€è¿‘çš„ LLM ä¸­åŸºæœ¬éƒ½æœ‰ä½¿ç”¨ã€‚
* **æ¨™è¨˜åŒ– Tokenization**: äº†è§£å¦‚ä½•å°‡åŸå§‹æ–‡å­—è³‡æ–™è½‰æ›ç‚ºæ¨¡å‹å¯ä»¥ç†è§£çš„æ ¼å¼ï¼Œé€™æ¶‰åŠå°‡æ–‡å­—æ‹†åˆ†ç‚ºæ¨™è¨˜ï¼ˆé€šå¸¸æ˜¯å–®å­—æˆ–å­å–®å­—ï¼‰ã€‚
* **æ³¨æ„åŠ›æ©Ÿåˆ¶**: æŒæ¡æ³¨æ„åŠ›æ©Ÿåˆ¶èƒŒå¾Œçš„ç†è«–ï¼ŒåŒ…æ‹¬è‡ªè¨»æ„åŠ›å’Œç¸®æ”¾é»ç©æ³¨æ„åŠ›ï¼Œé€™ä½¿å¾—æ¨¡å‹åœ¨ç”¢ç”Ÿè¼¸å‡ºæ™‚èƒ½å¤ å°ˆæ³¨æ–¼è¼¸å…¥çš„ä¸åŒéƒ¨åˆ†ã€‚
* **æ–‡å­—ç”Ÿæˆ**: äº†è§£æ¨¡å‹ç”¢ç”Ÿè¼¸å‡ºåºåˆ—çš„ä¸åŒæ–¹å¼ã€‚å¸¸è¦‹çš„ç­–ç•¥åŒ…æ‹¬è²ªå©ªè§£ç¢¼(greedy decoding)ã€æ³¢æŸæœå°‹(beam searc)ã€top-k æ¡æ¨£(top-k sampling)å’Œæ ¸æ¡æ¨£(nucleus sampling)ã€‚

ğŸ“š **åƒè€ƒè³‡æ–™**:
- [Transformer æ’åœ–](https://jalammar.github.io/illustrated-transformer/) by Jay Alammar: Transformer æ¨¡å‹çš„ç›´è§€è§£é‡‹ã€‚
- [GPT-2åœ–è§£](https://jalammar.github.io/illustrated-gpt2/) by Jay Alammar: æ­¤æ–‡æ¯”ä¸Šä¸€ç¯‡æ–‡ç« æƒ³å°æ›´é‡è¦äº›ï¼Œå®ƒå°ˆæ³¨æ–¼å’Œ Llama éå¸¸ç›¸ä¼¼çš„ GPT æ¶æ§‹ã€‚
- [LLM Visualization](https://bbycroft.net/llm) by Brendan Bycroft: ä»¥ 3D è¦–è¦ºåŒ–æ–¹å¼å‘ˆç¾ LLM å…§éƒ¨ç™¼ç”Ÿçš„æƒ…æ³ã€‚
* [nanoGPT](https://www.youtube.com/watch?v=kCc8FmEb1nY) by Andrej Karpathy:ä¸€æ®µ 2 å°æ™‚é•·çš„ YouTube å½±ç‰‡ï¼Œç”¨æ–¼å¾é ­é–‹å§‹é‡æ–°å¯¦ç¾ GPTï¼ˆä»¥ç¨‹å¼è¨­è¨ˆå¸«çš„è¦–è§’ï¼‰ã€‚
* [Attention? Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/) by Lilian Weng: ä»¥æ›´æ­£å¼çš„æ–¹å¼ä»‹ç´¹æ³¨æ„åŠ›çš„å¿…è¦æ€§ã€‚
* [Decoding Strategies in LLMs](https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html): æä¾›å°ç”Ÿæˆæ–‡æœ¬çš„ä¸åŒè§£ç¢¼ç­–ç•¥çš„åœ–åƒåŒ–ä»‹ç´¹ä»¥åŠç¨‹å¼ç¢¼ã€‚

---
### 2. æ§‹å»ºä¸€å€‹ç¯„ä¾‹(æˆ–æŒ‡ä»¤)è³‡æ–™é›† instruction dataset

é›–ç„¶å¾ç¶­åŸºç™¾ç§‘å’Œå…¶ä»–ç¶²ç«™æ‰¾åˆ°åŸå§‹è³‡æ–™å¾ˆå®¹æ˜“ï¼Œä½†åœ¨å¾ˆå¤šçš„ç’°å¢ƒä¸­æ”¶é›†æˆå°çš„æŒ‡ç¤º,ç¯„ä¾‹å’Œç­”æ¡ˆå»å¾ˆå›°é›£ã€‚èˆ‡å‚³çµ±æ©Ÿå™¨å­¸ç¿’ä¸€æ¨£ï¼Œè³‡æ–™é›†çš„å“è³ªå°‡ç›´æ¥å½±éŸ¿æ¨¡å‹çš„å“è³ªï¼Œé€™å°±æ˜¯ç‚ºä»€éº¼å®ƒå¯èƒ½æ˜¯å¾®èª¿éç¨‹ä¸­æœ€é‡è¦çš„çµ„æˆéƒ¨åˆ†ã€‚

* **[é¡ä¼¼ Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html)-çš„è³‡æ–™é›†**: ä½¿ç”¨ OpenAI API (GPT) å¾é ­é–‹å§‹ç”¢ç”Ÿåˆæˆè³‡æ–™ã€‚æ‚¨å¯ä»¥æŒ‡å®šç¨®å­å’Œç³»çµ±æç¤ºä¾†å»ºç«‹å¤šæ¨£åŒ–çš„è³‡æ–™é›†ã€‚
* **é€²éšæŠ€å·§**: äº†è§£å¦‚ä½•ä½¿ç”¨[Evol-Instruct](https://arxiv.org/abs/2304.12244)æ”¹é€²ç¾æœ‰è³‡æ–™é›†ï¼Œå¦‚ä½•ç”¢ç”Ÿå’Œ[Orca](https://arxiv.org/abs/2306.02707) å’Œ [phi-1](https://arxiv.org/abs/2306.11644) è«–æ–‡ä¸­é¡ä¼¼çš„é«˜å“è³ªåˆæˆè³‡æ–™.
* **è³‡æ–™éæ¿¾**: æ¶‰åŠæ­£è¦è¡¨ç¤ºå¼ã€åˆªé™¤è¿‘ä¼¼é‡è¤‡é …ã€é—œæ³¨å…·æœ‰å¤§é‡æ¨™è¨˜çš„ç­”æ¡ˆç­‰çš„å‚³çµ±æŠ€å·§.
* **æç¤ºè©æ¨¡æ¿**: ç›®å‰é‚„æ²’æœ‰çœŸæ­£çš„æ¨™æº–æ–¹æ³•ä¾†æ ¼å¼ä¾†æ¨™æº–åŒ–èªªæ˜ç¯„æœ¬å’Œç­”æ¡ˆï¼Œé€™å°±æ˜¯ç‚ºä»€éº¼äº†è§£ä¸åŒçš„èŠå¤©ç¯„æœ¬å¾ˆé‡è¦, åƒæ˜¯ [ChatML](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/chatgpt?tabs=python&pivots=programming-language-chat-ml), [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) ç­‰.

ğŸ“š **åƒè€ƒè³‡æ–™**:
* [Preparing a Dataset for Instruction tuning](https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-1-Preparing-a-Dataset-for-Instruction-Tuning--Vmlldzo1NTcxNzE2) by Thomas Capelle: Alpaca å’Œ Alpaca-GPT4 è³‡æ–™é›†çš„æ¢ç´¢ä»¥åŠå¦‚ä½•æ¨™æº–åŒ–è³‡æ–™.
* [Generating a Clinical Instruction Dataset](https://medium.com/mlearning-ai/generating-a-clinical-instruction-dataset-in-portuguese-with-langchain-and-gpt-4-6ee9abfa41ae) by Solano Todeschini: æœ‰é—œå¦‚ä½•ä½¿ç”¨ GPT-4 å»ºç«‹ç¶œåˆæŒ‡å°è³‡æ–™é›†çš„æ•™å­¸. 
* [GPT 3.5 for news classification](https://medium.com/@kshitiz.sahay26/how-i-created-an-instruction-dataset-using-gpt-3-5-to-fine-tune-llama-2-for-news-classification-ed02fe41c81f) by Kshitiz Sahay: ä½¿ç”¨ GPT 3.5 å»ºç«‹ç¯„ä¾‹è³‡æ–™é›†ä¾†å¾®èª¿ Llama 2 çš„æ–°èåˆ†é¡.
* [Dataset creation for fine-tuning LLM](https://colab.research.google.com/drive/1GH8PW9-zAe4cXEZyOIE-T9uHXblIldAg?usp=sharing): åŒ…å«ä¸€äº›éæ¿¾è³‡æ–™é›†å’Œä¸Šå‚³çµæœæŠ€è¡“çš„ Notebook .
* [Chat Template](https://huggingface.co/blog/chat-templates) by Matthew Carrigan: é—œæ–¼æç¤ºæ¨¡æ¿çš„ Hugging Face é é¢

---
### 3. é è¨“ç·´æ¨¡å‹

é è¨“ç·´æ˜¯ä¸€å€‹éå¸¸æ¼«é•·ä¸”æˆæœ¬é«˜æ˜‚çš„éç¨‹ï¼Œé€™å°±æ˜¯ç‚ºä»€éº¼é€™ä¸æ˜¯æœ¬èª²ç¨‹çš„é‡é»ã€‚ä½†æ˜¯å°é è¨“ç·´æœŸé–“ç™¼ç”Ÿçš„æƒ…æ³æœ‰ä¸€å®šç¨‹åº¦çš„äº†è§£æ˜¯å¾ˆå¥½çš„ã€‚

* **è³‡æ–™è™•ç†æµç¨‹**: é è¨“ç·´éœ€è¦é¾å¤§çš„è³‡æ–™é›† (ä¾‹å¦‚ï¼š [Llama 2](https://arxiv.org/abs/2307.09288) ä½¿ç”¨ 2 å…†å€‹tokensé€²è¡Œè¨“ç·´) ï¼Œéœ€è¦å°‡é€™äº›è³‡æ–™é›†éæ¿¾ã€æ¨™è¨˜åŒ–ä¸¦èˆ‡é å…ˆå®šç¾©çš„è©å½™é€²è¡Œæ•´ç†ã€‚
* **å› æœèªè¨€å»ºæ¨¡(Causal language modeling)**: äº†è§£å› æœèªè¨€å»ºæ¨¡å’Œæ©ç¢¼èªè¨€å»ºæ¨¡(causal and masked language modeling)ä¹‹é–“çš„å€åˆ¥,ä»¥åŠæœ¬ä¾‹ä¸­ä½¿ç”¨çš„æå¤±å‡½æ•¸ã€‚æ›´å¤šé«˜æ•ˆç‡çš„é è¨“ç·´çŸ¥è­˜å¯å‰å¾€ [Megatron-LM](https://github.com/NVIDIA/Megatron-LM) æˆ–[gpt-neox](https://github.com/EleutherAI/gpt-neox)äº†è§£ã€‚ 
* **ç¸®æ”¾çš„è¦å¾‹**: [ç¸®æ”¾çš„è¦å¾‹](https://arxiv.org/pdf/2001.08361.pdf) æ ¹æ“šæ¨¡å‹å¤§å°ã€è³‡æ–™é›†å¤§å°å’Œç”¨æ–¼è¨“ç·´çš„è¨ˆç®—é‡æè¿°é æœŸçš„æ¨¡å‹æ€§èƒ½ã€‚
* **é«˜æ•ˆèƒ½é‹ç®—**: é€™æœ‰é»è¶…å‡ºäº†æœ¬æ–‡çš„ç¯„åœï¼Œä½†å¦‚æœæ‚¨æ‰“ç®—å¾é ­é–‹å§‹å‰µå»ºè‡ªå·±çš„LLMs(å¤§èªè¨€æ¨¡å‹)ï¼ˆç¡¬é«”ã€åˆ†æ•£å¼å·¥ä½œè² è¼‰ç­‰ï¼‰ï¼Œé‚£éº¼æ›´å¤šæœ‰é—œ HPC çš„çŸ¥è­˜æ˜¯å°ä½ è€Œè¨€æ˜¯å¿…è¦çš„ã€‚
  
ğŸ“š **åƒè€ƒè³‡æ–™**:
* [LLMDataHub](https://github.com/Zjh-819/LLMDataHub) by Junhao Zhao: ç”¨æ–¼é è¨“ç·´ã€å¾®èª¿å’Œ RLHF çš„ç²¾é¸è³‡æ–™é›†æ¸…å–®ã€‚
* [Training a causal language model from scratch](https://huggingface.co/learn/nlp-course/chapter7/6?fw=pt) by Hugging Face: ä½¿ç”¨ Transformers åº«å¾é ­é–‹å§‹é å…ˆè¨“ç·´ GPT-2 æ¨¡å‹ã€‚
* [TinyLlama](https://github.com/jzhang38/TinyLlama) by Zhang et al.: å¯åœ¨æ­¤é …ç›®å¾ˆå¥½åœ°äº†è§£ Llama æ¨¡å‹æ˜¯å¦‚ä½•å¾é ­é–‹å§‹è¨“ç·´çš„ã€‚
* [Causal language modeling](https://huggingface.co/docs/transformers/tasks/language_modeling) by Hugging Face: è§£é‡‹å› æœèªè¨€å»ºæ¨¡å’Œå±è”½èªè¨€å»ºæ¨¡ä¹‹é–“çš„å·®ç•°ä»¥åŠå¦‚ä½•å¿«é€Ÿå¾®èª¿ DistilGPT-2 æ¨¡å‹ã€‚
* [Chinchilla's wild implications](https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications) by nostalgebraist: è¨è«–ç¸®æ”¾å®šå¾‹ä¸¦è§£é‡‹å®ƒå€‘å°å¤§èªè¨€æ¨¡å‹çš„æ„ç¾©ã€‚
* [BLOOM](https://bigscience.notion.site/BLOOM-BigScience-176B-Model-ad073ca07cdf479398d5f95d88e218c4) by BigScience: æè¿°å¦‚ä½•å»ºç«‹ BLOOM æ¨¡å‹çš„ Notion é é¢ï¼Œå…¶ä¸­åŒ…å«å¤§é‡æœ‰é—œå·¥ç¨‹éƒ¨åˆ†å’Œé‡åˆ°å•é¡Œçš„æœ‰ç”¨è³‡è¨Šã€‚
* [OPT-175 Logbook](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/OPT175B_Logbook.pdf) by Meta: ç ”ç©¶æ—¥èªŒé¡¯ç¤ºå‡ºäº†ä»€éº¼éŒ¯çš„ä»¥åŠä»€éº¼æ˜¯æ­£ç¢ºçš„ã€‚å¦‚æœæ‚¨è¨ˆåŠƒé å…ˆè¨“ç·´éå¸¸å¤§çš„èªè¨€æ¨¡å‹ï¼ˆåœ¨æœ¬ä¾‹ä¸­ç‚º 175B åƒæ•¸ï¼‰ï¼Œå‰‡éå¸¸æœ‰ç”¨ã€‚
* [LLM 360](https://www.llm360.ai/): é–‹æºå¤§æ–¼è¨€æ¨¡å‹æ¡†æ¶ï¼ŒåŒ…å«è¨“ç·´å’Œè³‡æ–™æº–å‚™ä»£ç¢¼ã€è³‡æ–™ã€æŒ‡æ¨™å’Œæ¨¡å‹ã€‚

---
### 4. ç›£ç£å¾®èª¿ (Supervised Fine-Tuning)

é è¨“ç·´æ¨¡å‹åƒ…é‡å°ä¸‹ä¸€å€‹æ¨™è¨˜(next-token)é æ¸¬ä»»å‹™é€²è¡Œè¨“ç·´ï¼Œé€™å°±æ˜¯ç‚ºä»€éº¼å®ƒå€‘ä¸æ˜¯æœ‰ç”¨çš„åŠ©æ‰‹ã€‚SFT å…è¨±æ‚¨èª¿æ•´å®ƒå€‘ä»¥å›æ‡‰æŒ‡ä»¤ã€‚æ­¤å¤–ï¼Œå®ƒå…è¨±æ‚¨æ ¹æ“šä»»ä½•è³‡æ–™ï¼ˆç§äººè³‡æ–™ã€GPT-4 ç„¡æ³•çœ‹åˆ°çš„è³‡æ–™ç­‰ï¼‰å¾®èª¿æ‚¨çš„æ¨¡å‹ä¸¦ä½¿ç”¨å®ƒï¼Œè€Œç„¡éœ€æ”¯ä»˜ OpenAI ç­‰ API çš„è²»ç”¨ã€‚

* **å…¨å¾®èª¿**: å…¨å¾®èª¿æ˜¯æŒ‡è¨“ç·´æ¨¡å‹ä¸­çš„æ‰€æœ‰åƒæ•¸(å°±æ˜¯æ¨¡å‹è¨“ç·´)ã€‚é€™ä¸æ˜¯ä¸€ç¨®æœ‰æ•ˆçš„æŠ€è¡“ï¼Œä½†å®ƒæœƒç”¢ç”Ÿç¨å¾®å¥½ä¸€é»çš„çµæœ.
* [**LoRA**](https://arxiv.org/abs/2106.09685): ä¸€ç¨®åŸºæ–¼ä½éšé©é…å™¨(low-rank adapters)çš„é«˜æ•ˆåƒæ•¸å¾®èª¿æŠ€è¡“ï¼ˆPEFTï¼‰ã€‚æˆ‘å€‘ä¸è¨“ç·´æ‰€æœ‰åƒæ•¸ï¼Œè€Œæ˜¯åªè¨“ç·´é€™äº›é©é…å™¨(adapters)ã€‚
* [**QLoRA**](https://arxiv.org/abs/2305.14314): å¦ä¸€å€‹åŸºæ–¼ LoRA çš„ PEFTï¼Œå®ƒé‚„å°‡æ¨¡å‹çš„æ¬Šé‡é‡åŒ–ç‚º 4 bitsï¼Œä¸¦å¼•å…¥åˆ†é å„ªåŒ–å™¨ä¾†ç®¡ç†è¨˜æ†¶é«”å³°å€¼ã€‚å°‡å…¶èˆ‡[Unsloth](https://github.com/unslothai/unsloth)çµåˆä½¿ç”¨ï¼Œå¯ä»¥åœ¨å…è²»çš„ Colab ç­†è¨˜æœ¬ä¸Šé‹è¡Œã€‚
* **[Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)**: ä¸€ç¨®ç”¨æˆ¶å‹å¥½ä¸”åŠŸèƒ½å¼·å¤§çš„å¾®èª¿å·¥å…·ï¼Œç”¨æ–¼è¨±å¤šæœ€å…ˆé€²çš„é–‹æºæ¨¡å‹ã€‚
* [**DeepSpeed**](https://www.deepspeed.ai/): é‡å°å¤š GPU å’Œå¤šç¯€é»è¨­å®šçš„ LLM çš„é«˜æ•ˆé è¨“ç·´å’Œå¾®èª¿ï¼ˆåœ¨ Axolotl ä¸­å¯¦ç¾ï¼‰ã€‚

ğŸ“š **åƒè€ƒè³‡æ–™**:
* [The Novice's LLM Training Guide](https://rentry.org/llm-training) by Alpin: æ¦‚è¿°å¾®èª¿ LLM æ™‚è¦è€ƒæ…®çš„ä¸»è¦æ¦‚å¿µå’Œåƒæ•¸.
* [LoRA insights](https://lightning.ai/pages/community/lora-insights/) by Sebastian Raschka: æœ‰é—œ LoRA ä»¥åŠå¦‚ä½•é¸æ“‡æœ€ä½³åƒæ•¸çš„å¯¦ç”¨è¦‹è§£.
* [Fine-Tune Your Own Llama 2 Model](https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html): æœ‰é—œå¦‚ä½•ä½¿ç”¨ Hugging Face åº«å¾®èª¿ Llama 2 æ¨¡å‹çš„å¯¦ä½œæ•™å­¸.
* [Padding Large Language Models](https://towardsdatascience.com/padding-large-language-models-examples-with-llama-2-199fb10df8ff) by Benjamin Marie: ç‚ºå› æœLLMs(causal LLMs)å¡«å……è¨“ç·´ç¯„ä¾‹çš„æœ€ä½³å¯¦è¸ 
* [A Beginner's Guide to LLM Fine-Tuning](https://mlabonne.github.io/blog/posts/A_Beginners_Guide_to_LLM_Finetuning.html): æœ‰é—œå¦‚ä½•ä½¿ç”¨ Axolotl å¾®èª¿ CodeLlama æ¨¡å‹çš„æ•™å­¸.

---
### 5. Reinforcement Learning from Human Feedback RLHF (æ ¹æ“šäººé¡å›é¥‹é€²è¡Œå¼·åŒ–å­¸ç¿’)

ç¶“éç›£ç£å¾®èª¿å¾Œï¼ŒRLHF æ˜¯ç”¨ä¾†ä½¿ LLM çš„ç­”æ¡ˆèˆ‡äººé¡æœŸæœ›ä¿æŒä¸€è‡´çš„ä¸€å€‹æ­¥é©Ÿã€‚é€™å€‹æƒ³æ³•æ˜¯å¾äººé¡ï¼ˆæˆ–äººå·¥ï¼‰å›é¥‹ä¸­å­¸ç¿’åå¥½ï¼Œé€™å¯ç”¨æ–¼æ¸›å°‘åè¦‹ã€å¯©æŸ¥æ¨¡å‹æˆ–ä½¿å®ƒå€‘ä»¥æ›´æœ‰ç”¨çš„æ–¹å¼è¡Œäº‹ã€‚å®ƒæ¯” SFT æ›´è¤‡é›œï¼Œä¸¦ä¸”é€šå¸¸è¢«è¦–ç‚ºå¯é¸é …ä¹‹ä¸€ã€‚


* **åå¥½è³‡æ–™é›†**: é€™äº›è³‡æ–™é›†é€šå¸¸åŒ…å«å…·æœ‰æŸç¨®æ’åçš„å¤šå€‹ç­”æ¡ˆï¼Œé€™ä½¿å¾—å®ƒå€‘æ¯”æŒ‡ä»¤è³‡æ–™é›†æ›´é›£ç”¢ç”Ÿ.
* [**è¿‘ç«¯ç­–ç•¥æœ€ä½³åŒ–**](https://arxiv.org/abs/1707.06347): æ­¤æ¼”ç®—æ³•åˆ©ç”¨çå‹µæ¨¡å‹ä¾†é æ¸¬çµ¦å®šæ–‡å­—æ˜¯å¦è¢«äººé¡æ’åè¼ƒé«˜ã€‚ç„¶å¾Œä½¿ç”¨è©²é æ¸¬ä¾†æœ€ä½³åŒ– SFT æ¨¡å‹ï¼Œä¸¦æ ¹æ“š KL æ•£åº¦é€²è¡Œçæ‡²ã€‚
* **[ç›´æ¥åå¥½å„ªåŒ–](https://arxiv.org/abs/2305.18290)**: DPO é€éå°‡å…¶é‡æ–°å®šç¾©ç‚ºåˆ†é¡å•é¡Œä¾†ç°¡åŒ–æµç¨‹ã€‚å®ƒä½¿ç”¨åƒè€ƒæ¨¡å‹è€Œä¸æ˜¯çå‹µæ¨¡å‹ï¼ˆç„¡éœ€è¨“ç·´ï¼‰ï¼Œä¸¦ä¸”åªéœ€è¦ä¸€å€‹è¶…åƒæ•¸ï¼Œä½¿å…¶æ›´åŠ ç©©å®šå’Œé«˜æ•ˆã€‚

ğŸ“š **åƒè€ƒè³‡æ–™**:
* [An Introduction to Training LLMs using RLHF](https://wandb.ai/ayush-thakur/Intro-RLAIF/reports/An-Introduction-to-Training-LLMs-Using-Reinforcement-Learning-From-Human-Feedback-RLHF---VmlldzozMzYyNjcy) by Ayush Thakur: é€™è§£é‡‹äº†ç‚ºä»€éº¼ RLHF å°æ–¼æ¸›å°‘å¤§èªè¨€æ¨¡å‹çš„åè¦‹å’Œæé«˜ç¸¾æ•ˆæ˜¯å¯å–çš„ã€‚
* [Illustration RLHF](https://huggingface.co/blog/rlhf) by Hugging Face: RLHF ç°¡ä»‹ï¼ŒåŒ…æ‹¬çå‹µæ¨¡å‹è¨“ç·´å’Œå¼·åŒ–å­¸ç¿’å¾®èª¿.
* [StackLLaMA](https://huggingface.co/blog/stackllama) by Hugging Face: ä½¿ç”¨ Transformer å‡½å¼åº«æœ‰æ•ˆåœ°å°‡ LLaMA æ¨¡å‹èˆ‡ RLHF å°é½Šçš„æ•™å­¸.
* [LLM Training: RLHF and Its Alternatives](https://substack.com/profile/27393275-sebastian-raschka-phd) by Sebastian Rashcka: RLHF æµç¨‹å’Œ RLAIF ç­‰æ›¿ä»£æ–¹æ¡ˆçš„æ¦‚è¿°.
* [Fine-tune Mistral-7b with DPO](https://huggingface.co/blog/dpo-trl):ä½¿ç”¨ DPO å¾®èª¿ Mistral-7b æ¨¡å‹ä¸¦é‡ç¾[NeuralHermes-2.5](https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B) çš„æ•™å­¸.

---
### 6. è©•ä¼° Evaluation

è©•ä¼°å¤§å‹èªè¨€æ¨¡å‹(LLMs)æ˜¯æµç¨‹ä¸­ä¸€å€‹è¢«ä½ä¼°çš„éƒ¨åˆ†ï¼Œå› ç‚ºè©•ä¼°é€™ä¸€å€‹éç¨‹è€—æ™‚ä¸”ç›¸å°å¯é æ€§è¼ƒä½ã€‚ä½ çš„ä¸‹æ¸¸ä»»å‹™æ‡‰è©²æŒ‡æ˜ä½ æƒ³è¦è©•ä¼°çš„å…§å®¹ï¼Œä½†è¨˜å¾—å¤å¾·å“ˆç‰¹å®šå¾‹(Goodhart's law)æåˆ°çš„ï¼šâ€œç•¶ä¸€å€‹è¡¡é‡æŒ‡æ¨™è®Šæˆäº†ç›®æ¨™ï¼Œå®ƒå°±ä¸å†æ˜¯ä¸€å€‹å¥½çš„è¡¡é‡æŒ‡æ¨™ã€‚â€

* **å‚³çµ±æŒ‡æ¨™ Traditional metrics**: åƒå›°æƒ‘åº¦(perplexity)å’ŒBLEUåˆ†æ•¸é€™æ¨£çš„æŒ‡æ¨™ä¸å†åƒä»¥å‰é‚£æ¨£å—æ­¡è¿ï¼Œå› ç‚ºåœ¨å¤§å¤šæ•¸æƒ…æ³ä¸‹å®ƒå€‘æ˜¯æœ‰ç¼ºé™·çš„ã€‚ä½†äº†è§£å®ƒå€‘ä»¥åŠå®ƒå€‘é©ç”¨çš„æƒ…å¢ƒä»ç„¶å¾ˆé‡è¦ã€‚
* **é€šç”¨åŸºæº– General benchmarks**: åŸºæ–¼èªè¨€æ¨¡å‹è©•ä¼°å·¥å…·ç®± [Language Model Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness)ï¼ŒOpen LLMæ’è¡Œæ¦œ [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) æ˜¯ç”¨æ–¼é€šç”¨å¤§å‹èªè¨€æ¨¡å‹ï¼ˆå¦‚ChatGPTï¼‰çš„ä¸»è¦åŸºæº–ã€‚é‚„æœ‰å…¶ä»–å—æ­¡è¿çš„åŸºæº–ï¼Œå¦‚[BigBench](https://github.com/google/BIG-bench), [MT-Bench](https://arxiv.org/abs/2306.05685)ç­‰ã€‚
* **ä»»å‹™ç‰¹å®šåŸºæº– Task-specific benchmarks**: å¦‚æ‘˜è¦ã€ç¿»è­¯å’Œå•ç­”ç­‰ä»»å‹™æœ‰å°ˆé–€çš„åŸºæº–ã€æŒ‡æ¨™ç”šè‡³å­é ˜åŸŸï¼ˆé†«ç™‚ã€é‡‘èç­‰ï¼‰ï¼Œä¾‹å¦‚ç”¨æ–¼ç”Ÿç‰©é†«å­¸å•ç­” [PubMedQA](https://pubmedqa.github.io/)ã€‚
* **äººé¡è©•ä¼° Human evaluation**: æœ€å¯é çš„è©•ä¼°æ˜¯ç”¨æˆ¶çš„æ¥å—åº¦æˆ–ç”±äººé¡æ‰€åšçš„æ¯”è¼ƒã€‚å¦‚æœä½ æƒ³çŸ¥é“ä¸€å€‹æ¨¡å‹è¡¨ç¾å¾—å¦‚ä½•ï¼Œæœ€ç°¡å–®ä½†æœ€ç¢ºå®šçš„æ–¹å¼å°±æ˜¯è‡ªå·±ä½¿ç”¨å®ƒã€‚

ğŸ“š **åƒè€ƒæ–‡ç»**:
* [å›ºå®šé•·åº¦è¼¸å…¥(æœ‰æœ€å¤§è¼¸å…¥é™åˆ¶)æ¨¡å‹çš„å›°æƒ‘åº¦](https://huggingface.co/docs/transformers/perplexity) by Hugging Face: å›°æƒ‘åº¦(perplexity)çš„æ¦‚è¿°ï¼Œä¸¦ä½¿ç”¨ Transformer åº«å¯¦ç¾äº†å®ƒçš„ç¨‹å¼ç¢¼ã€‚
* [BLEU ä½¿ç”¨é¢¨éšª](https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213) by Rachael Tatman: BLEU åˆ†æ•¸åŠå…¶è¨±å¤šå•é¡Œçš„æ¦‚è¿°ï¼Œä¸¦æä¾›äº†ç¤ºä¾‹ã€‚
* [å¤§å‹èªè¨€æ¨¡å‹è©•ä¼°èª¿æŸ¥](https://arxiv.org/abs/2307.03109) by Chang et al.: é—œæ–¼è©•ä¼°ä»€éº¼ã€åœ¨å“ªè£¡è©•ä¼°ä»¥åŠå¦‚ä½•è©•ä¼°çš„ç¶œåˆæ€§è«–æ–‡ã€‚
* [èŠå¤©æ©Ÿå™¨äººæ’è¡Œæ¦œ](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard) by lmsys: åŸºæ–¼äººé¡æ¯”è¼ƒçš„é€šç”¨å¤§å‹èªè¨€æ¨¡å‹çš„Eloè©•åˆ†ã€‚

---
### 7. é‡åŒ–

é‡åŒ–æ˜¯å°‡æ¨¡å‹çš„æ¬Šé‡ï¼ˆå’Œå•Ÿå‹•å€¼ï¼‰è½‰æ›æˆæ›´ä½ç²¾åº¦è¡¨ç¤ºçš„éç¨‹ã€‚ä¾‹å¦‚ï¼ŒåŸæœ¬ä½¿ç”¨16ä½å…ƒå„²å­˜çš„æ¬Šé‡å¯ä»¥è½‰æ›æˆ4ä½å…ƒè¡¨ç¤ºã€‚é€™ç¨®æŠ€è¡“æ„ˆä¾†æ„ˆé‡è¦ï¼Œç”¨ä¾†æ¸›å°‘èˆ‡å¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç›¸é—œçš„è¨ˆç®—èˆ‡è¨˜æ†¶é«”æˆæœ¬ã€‚

* **åŸºç¤æŠ€è¡“**: ç­è§£ä¸åŒçš„ç²¾ç¢ºåº¦å±¤ç´šï¼ˆFP32ã€FP16ã€INT8ç­‰ï¼‰ä»¥åŠå¦‚ä½•ä½¿ç”¨absmaxèˆ‡é›¶é»æŠ€è¡“(zero-point techniques)é€²è¡Œç°¡å–®çš„é‡åŒ–ã€‚
* **GGUFå’Œllama.cpp**: æœ€åˆè¨­è¨ˆç”¨æ–¼åœ¨CPUä¸Šé‹è¡Œï¼Œ[llama.cpp](https://github.com/ggerganov/llama.cpp) å’ŒGGUFæ ¼å¼å·²æˆç‚ºåœ¨æ¶ˆè²»ç´šç¡¬é«”ä¸Šé‹è¡ŒLLMsçš„æœ€å—æ­¡è¿çš„å·¥å…·ã€‚
* **GPTQå’ŒEXL2**: [GPTQ](https://arxiv.org/abs/2210.17323) ï¼Œç‰¹åˆ¥æ˜¯ [EXL2](https://github.com/turboderp/exllamav2) ï¼Œæä¾›äº†è¼ƒå¿«çš„é€Ÿåº¦ï¼Œä½†åªèƒ½åœ¨GPUä¸Šé‹è¡Œã€‚æ¨¡å‹é‡åŒ–ä¹Ÿéœ€è¦å¾ˆé•·æ™‚é–“ã€‚
* **AWQ**: é€™ç¨®æ–°æ ¼å¼æ¯”GPTQæ›´æº–ç¢ºï¼ˆå›°æƒ‘åº¦æ›´ä½ï¼‰ï¼Œä½†ä½¿ç”¨çš„é¡¯å­˜æ›´å¤šï¼Œé€Ÿåº¦ä¹Ÿä¸ä¸€å®šæ›´å¿«ã€‚

ğŸ“š **åƒè€ƒæ–‡ç»**:
* [é‡åŒ–ç°¡ä»‹](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html): é‡åŒ–æ¦‚è¿°ï¼Œabsmaxèˆ‡é›¶é»é‡åŒ–ï¼Œä»¥åŠä½¿ç”¨ LLM.int8()åœ¨ç¨‹å¼ç¢¼ä¸Šã€‚
* [ä½¿ç”¨llama.cppé‡åŒ–Llamaæ¨¡å‹](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html): é—œæ–¼å¦‚ä½•ä½¿ç”¨llama.cppå’ŒGGUFæ ¼å¼é‡åŒ–Llama 2æ¨¡å‹çš„æ•™å­¸ã€‚
* [ä½¿ç”¨GPTQé€²è¡Œ4ä½å…ƒLLMé‡åŒ–](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html):é—œæ–¼å¦‚ä½•ä½¿ç”¨GPTQæ¼”ç®—æ³•å’ŒAutoGPTQé‡åŒ–LLMçš„æ•™å­¸ã€‚
* [ExLlamaV2: é‹è¡ŒLLMsçš„æœ€å¿«ç¨‹å¼åº«](https://mlabonne.github.io/blog/posts/ExLlamaV2_The_Fastest_Library_to_Run%C2%A0LLMs.html): æŒ‡å—ï¼›é—œæ–¼å¦‚ä½•ä½¿ç”¨EXL2æ ¼å¼é‡åŒ–Mistralæ¨¡å‹ï¼Œä¸¦ä½¿ç”¨ExLlamaV2ç¨‹å¼åº«é‹è¡Œã€‚
* [äº†è§£å•Ÿå‹•æ„ŸçŸ¥æ¬Šé‡é‡åŒ–](https://medium.com/friendliai/understanding-activation-aware-weight-quantization-awq-boosting-inference-serving-efficiency-in-10bb0faf63a8) by FriendliAI: AWQæŠ€è¡“åŠå…¶å„ªå‹¢çš„æ¦‚è¿°ã€‚

---
### 8. æ–°è¶¨å‹¢

* **ä½ç½®åµŒå…¥ Positional embeddings**: äº†è§£å¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ä½•ç·¨ç¢¼ä½ç½®ï¼Œç‰¹åˆ¥æ˜¯åƒ [RoPE](https://arxiv.org/abs/2104.09864) é€™æ¨£çš„ç›¸å°ä½ç½®ç·¨ç¢¼æ–¹æ¡ˆã€‚å¯¦ç¾ [YaRN](https://arxiv.org/abs/2309.00071) (é€šéæº«åº¦å› å­ä¹˜ä»¥æ³¨æ„åŠ›çŸ©é™£) or [ALiBi](https://arxiv.org/abs/2108.12409) (åŸºæ–¼tokenè·é›¢çš„æ³¨æ„åŠ›çæ‡²) ä¾†æ“´å±•ä¸Šä¸‹æ–‡é•·åº¦ã€‚
* **æ¨¡å‹èåˆ Model merging**: å°‡è¨“ç·´å¥½çš„æ¨¡å‹åˆä½µå·²æˆç‚ºä¸€ç¨®æµè¡Œçš„æ–¹å¼ï¼Œç”¨æ–¼å‰µå»ºè¡¨ç¾å„ªç•°çš„æ¨¡å‹ï¼Œç„¡éœ€ä»»ä½•å¾®èª¿ã€‚æµè¡Œçš„ [mergekit](https://github.com/cg123/mergekit) åº«å¯¦ç¾äº†æœ€å—æ­¡è¿çš„èåˆæ–¹æ³•ï¼Œå¦‚e SLERP, [DARE](https://arxiv.org/abs/2311.03099), å’Œ [TIES](https://arxiv.org/abs/2311.03099)ã€‚æ¨¡å‹èåˆé€šå¸¸æŒ‡çš„æ˜¯å°‡å¤šå€‹å·²è¨“ç·´çš„æ¨¡å‹åˆä½µæˆä¸€å€‹å–®ä¸€æ¨¡å‹çš„éç¨‹ã€‚é€™ä¸åƒ…åƒ…æ˜¯å¹³å‡æˆ–æŠ•ç¥¨æ±ºå®šè¼¸å‡ºï¼Œè€Œæ˜¯åœ¨æ¨¡å‹çš„æ¬Šé‡å’Œçµæ§‹å±¤é¢ä¸Šé€²è¡Œåˆä½µã€‚é€™å€‹éç¨‹ä¸éœ€è¦å†æ¬¡è¨“ç·´ï¼Œå¯ä»¥é€šéæ•¸å­¸æ“ä½œï¼ˆå¦‚çƒé¢ç·šæ€§å…§æ’ï¼ˆSLERPï¼‰æˆ–å…¶ä»–èåˆæŠ€è¡“ï¼‰å°‡ä¸åŒæ¨¡å‹çš„çŸ¥è­˜æ•´åˆèµ·ä¾†ã€‚æ¨¡å‹èåˆç”¨æ–¼å‰µå»ºä¸€å€‹è¡¨ç¾æ›´ä½³ã€æ›´å¼·å¤§çš„æ¨¡å‹ï¼Œé€šå¸¸æ˜¯å°‡å¤šå€‹æ¨¡å‹åœ¨ç‰¹å®šä»»å‹™ä¸Šçš„å„ªå‹¢çµåˆèµ·ä¾†ã€‚
* **å°ˆå®¶æ··åˆ Mixture of Experts**: [Mixtral](https://arxiv.org/abs/2401.04088) å› å…¶å“è¶Šçš„æ€§èƒ½è€Œé‡æ–°ä½¿MoEæ¶æ§‹æµè¡Œèµ·ä¾†ã€‚ èˆ‡æ­¤åŒæ™‚ï¼ŒOSSç¤¾å€å‡ºç¾äº†ä¸€ç¨®frankenMoEï¼Œé€šéèåˆåƒ [Phixtral](https://huggingface.co/mlabonne/phixtral-2x2_8)é€™æ¨£çš„æ¨¡å‹ï¼Œé€™æ˜¯ä¸€å€‹æ›´ç¶“æ¿Ÿä¸”æ€§èƒ½è‰¯å¥½çš„é¸é …ã€‚MoEæ˜¯ä¸€ç¨®çµæ§‹ï¼Œå®ƒåŒ…å«å¤šå€‹å­æ¨¡å‹æˆ–â€œå°ˆå®¶â€ï¼Œæ¯å€‹å°ˆå®¶å°ˆé–€è™•ç†ä¸åŒçš„ä»»å‹™æˆ–æ•¸æ“šå­é›†ã€‚åœ¨MoEæ¶æ§‹ä¸­ï¼Œä¸€å€‹â€œgateâ€æˆ–èª¿åº¦å™¨æ±ºå®šå°æ–¼çµ¦å®šçš„è¼¸å…¥ï¼Œå“ªå€‹å°ˆå®¶è¢«ä½¿ç”¨ã€‚é€™æ˜¯ä¸€ç¨®ç¨€ç–å•Ÿå‹•æ–¹æ³•ï¼Œå¯ä»¥å¤§å¹…æå‡æ¨¡å‹çš„å®¹é‡å’Œæ•ˆç‡ï¼Œå› ç‚ºä¸æ˜¯æ‰€æœ‰çš„å°ˆå®¶éƒ½æœƒå°æ¯å€‹è¼¸å…¥é€²è¡ŒéŸ¿æ‡‰ã€‚
* **å¤šæ¨¡æ…‹æ¨¡å‹ Multimodal models**: é€™äº›æ¨¡å‹ï¼ˆ [CLIP](https://openai.com/research/clip), [Stable Diffusion](https://stability.ai/stable-image), æˆ– [LLaVA](https://llava-vl.github.io/)) è™•ç†å¤šç¨®é¡å‹çš„è¼¸å…¥ï¼ˆæ–‡æœ¬ã€åœ–åƒã€éŸ³é »ç­‰ï¼‰èˆ‡çµ±ä¸€çš„åµŒå…¥ç©ºé–“ï¼Œå¾è€Œè§£é–äº†å¼·å¤§çš„æ‡‰ç”¨ï¼Œå¦‚æ–‡æœ¬åˆ°åœ–åƒã€‚

ğŸ“š **åƒè€ƒæ–‡ç»**:
* [Extending the RoPE](https://blog.eleuther.ai/yarn/) by EleutherAI: ç¸½çµä¸åŒä½ç½®ç·¨ç¢¼æŠ€è¡“çš„æ–‡ç« .
* [Understanding YaRN](https://medium.com/@rcrajatchawla/understanding-yarn-extending-context-window-of-llms-3f21e3522465) by Rajat Chawla: å°YaRNçš„ä»‹ç´¹.
* [Merge LLMs with mergekit](https://mlabonne.github.io/blog/posts/2024-01-08_Merge_LLMs_with_mergekit.html): é—œæ–¼ä½¿ç”¨mergekité€²è¡Œæ¨¡å‹èåˆçš„æ•™ç¨‹.
* [Mixture of Experts Explained](https://huggingface.co/blog/moe) by Hugging Face: é—œæ–¼MoEåŠå…¶å·¥ä½œæ–¹å¼çš„è©³ç›¡æŒ‡å—.
* [Large Multimodal Models](https://huyenchip.com/2023/10/10/multimodal.html) by Chip Huyen: å°å¤šæ¨¡æ…‹ç³»çµ±åŠå…¶è¿‘æœŸç™¼å±•æ­·å²çš„æ¦‚è¿°.

## ğŸ‘· LLM æ‡‰ç”¨å·¥ç¨‹

æœ¬èª²ç¨‹çš„é€™ä¸€éƒ¨åˆ†é‡é»æ˜¯å­¸ç¿’å¦‚ä½•å»ºç«‹å¯åœ¨ç”Ÿç”¢ä¸­ä½¿ç”¨çš„ç”± LLM æ”¯æ´çš„æ‡‰ç”¨ç¨‹åºï¼Œé‡é»ä¸»è¦åœ¨å¢å¼·æ¨¡å‹å’Œéƒ¨ç½²å®ƒå€‘ã€‚

![](img/roadmap_engineer.png)


### 1. é‹è¡Œ LLMs


ç”±æ–¼ç¡¬é«”è¦æ±‚é«˜ï¼Œé‹è¡Œå¤§å‹èªè¨€æ¨¡å‹å¯èƒ½æœƒæœ‰å›°é›£ã€‚æ ¹æ“šæ‚¨çš„ä½¿ç”¨æ¡ˆä¾‹ï¼Œæ‚¨ä¹Ÿå¯èƒ½åªæƒ³é€šéAPIï¼ˆå¦‚GPT-4ï¼‰ç°¡å–®åœ°ä½¿ç”¨æ¨¡å‹ï¼Œæˆ–è€…åœ¨æœ¬åœ°é‹è¡Œå®ƒã€‚ç„¡è«–å“ªç¨®æƒ…æ³ï¼Œé¡å¤–çš„æç¤ºå’ŒæŒ‡å°æŠ€è¡“éƒ½å¯ä»¥æ”¹å–„æˆ–é™åˆ¶æ‚¨çš„æ‡‰ç”¨ç¨‹åºçš„è¼¸å‡ºã€‚

* **LLM APIs**: APIsæ˜¯éƒ¨ç½²LLMsçš„ä¾¿æ·æ–¹å¼ã€‚é€™å€‹é ˜åŸŸåˆ†åˆ¥æœ‰ç§æœ‰LLMs ([OpenAI](https://platform.openai.com/), [Google](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview), [Anthropic](https://docs.anthropic.com/claude/reference/getting-started-with-the-api), [Cohere](https://docs.cohere.com/docs), ç­‰.) å’Œé–‹æºLLMs ([OpenRouter](https://openrouter.ai/), [Hugging Face](https://huggingface.co/inference-api), [Together AI](https://www.together.ai/), ç­‰.).
* **é–‹æº LLMs**: [Hugging Face Hub](https://huggingface.co/models) æ˜¯å°‹æ‰¾LLMsçš„å¥½åœ°æ–¹ã€‚ æ‚¨å¯ä»¥ç›´æ¥åœ¨ [Hugging Face Spaces](https://huggingface.co/spaces)ä¸­é‹è¡Œä¸€äº›æ¨¡å‹ï¼Œ æˆ–è€…ä¸‹è¼‰ä¸¦åœ¨åƒ [LM Studio](https://lmstudio.ai/) é€™æ¨£çš„æ‡‰ç”¨ç¨‹åºä¸­æœ¬åœ°é‹è¡Œä»¥åŠé€šéCLIä½¿ç”¨ [llama.cpp](https://github.com/ggerganov/llama.cpp) æˆ– [Ollama](https://ollama.ai/)ã€‚
* **æç¤ºå·¥ç¨‹ Prompt engineering**: å¸¸è¦‹æŠ€è¡“åŒ…æ‹¬é›¶æç¤ºè©ã€å°‘é‡æç¤ºè©ã€æ€ç¶­éˆèˆ‡ReActã€‚å®ƒå€‘åœ¨æ›´å¤§çš„æ¨¡å‹ä¸Šå·¥ä½œå¾—æ›´å¥½ï¼Œä½†ä¹Ÿå¯ä»¥é©æ‡‰è¼ƒå°çš„æ¨¡å‹ã€‚
* **çµæ§‹åŒ–è¼¸å‡º Structuring outputs**: è¨±å¤šä»»å‹™éœ€è¦çµæ§‹åŒ–çš„è¼¸å‡ºï¼Œå¦‚åš´æ ¼çš„æ¨¡æ¿æˆ–JSONæ ¼å¼ã€‚åƒ [LMQL](https://lmql.ai/), [Outlines](https://github.com/outlines-dev/outlines), [Guidance](https://github.com/guidance-ai/guidance), ç­‰åº«å¯ä»¥ç”¨ä¾†æŒ‡å°ç”Ÿæˆä¸¦éµå¾ªçµ¦å®šçš„çµæ§‹ã€‚

ğŸ“š **åƒè€ƒè³‡æ–™**:
* [Run an LLM locally with LM Studio](https://www.kdnuggets.com/run-an-llm-locally-with-lm-studio) by Nisha Arya: é—œæ–¼å¦‚ä½•ä½¿ç”¨LM Studioçš„ç°¡çŸ­æŒ‡å—ã€‚
* [Prompt engineering guide](https://www.promptingguide.ai/) by DAIR.AI: å¸¶æœ‰ç¤ºä¾‹çš„æç¤ºæŠ€è¡“çš„è©³ç›¡åˆ—è¡¨ã€‚
* [Outlines - Quickstart](https://outlines-dev.github.io/outlines/quickstart/): Outlineså•Ÿç”¨çš„æŒ‡å°ç”ŸæˆæŠ€è¡“çš„åˆ—è¡¨ã€‚
* [LMQL - Overview](https://lmql.ai/docs/language/overview.html): å°LMQLèªè¨€çš„ä»‹ç´¹ã€‚
---
### 2. å»ºç«‹å‘é‡å„²å­˜

å‰µå»ºå‘é‡å„²å­˜æ˜¯å»ºç«‹æª¢ç´¢å¢å¼·ç”Ÿæˆï¼ˆRetrieval Augmented Generationï¼Œç°¡ç¨±RAGï¼‰æµç¨‹çš„ç¬¬ä¸€æ­¥ã€‚æ–‡ä»¶è¢«åŠ è¼‰ã€æ‹†åˆ†ï¼Œä¸¦ä½¿ç”¨ç›¸é—œçš„ç‰‡æ®µä¾†ç”¢ç”Ÿå‘é‡è¡¨ç¤ºï¼ˆåµŒå…¥ï¼‰ï¼Œé€™äº›å‘é‡è¡¨ç¤ºå°‡è¢«å­˜å„²ä»¥ä¾¿åœ¨æ¨ç†éç¨‹ä¸­ä½¿ç”¨ã€‚

* **æ–‡æª”å°å…¥ Ingesting documents**: æ–‡æª”åŠ è¼‰å™¨æ˜¯æ–¹ä¾¿çš„åŒ…è£å™¨ï¼Œå¯ä»¥è™•ç†å¤šç¨®æ ¼å¼ï¼š PDF, JSON, HTML, Markdown, ç­‰ã€‚ å®ƒå€‘é‚„å¯ä»¥ç›´æ¥å¾ä¸€äº›æ•¸æ“šåº«å’ŒAPIï¼ˆGitHub, Reddit, Google Drive, ç­‰ï¼‰æª¢ç´¢æ•¸æ“šã€‚
* **æ–‡æª”æ‹†åˆ† Splitting documents**: æ–‡æœ¬æ‹†åˆ†å™¨å°‡æ–‡æª”æ‹†åˆ†æˆè¼ƒå°çš„ã€èªç¾©ä¸Šæœ‰æ„ç¾©çš„ç‰‡æ®µã€‚é€šå¸¸æœ€å¥½ä¸è¦åœ¨nå€‹å­—ç¬¦å¾Œæ‹†åˆ†æ–‡æœ¬ï¼Œè€Œæ˜¯æŒ‰ç…§æ¨™é¡Œæˆ–éè¿´åœ°æ‹†åˆ†ï¼Œä¸¦é™„å¸¶ä¸€äº›é¡å¤–çš„å…ƒæ•¸æ“šã€‚
* **åµŒå…¥æ¨¡å‹ Embedding models**: åµŒå…¥æ¨¡å‹å°‡æ–‡æœ¬è½‰æ›ç‚ºå‘é‡è¡¨ç¤ºã€‚é€™å…è¨±å°èªè¨€é€²è¡Œæ›´æ·±å…¥ã€æ›´ç´°è†©çš„ç†è§£ï¼Œé€™å°æ–¼é€²è¡Œèªç¾©æœç´¢è‡³é—œé‡è¦ã€‚
* **å‘é‡æ•¸æ“šåº« Vector databases**: å‘é‡æ•¸æ“šåº«ï¼ˆå¦‚ [Chroma](https://www.trychroma.com/), [Pinecone](https://www.pinecone.io/), [Milvus](https://milvus.io/), [FAISS](https://faiss.ai/), [Annoy](https://github.com/spotify/annoy), ç­‰ï¼‰å°ˆç‚ºå„²å­˜åµŒå…¥å‘é‡è€Œè¨­è¨ˆã€‚å®ƒå€‘æ”¯æ´åŸºæ–¼å‘é‡ç›¸ä¼¼æ€§æœ‰æ•ˆæª¢ç´¢èˆ‡æŸ¥è©¢æœ€ç›¸ä¼¼çš„æ•¸æ“šã€‚

ğŸ“š ** åƒè€ƒè³‡æ–™**:
* [LangChain - Text splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/): LangChainå¯¦ç¾çš„ä¸åŒæ–‡æœ¬æ‹†åˆ†å™¨åˆ—è¡¨ã€‚
* [Sentence Transformers library](https://www.sbert.net/): æµè¡Œçš„åµŒå…¥æ¨¡å‹åº«ã€‚
* [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard): åµŒå…¥æ¨¡å‹çš„æ’è¡Œæ¦œã€‚
* [The Top 5 Vector Databases](https://www.datacamp.com/blog/the-top-5-vector-databases) by Moez Ali: æœ€ä½³å’Œæœ€æµè¡Œçš„å‘é‡æ•¸æ“šåº«æ¯”è¼ƒã€‚

---
### 3. Retrieval Augmented Generation æª¢ç´¢å¢å¼·ç”Ÿæˆ

å€ŸåŠ© RAGï¼ŒLLMs å¯ä»¥å¾è³‡æ–™åº«ä¸­æª¢ç´¢ä¸Šä¸‹æ–‡æ–‡æª”ï¼Œä»¥æé«˜ç­”æ¡ˆçš„æº–ç¢ºæ€§ã€‚RAG æ˜¯ä¸€ç¨®ç„¡éœ€ä»»ä½•å¾®èª¿å³å¯å¢å¼·æ¨¡å‹çŸ¥è­˜çš„æµè¡Œæ–¹æ³•ã€‚

* **Orchestrators å”ä½œå™¨**: Orchestrators å”ä½œå™¨ (å¦‚ [LangChain](https://python.langchain.com/docs/get_started/introduction), [LlamaIndex](https://docs.llamaindex.ai/en/stable/), [FastRAG](https://github.com/IntelLabs/fastRAG), ç­‰ï¼‰æ˜¯æµè¡Œçš„æ¡†æ¶ï¼Œç”¨æ–¼å°‡æ‚¨çš„ LLM èˆ‡å·¥å…·ã€è³‡æ–™åº«ã€è¨˜æ†¶é«”ç­‰é€£æ¥èµ·ä¾†ä¸¦å¢å¼·ä»–å€‘çš„èƒ½åŠ›ã€‚
* **Retrievers æª¢ç´¢å™¨**: ä½¿ç”¨è€…æŒ‡ä»¤æœªé‡å°æª¢ç´¢é€²è¡Œæœ€ä½³åŒ–ã€‚å¯ä»¥æ‡‰ç”¨ä¸åŒçš„æŠ€è¡“ï¼ˆä¾‹å¦‚ï¼Œå¤šæŸ¥è©¢æª¢ç´¢å™¨ã€ [HyDE](https://arxiv.org/abs/2212.10496), ç­‰ï¼‰ä¾†é‡æ–°è¡¨è¿°/æ“´å±•å®ƒå€‘ä¸¦æé«˜æ•ˆèƒ½ã€‚
* **è¨˜æ†¶**: ç‚ºäº†è¨˜ä½å…ˆå‰çš„èªªæ˜å’Œç­”æ¡ˆï¼ŒLLM å’Œ ChatGPT ç­‰èŠå¤©æ©Ÿå™¨äººæœƒå°‡æ­¤æ­·å²è¨˜éŒ„æ·»åŠ åˆ°å…¶ä¸Šä¸‹æ–‡è¦–çª—ä¸­ã€‚æ­¤ç·©è¡å€å¯ä»¥é€éåŒ¯ç¸½ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨è¼ƒå°çš„ LLMï¼‰ã€å‘é‡å„²å­˜ + RAG ç­‰ä¾†æ”¹é€²ã€‚
* **è©•ä¼°**: æˆ‘å€‘éœ€è¦è©•ä¼°æ–‡ä»¶æª¢ç´¢ï¼ˆä¸Šä¸‹æ–‡ç²¾ç¢ºåº¦å’Œå¬å›ç‡ï¼‰å’Œç”Ÿæˆéšæ®µï¼ˆå¯ä¿¡åº¦å’Œç­”æ¡ˆç›¸é—œæ€§ï¼‰ã€‚å¯ä»¥ä½¿ç”¨ [Ragas](https://github.com/explodinggradients/ragas/tree/main) å’Œ [DeepEval](https://github.com/confident-ai/deepeval)å·¥å…·é€²è¡Œç°¡åŒ–ã€‚

ğŸ“š **åƒè€ƒè³‡æ–™**:
* [Llamaindex - High-level concepts](https://docs.llamaindex.ai/en/stable/getting_started/concepts.html): å»ºé€  RAG ç®¡é“æ™‚éœ€è¦äº†è§£çš„ä¸»è¦æ¦‚å¿µã€‚
* [Pinecone - Retrieval Augmentation](https://www.pinecone.io/learn/series/langchain/langchain-retrieval-augmentation/): æª¢ç´¢å¢å¼·æµç¨‹æ¦‚è¿°ã€‚
* [LangChain - Q&A with RAG](https://python.langchain.com/docs/use_cases/question_answering/quickstart): å»ºç«‹å…¸å‹ RAG ç®¡é“çš„é€æ­¥æ•™å­¸ã€‚
* [LangChain - Memory types](https://python.langchain.com/docs/modules/memory/types/): ä¸åŒé¡å‹è¨˜æ†¶é«”åŠå…¶ç›¸é—œç”¨é€”çš„æ¸…å–®ã€‚
* [RAG pipeline - Metrics](https://docs.ragas.io/en/stable/concepts/metrics/index.html): ç”¨æ–¼è©•ä¼° RAG ç®¡é“çš„ä¸»è¦æŒ‡æ¨™çš„æ¦‚è¿°ã€‚
---
### 4. é€²éš RAG

ç¾å¯¦æ‡‰ç”¨ç¨‹å¼å¯èƒ½éœ€è¦è¤‡é›œçš„ç®¡é“ï¼ŒåŒ…æ‹¬ SQL æˆ–åœ–å½¢è³‡æ–™åº«ï¼Œä»¥åŠè‡ªå‹•é¸æ“‡ç›¸é—œå·¥å…·å’Œ APIã€‚é€™äº›å…ˆé€²æŠ€è¡“å¯ä»¥æ”¹é€²åŸºæº–è§£æ±ºæ–¹æ¡ˆä¸¦æä¾›é™„åŠ åŠŸèƒ½ã€‚

* **æŸ¥è©¢å»ºæ§‹**: å„²å­˜åœ¨å‚³çµ±æ•¸æ“šåº«ä¸­çš„çµæ§‹åŒ–æ•¸æ“šéœ€è¦ç‰¹å®šçš„æŸ¥è©¢èªè¨€ï¼Œå¦‚SQLã€Cypherã€å…ƒæ•¸æ“šç­‰ã€‚æˆ‘å€‘å¯ä»¥ç›´æ¥å°‡ç”¨æˆ¶æŒ‡ä»¤ç¿»è­¯æˆæŸ¥è©¢ï¼Œé€šéæŸ¥è©¢å»ºæ§‹ä¾†å­˜å–æ•¸æ“šã€‚
* **ä»£ç†èˆ‡å·¥å…·**: ä»£ç†é€éè‡ªå‹•é¸æ“‡æœ€ç›¸é—œçš„å·¥å…·ä¾†å¢å¼·LLMsçš„å›ç­”èƒ½åŠ›ã€‚é€™äº›å·¥å…·å¯ä»¥åƒä½¿ç”¨Googleæˆ–Wikipediaé‚£éº¼ç°¡å–®ï¼Œæˆ–è€…åƒPythonè§£é‡‹å™¨æˆ–Jiraé€™æ¨£è¤‡é›œã€‚
* **å¾Œè™•ç†**: è™•ç†è¼¸å…¥åˆ°LLMçš„æœ€å¾Œä¸€æ­¥ã€‚å®ƒé€šéé‡æ–°æ’åºã€[RAGèåˆ](https://github.com/Raudaschl/rag-fusion)å’Œåˆ†é¡ä¾†å¢å¼·æª¢ç´¢æ–‡æª”çš„ç›¸é—œæ€§å’Œå¤šæ¨£æ€§ã€‚

ğŸ“š **åƒè€ƒè³‡æ–™**:
* [LangChain - Query ConstructionæŸ¥è©¢å»ºæ§‹](https://blog.langchain.dev/query-construction/): é—œæ–¼ä¸åŒé¡å‹æŸ¥è©¢å»ºæ§‹çš„åšå®¢æ–‡ç« .
* [LangChain - SQL](https://python.langchain.com/docs/use_cases/qa_structured/sql): æ•™ç¨‹ï¼Œä»‹ç´¹å¦‚ä½•åˆ©ç”¨LLMsèˆ‡SQLæ•¸æ“šåº«äº’å‹•ï¼ŒåŒ…æ‹¬Text-to-SQLå’Œå¯é¸çš„SQLä»£ç†ã€‚
* [Pinecone - LLM agents(ä»£ç†)](https://www.pinecone.io/learn/series/langchain/langchain-agents/): ä»‹ç´¹ä¸åŒé¡å‹çš„ä»£ç†å’Œå·¥å…·ã€‚
* [LLM Powered Autonomous Agents(ä»£ç†)](https://lilianweng.github.io/posts/2023-06-23-agent/) by Lilian Weng: é—œæ–¼LLMä»£ç†çš„æ›´ç†è«–æ€§æ–‡ç« ã€‚
* [LangChain - OpenAI's RAG](https://blog.langchain.dev/applying-openai-rag/): æ¦‚è¿°OpenAIä½¿ç”¨çš„RAGç­–ç•¥ï¼ŒåŒ…æ‹¬å¾Œè™•ç†ã€‚

---
### 5. Inference optimization

Text generation is a costly process that requires expensive hardware. In addition to quantization, various techniques have been proposed to maximize throughput and reduce inference costs.

* **Flash Attention**: Optimization of the attention mechanism to transform its complexity from quadratic to linear, speeding up both training and inference.
* **Key-value cache**: Understand the key-value cache and the improvements introduced in [Multi-Query Attention](https://arxiv.org/abs/1911.02150) (MQA) and [Grouped-Query Attention](https://arxiv.org/abs/2305.13245) (GQA).
* **Speculative decoding**: Use a small model to produce drafts that are then reviewed by a larger model to speed up text generation.

ğŸ“š **References**:
* [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one) by Hugging Face: Explain how to optimize inference on GPUs.
* [LLM Inference](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices) by Databricks: Best practices for how to optimize LLM inference in production.
* [Optimizing LLMs for Speed and Memory](https://huggingface.co/docs/transformers/main/en/llm_tutorial_optimization) by Hugging Face: Explain three main techniques to optimize speed and memory, namely quantization, Flash Attention, and architectural innovations.
* [Assisted Generation](https://huggingface.co/blog/assisted-generation) by Hugging Face: HF's version of speculative decoding, it's an interesting blog post about how it works with code to implement it.

---
### 6. Deploying LLMs

Deploying LLMs at scale is an engineering feat that can require multiple clusters of GPUs. In other scenarios, demos and local apps can be achieved with a much lower complexity. 

* **Local deployment**: Privacy is an important advantage that open-source LLMs have over private ones. Local LLM servers ([LM Studio](https://lmstudio.ai/), [Ollama](https://ollama.ai/), [oobabooga](https://github.com/oobabooga/text-generation-webui), [kobold.cpp](https://github.com/LostRuins/koboldcpp), etc.) capitalize on this advantage to power local apps. 
* **Demo deployment**: Frameworks like [Gradio](https://www.gradio.app/) and [Streamlit](https://docs.streamlit.io/) are helpful to prototype applications and share demos. You can also easily host them online, for example using [Hugging Face Spaces](https://huggingface.co/spaces).
* **Server deployment**: Deploy LLMs at scale requires cloud (see also [SkyPilot](https://skypilot.readthedocs.io/en/latest/)) or on-prem infrastructure and often leverage optimized text generation frameworks like [TGI](https://github.com/huggingface/text-generation-inference), [vLLM](https://github.com/vllm-project/vllm/tree/main), etc.
* **Edge deployment**: In constrained environments, high-performance frameworks like [MLC LLM](https://github.com/mlc-ai/mlc-llm) and [mnn-llm](https://github.com/wangzhaode/mnn-llm/blob/master/README_en.md) can deploy LLM in web browsers, Android, and iOS.

ğŸ“š **References**:
* [Streamlit - Build a basic LLM app](https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps): Tutorial to make a basic ChatGPT-like app using Streamlit.
* [HF LLM Inference Container](https://huggingface.co/blog/sagemaker-huggingface-llm): Deploy LLMs on Amazon SageMaker using Hugging Face's inference container.
* [PhilschmidÂ blog](https://www.philschmid.de/) by Philipp Schmid: Collection of high-quality articles about LLM deployment using Amazon SageMaker.
* [Optimizing latence](https://hamel.dev/notes/llm/inference/03_inference.html) by Hamel Husain: Comparison of TGI, vLLM, CTranslate2, and mlc in terms of throughput and latency.

---
### 7. Securing LLMs

In addition to traditional security problems associated with software, LLMs have unique weaknesses due to the way they are trained and prompted.

* **Prompt hacking**: Different techniques related to prompt engineering, including prompt injection (additional instruction to hijack the model's answer), data/prompt leaking (retrieve its original data/prompt), and jailbreaking (craft prompts to bypass safety features).
* **Backdoors**: Attack vectors can target the training data itself, by poisoning the training data (e.g., with false information) or creating backdoors (secret triggers to change the model's behavior during inference).
* **Defensive measures**: The best way to protect your LLM applications is to test them against these vulnerabilities (e.g., using red teaming and checks like [garak](https://github.com/leondz/garak/)) and observe them in production (with a framework like [langfuse](https://github.com/langfuse/langfuse)).

ğŸ“š **References**:
* [OWASP LLM Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/) by HEGO Wiki: List of the 10 most critic vulnerabilities seen in LLM applications.
* [Prompt Injection Primer](https://github.com/jthack/PIPE) by Joseph Thacker: Short guide dedicated to prompt injection for engineers.
* [LLM Security](https://llmsecurity.net/) by [@llm_sec](https://twitter.com/llm_sec): Extensive list of resources related to LLM security.
* [Red teaming LLMs](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/red-teaming) by Microsoft: Guide on how to perform red teaming with LLMs.
---
## Acknowledgements

This roadmap was inspired by the excellent [DevOps Roadmap](https://github.com/milanm/DevOps-Roadmap) from Milan MilanoviÄ‡ and Romano Roth.

Special thanks to:

* Thomas Thelen for motivating me to create a roadmap
* AndrÃ© Frade for his input and review of the first draft
* Dino Dunn for providing resources about LLM security

*Disclaimer: I am not affiliated with any sources listed here.*

---
<p align="center">
  <a href="https://star-history.com/#mlabonne/llm-course&Date">
    <img src="https://api.star-history.com/svg?repos=mlabonne/llm-course&type=Date" alt="Star History Chart">
  </a>
</p>

2. ğŸ§‘â€ğŸ”¬ **LLM æ¨¡å‹å·¥ç¨‹** å°ˆæ³¨åœ¨ä½¿ç”¨æœ€æ–°çš„æŠ€å·§,æŠ€è¡“æ­å»ºç¾éšæ®µå€‹äººå¯å¯¦ç¾æœ€å¥½çš„LLMs.

## ğŸ“ Notebooks

èˆ‡å¤§å‹èªè¨€æ¨¡å‹ç›¸é—œçš„ colab notebook å’Œæ–‡ç« æ¸…å–®ã€‚

### Tools

| åç¨± | æ•˜è¿° | é€£çµ |
|----------|-------------|----------|
| ğŸ§ [LLM AutoEval](https://github.com/mlabonne/llm-autoeval) | ä½¿ç”¨ RunPod è‡ªå‹•è©•ä¼°æ‚¨çš„LLMs | <a href="https://colab.research.google.com/drive/1Igs3WZuXAIv9X0vwqiE90QlEPys8e8Oa?usp=sharing"><img src="../../img/colab.svg" alt="Open In Colab"></a> |
| ğŸ¥± LazyMergekit | ä½¿ç”¨ mergekit ä¸€éµè¼•é¬†åˆä½µæ¨¡å‹. | <a href="https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing"><img src="../../img/colab.svg" alt="Open In Colab"></a> |
| âš¡ AutoGGUF | ä¸€éµé‡åŒ– GGUF æ ¼å¼çš„ LLM. | <a href="https://colab.research.google.com/drive/1P646NEg33BZy4BfLDNpTz0V0lwIU3CHu?usp=sharing"><img src="../../img/colab.svg" alt="Open In Colab"></a> |
| ğŸŒ³ Model Family Tree | å¯è¦–åŒ–åˆä½µæ¨¡å‹çš„æ¨¹ç‹€çµæ§‹åœ–. | <a href="https://colab.research.google.com/drive/1s2eQlolcI1VGgDhqWIANfkfKvcKrMyNr?usp=sharing"><img src="../../img/colab.svg" alt="Open In Colab"></a> |

### Fine-tuning (å¾®èª¿)

| åç¨± | æ•˜è¿° | æ–‡ç«  | é€£çµ |
|---------------------------------------|-------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|
| Fine-tune Llama 2 in Google Colab | å¾®èª¿æ‚¨çš„ç¬¬ä¸€å€‹ Llama 2 æ¨¡å‹çš„é€æ­¥æŒ‡å—ã€‚ | [Article](https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html) | <a href="https://colab.research.google.com/drive/1PEQyJO1-f6j0S_XJ8DV50NkpzasXkrzd?usp=sharing"><img src="../../img/colab.svg" alt="Open In Colab"></a> |
| Fine-tune LLMs with Axolotl | æœ€å…ˆé€²å¾®èª¿å·¥å…·çš„ç«¯åˆ°ç«¯æŒ‡å—ã€‚| [Article](https://mlabonne.github.io/blog/posts/A_Beginners_Guide_to_LLM_Finetuning.html) | <a href="https://colab.research.google.com/drive/1Xu0BrCB7IShwSWKVcfAfhehwjDrDMH5m?usp=sharing"><img src="../../img/colab.svg" alt="Open In Colab"></a> |
| Fine-tune Mistral-7b with DPO | ä½¿ç”¨ DPO æå‡ç›£ç£å¾®èª¿æ¨¡å‹çš„æ€§èƒ½ | [Article](https://medium.com/towards-data-science/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac) | <a href="https://colab.research.google.com/drive/15iFBr1xWgztXvhrj5I9fBv20c7CFOPBE?usp=sharing"><img src="../../img/colab.svg" alt="Open In Colab"></a> |

### Quantization(é‡åŒ–)

| åç¨± | æ•˜è¿° | æ–‡ç«  | é€£çµ |
|---------------------------------------|-------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|
| 1. Introduction to Quantization | ä½¿ç”¨ 8 ä½å…ƒé‡åŒ–çš„å¤§å‹èªè¨€æ¨¡å‹æœ€ä½³åŒ–ã€‚ | [Article](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html) | <a href="https://colab.research.google.com/drive/1DPr4mUQ92Cc-xf4GgAaB6dFcFnWIvqYi?usp=sharing"><img src="../../img/colab.svg" alt="Open In Colab"></a> |
| 2. 4-bit Quantization using GPTQ | é‡åŒ–æ‚¨è‡ªå·±çš„é–‹æº LLM ä»¥åœ¨æ¶ˆè²»æ€§ç¡¬é«”ä¸Šé‹è¡Œå®ƒå€‘ã€‚ | [Article](https://mlabonne.github.io/blog/4bit_quantization/) | <a href="https://colab.research.google.com/drive/1lSvVDaRgqQp_mWK_jC9gydz6_-y6Aq4A?usp=sharing"><img src="../../img/colab.svg" alt="Open In Colab"></a> |
| 3. Quantization with GGUF and llama.cpp | ä½¿ç”¨ llama.cpp é‡åŒ– Llama 2 æ¨¡å‹ä¸¦å°‡ GGUF ç‰ˆæœ¬ä¸Šå‚³åˆ° HF Hubã€‚ | [Article](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html) | <a href="https://colab.research.google.com/drive/1pL8k7m04mgE5jo2NrjGi8atB0j_37aDD?usp=sharing"><img src="../../img/colab.svg" alt="Open In Colab"></a> |
| 4. ExLlamaV2: The Fastest Library to RunÂ LLMs | é‡åŒ–ä¸¦åŸ·è¡Œ EXL2 æ¨¡å‹ä¸¦å°‡å…¶ä¸Šå‚³è‡³ HF Hubã€‚| [Article](https://mlabonne.github.io/blog/posts/ExLlamaV2_The_Fastest_Library_to_Run%C2%A0LLMs.html) | <a href="https://colab.research.google.com/drive/1yrq4XBlxiA0fALtMoT2dwiACVc77PHou?usp=sharing"><img src="../../img/colab.svg" alt="Open In Colab"></a> |

### å…¶ä»–

| åç¨± | æ•˜è¿° | æ–‡ç«  | é€£çµ |
|---------------------------------------|-------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|
| Decoding Strategies in Large Language Models | å¾æ³¢æŸæœå°‹(beam search)åˆ°æ ¸æ¡æ¨£(nucleus sampling)çš„æ–‡æœ¬ç”ŸæˆæŒ‡å—| [Article](https://mlabonne.github.io/blog/posts/2022-06-07-Decoding_strategies.html) | <a href="https://colab.research.google.com/drive/19CJlOS5lI29g-B3dziNn93Enez1yiHk2?usp=sharing"><img src="../../img/colab.svg" alt="Open In Colab"></a> |
| Visualizing GPT-2's Loss Landscape | åŸºæ–¼æ¬Šé‡æ“¾å‹•çš„æå¤±æ™¯è§€ä¸‰ç¶­åœ–(3D plot of the loss landscape based on weight perturbations.)| [Tweet](https://twitter.com/maximelabonne/status/1667618081844219904) | <a href="https://colab.research.google.com/drive/1Fu1jikJzFxnSPzR_V2JJyDVWWJNXssaL?usp=sharing"><img src="../../img/colab.svg" alt="Open In Colab"></a> |
| Improve ChatGPT with Knowledge Graphs | ç”¨çŸ¥è­˜åœ–è­œå¢å¼· ChatGPT çš„ç­”æ¡ˆ | [Article](https://mlabonne.github.io/blog/posts/Article_Improve_ChatGPT_with_Knowledge_Graphs.html) | <a href="https://colab.research.google.com/drive/1mwhOSw9Y9bgEaIFKT4CLi0n18pXRM4cj?usp=sharing"><img src="../../img/colab.svg" alt="Open In Colab"></a> |
| Merge LLMs with mergekit | è¼•é¬†å‰µå»ºæ‚¨è‡ªå·±çš„æ¨¡å‹ï¼Œç„¡éœ€ GPUï¼| [Article](https://towardsdatascience.com/merge-large-language-models-with-mergekit-2118fb392b54) | <a href="https://colab.research.google.com/drive/1_JS7JKJAQozD48-LhYdegcuuZ2ddgXfr?usp=sharing"><img src="../../img/colab.svg" alt="Open In Colab"></a> |


## ğŸ§‘â€ğŸ”¬ LLM æ¨¡å‹å·¥ç¨‹

æœ¬èª²ç¨‹çš„é€™ä¸€éƒ¨åˆ†é‡é»åœ¨æ–¼å­¸ç¿’å¦‚ä½•ä½¿ç”¨æœ€æ–°æŠ€è¡“ä¾†å»ºç«‹æœ€å¥½çš„ LLMsã€‚

![](../img/roadmap_scientist.png)

### 1. The LLM æ¶æ§‹

é›–ç„¶ä¸éœ€è¦æ·±å…¥äº†è§£ Transformer æ¶æ§‹ï¼Œä½†æ·±å…¥äº†è§£å…¶è¼¸å…¥ï¼ˆtokens ä»¤ç‰Œï¼‰å’Œè¼¸å‡ºï¼ˆlogitsï¼‰éå¸¸é‡è¦ã€‚æ™®é€šçš„æ³¨æ„åŠ›æ©Ÿåˆ¶æ˜¯å¦ä¸€å€‹éœ€è¦æŒæ¡çš„é—œéµçµ„æˆéƒ¨åˆ†ï¼Œç¨å¾Œæœƒä»‹ç´¹å®ƒçš„æ”¹é€²ç‰ˆæœ¬ã€‚

* **é«˜éšè¦–åœ– High-level view**: é‡æ–°å¯©è¦–ç·¨ç¢¼å™¨-è§£ç¢¼å™¨ Transformer æ¶æ§‹ï¼Œæ›´å…·é«”åœ°èªªï¼Œæ˜¯åƒ…è§£ç¢¼å™¨çš„ GPT æ¶æ§‹ï¼Œè©²æ¶æ§‹åœ¨æ¯å€‹æœ€è¿‘çš„ LLM ä¸­åŸºæœ¬éƒ½æœ‰ä½¿ç”¨ã€‚
    -[LLM Foundations](https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/llm-foundations/)
    -[Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY)
    -[LLM-from-scratch.ipynb](https://colab.research.google.com/gist/iamaziz/171170dce60d9cd07fab221507fd1d52)
* **æ¨™è¨˜åŒ– Tokenization**: äº†è§£å¦‚ä½•å°‡åŸå§‹æ–‡å­—è³‡æ–™è½‰æ›ç‚ºæ¨¡å‹å¯ä»¥ç†è§£çš„æ ¼å¼ï¼Œé€™æ¶‰åŠå°‡æ–‡å­—æ‹†åˆ†ç‚ºæ¨™è¨˜ï¼ˆé€šå¸¸æ˜¯å–®å­—æˆ–å­å–®å­—ï¼‰ã€‚
* **æ³¨æ„åŠ›æ©Ÿåˆ¶**: æŒæ¡æ³¨æ„åŠ›æ©Ÿåˆ¶èƒŒå¾Œçš„ç†è«–ï¼ŒåŒ…æ‹¬è‡ªè¨»æ„åŠ›å’Œç¸®æ”¾é»ç©æ³¨æ„åŠ›ï¼Œé€™ä½¿å¾—æ¨¡å‹åœ¨ç”¢ç”Ÿè¼¸å‡ºæ™‚èƒ½å¤ å°ˆæ³¨æ–¼è¼¸å…¥çš„ä¸åŒéƒ¨åˆ†ã€‚
* **æ–‡å­—ç”Ÿæˆ**: äº†è§£æ¨¡å‹ç”¢ç”Ÿè¼¸å‡ºåºåˆ—çš„ä¸åŒæ–¹å¼ã€‚å¸¸è¦‹çš„ç­–ç•¥åŒ…æ‹¬è²ªå©ªè§£ç¢¼(greedy decoding)ã€æ³¢æŸæœå°‹(beam searc)ã€top-k æ¡æ¨£(top-k sampling)å’Œæ ¸æ¡æ¨£(nucleus sampling)ã€‚
- å²ä¸¹ä½›çš„Transformersèª²ï¼Œå¾æ¶æ§‹åˆ°æ‡‰ç”¨éƒ½æœ‰ [CS25: Transformers United V3](https://web.stanford.edu/class/cs25/)
- Youtube Transformers United ä¸Šèª²éŒ„å½±[Stanford CS25: V1 I Transformers United: DL Models that have revolutionized NLP, CV, RL](https://www.youtube.com/watch?v=P127jhj-8-Y&list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM)

ğŸ“š **åƒè€ƒè³‡æ–™**:
- [Building LLMs from Scratch](https://youtu.be/UU1WVnMk4E8?si=Vn1IbHE5p5LUQmKi) å¾é›¶é–‹å§‹ build LLMsã€‚
- [Transformer æ’åœ–](https://jalammar.github.io/illustrated-transformer/) by Jay Alammar: Transformer æ¨¡å‹çš„ç›´è§€è§£é‡‹ã€‚
- [GPT-2åœ–è§£](https://jalammar.github.io/illustrated-gpt2/) by Jay Alammar: æ­¤æ–‡æ¯”ä¸Šä¸€ç¯‡æ–‡ç« æƒ³å°æ›´é‡è¦äº›ï¼Œå®ƒå°ˆæ³¨æ–¼å’Œ Llama éå¸¸ç›¸ä¼¼çš„ GPT æ¶æ§‹ã€‚
- [LLM Visualization](https://bbycroft.net/llm) by Brendan Bycroft: ä»¥ 3D è¦–è¦ºåŒ–æ–¹å¼å‘ˆç¾ LLM å…§éƒ¨ç™¼ç”Ÿçš„æƒ…æ³ã€‚
* [nanoGPT](https://www.youtube.com/watch?v=kCc8FmEb1nY) by Andrej Karpathy:ä¸€æ®µ 2 å°æ™‚é•·çš„ YouTube å½±ç‰‡ï¼Œç”¨æ–¼å¾é ­é–‹å§‹é‡æ–°å¯¦ç¾ GPTï¼ˆä»¥ç¨‹å¼è¨­è¨ˆå¸«çš„è¦–è§’ï¼‰ã€‚
* [Attention? Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/) by Lilian Weng: ä»¥æ›´æ­£å¼çš„æ–¹å¼ä»‹ç´¹æ³¨æ„åŠ›çš„å¿…è¦æ€§ã€‚
* [Decoding Strategies in LLMs](https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html): æä¾›å°ç”Ÿæˆæ–‡æœ¬çš„ä¸åŒè§£ç¢¼ç­–ç•¥çš„åœ–åƒåŒ–ä»‹ç´¹ä»¥åŠç¨‹å¼ç¢¼ã€‚

2. ğŸ§‘â€ğŸ”¬ **LLM æ¨¡å‹å·¥ç¨‹** å°ˆæ³¨åœ¨ä½¿ç”¨æœ€æ–°çš„æŠ€å·§,æŠ€è¡“æ­å»ºç¾éšæ®µå€‹äººå¯å¯¦ç¾æœ€å¥½çš„LLMs.

## ğŸ“ Notebooks

èˆ‡å¤§å‹èªè¨€æ¨¡å‹ç›¸é—œçš„ colab notebook å’Œæ–‡ç« æ¸…å–®ã€‚

### Tools

| åç¨± | æ•˜è¿° | é€£çµ |
|----------|-------------|----------|
| ğŸ§ [LLM AutoEval](https://github.com/mlabonne/llm-autoeval) | ä½¿ç”¨ RunPod è‡ªå‹•è©•ä¼°æ‚¨çš„LLMs | <a href="https://colab.research.google.com/drive/1Igs3WZuXAIv9X0vwqiE90QlEPys8e8Oa?usp=sharing"><img src="../img/colab.svg" alt="Open In Colab"></a> |
| ğŸ¥± LazyMergekit | ä½¿ç”¨ mergekit ä¸€éµè¼•é¬†åˆä½µæ¨¡å‹. | <a href="https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing"><img src="../img/colab.svg" alt="Open In Colab"></a> |
| âš¡ AutoGGUF | ä¸€éµé‡åŒ– GGUF æ ¼å¼çš„ LLM. | <a href="https://colab.research.google.com/drive/1P646NEg33BZy4BfLDNpTz0V0lwIU3CHu?usp=sharing"><img src="../img/colab.svg" alt="Open In Colab"></a> |
| ğŸŒ³ Model Family Tree | å¯è¦–åŒ–åˆä½µæ¨¡å‹çš„æ¨¹ç‹€çµæ§‹åœ–. | <a href="https://colab.research.google.com/drive/1s2eQlolcI1VGgDhqWIANfkfKvcKrMyNr?usp=sharing"><img src="../img/colab.svg" alt="Open In Colab"></a> |

### Fine-tuning (å¾®èª¿)

| åç¨± | æ•˜è¿° | æ–‡ç«  | é€£çµ |
|---------------------------------------|-------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|
| Fine-tune Llama 2 in Google Colab | å¾®èª¿æ‚¨çš„ç¬¬ä¸€å€‹ Llama 2 æ¨¡å‹çš„é€æ­¥æŒ‡å—ã€‚ | [Article](https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html) | <a href="https://colab.research.google.com/drive/1PEQyJO1-f6j0S_XJ8DV50NkpzasXkrzd?usp=sharing"><img src="../img/colab.svg" alt="Open In Colab"></a> |
| Fine-tune LLMs with Axolotl | æœ€å…ˆé€²å¾®èª¿å·¥å…·çš„ç«¯åˆ°ç«¯æŒ‡å—ã€‚| [Article](https://mlabonne.github.io/blog/posts/A_Beginners_Guide_to_LLM_Finetuning.html) | <a href="https://colab.research.google.com/drive/1Xu0BrCB7IShwSWKVcfAfhehwjDrDMH5m?usp=sharing"><img src="../img/colab.svg" alt="Open In Colab"></a> |
| Fine-tune Mistral-7b with DPO | ä½¿ç”¨ DPO æå‡ç›£ç£å¾®èª¿æ¨¡å‹çš„æ€§èƒ½ | [Article](https://medium.com/towards-data-science/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac) | <a href="https://colab.research.google.com/drive/15iFBr1xWgztXvhrj5I9fBv20c7CFOPBE?usp=sharing"><img src="../img/colab.svg" alt="Open In Colab"></a> |

### Quantization(é‡åŒ–)

| åç¨± | æ•˜è¿° | æ–‡ç«  | é€£çµ |
|---------------------------------------|-------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|
| 1. Introduction to Quantization | ä½¿ç”¨ 8 ä½å…ƒé‡åŒ–çš„å¤§å‹èªè¨€æ¨¡å‹æœ€ä½³åŒ–ã€‚ | [Article](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html) | <a href="https://colab.research.google.com/drive/1DPr4mUQ92Cc-xf4GgAaB6dFcFnWIvqYi?usp=sharing"><img src="../img/colab.svg" alt="Open In Colab"></a> |
| 2. 4-bit Quantization using GPTQ | é‡åŒ–æ‚¨è‡ªå·±çš„é–‹æº LLM ä»¥åœ¨æ¶ˆè²»æ€§ç¡¬é«”ä¸Šé‹è¡Œå®ƒå€‘ã€‚ | [Article](https://mlabonne.github.io/blog/4bit_quantization/) | <a href="https://colab.research.google.com/drive/1lSvVDaRgqQp_mWK_jC9gydz6_-y6Aq4A?usp=sharing"><img src="../img/colab.svg" alt="Open In Colab"></a> |
| 3. Quantization with GGUF and llama.cpp | ä½¿ç”¨ llama.cpp é‡åŒ– Llama 2 æ¨¡å‹ä¸¦å°‡ GGUF ç‰ˆæœ¬ä¸Šå‚³åˆ° HF Hubã€‚ | [Article](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html) | <a href="https://colab.research.google.com/drive/1pL8k7m04mgE5jo2NrjGi8atB0j_37aDD?usp=sharing"><img src="../img/colab.svg" alt="Open In Colab"></a> |
| 4. ExLlamaV2: The Fastest Library to RunÂ LLMs | é‡åŒ–ä¸¦åŸ·è¡Œ EXL2 æ¨¡å‹ä¸¦å°‡å…¶ä¸Šå‚³è‡³ HF Hubã€‚| [Article](https://mlabonne.github.io/blog/posts/ExLlamaV2_The_Fastest_Library_to_Run%C2%A0LLMs.html) | <a href="https://colab.research.google.com/drive/1yrq4XBlxiA0fALtMoT2dwiACVc77PHou?usp=sharing"><img src="../img/colab.svg" alt="Open In Colab"></a> |

### å…¶ä»–

| åç¨± | æ•˜è¿° | æ–‡ç«  | é€£çµ |
|---------------------------------------|-------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|
| Decoding Strategies in Large Language Models | å¾æ³¢æŸæœå°‹(beam search)åˆ°æ ¸æ¡æ¨£(nucleus sampling)çš„æ–‡æœ¬ç”ŸæˆæŒ‡å—| [Article](https://mlabonne.github.io/blog/posts/2022-06-07-Decoding_strategies.html) | <a href="https://colab.research.google.com/drive/19CJlOS5lI29g-B3dziNn93Enez1yiHk2?usp=sharing"><img src="../img/colab.svg" alt="Open In Colab"></a> |
| Visualizing GPT-2's Loss Landscape | åŸºæ–¼æ¬Šé‡æ“¾å‹•çš„æå¤±æ™¯è§€ä¸‰ç¶­åœ–(3D plot of the loss landscape based on weight perturbations.)| [Tweet](https://twitter.com/maximelabonne/status/1667618081844219904) | <a href="https://colab.research.google.com/drive/1Fu1jikJzFxnSPzR_V2JJyDVWWJNXssaL?usp=sharing"><img src="../img/colab.svg" alt="Open In Colab"></a> |
| Improve ChatGPT with Knowledge Graphs | ç”¨çŸ¥è­˜åœ–è­œå¢å¼· ChatGPT çš„ç­”æ¡ˆ | [Article](https://mlabonne.github.io/blog/posts/Article_Improve_ChatGPT_with_Knowledge_Graphs.html) | <a href="https://colab.research.google.com/drive/1mwhOSw9Y9bgEaIFKT4CLi0n18pXRM4cj?usp=sharing"><img src="../img/colab.svg" alt="Open In Colab"></a> |
| Merge LLMs with mergekit | è¼•é¬†å‰µå»ºæ‚¨è‡ªå·±çš„æ¨¡å‹ï¼Œç„¡éœ€ GPUï¼| [Article](https://towardsdatascience.com/merge-large-language-models-with-mergekit-2118fb392b54) | <a href="https://colab.research.google.com/drive/1_JS7JKJAQozD48-LhYdegcuuZ2ddgXfr?usp=sharing"><img src="../img/colab.svg" alt="Open In Colab"></a> |



### 2. æ§‹å»ºä¸€å€‹ç¯„ä¾‹(æˆ–æŒ‡ä»¤)è³‡æ–™é›† instruction dataset

é›–ç„¶å¾ç¶­åŸºç™¾ç§‘å’Œå…¶ä»–ç¶²ç«™æ‰¾åˆ°åŸå§‹è³‡æ–™å¾ˆå®¹æ˜“ï¼Œä½†åœ¨å¾ˆå¤šçš„ç’°å¢ƒä¸­æ”¶é›†æˆå°çš„æŒ‡ç¤º,ç¯„ä¾‹å’Œç­”æ¡ˆå»å¾ˆå›°é›£ã€‚èˆ‡å‚³çµ±æ©Ÿå™¨å­¸ç¿’ä¸€æ¨£ï¼Œè³‡æ–™é›†çš„å“è³ªå°‡ç›´æ¥å½±éŸ¿æ¨¡å‹çš„å“è³ªï¼Œé€™å°±æ˜¯ç‚ºä»€éº¼å®ƒå¯èƒ½æ˜¯å¾®èª¿éç¨‹ä¸­æœ€é‡è¦çš„çµ„æˆéƒ¨åˆ†ã€‚

* **[é¡ä¼¼ Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html)-çš„è³‡æ–™é›†**: ä½¿ç”¨ OpenAI API (GPT) å¾é ­é–‹å§‹ç”¢ç”Ÿåˆæˆè³‡æ–™ã€‚æ‚¨å¯ä»¥æŒ‡å®šç¨®å­å’Œç³»çµ±æç¤ºä¾†å»ºç«‹å¤šæ¨£åŒ–çš„è³‡æ–™é›†ã€‚
    * ç°¡å–®ä»»å‹™çš„ç¯„ä¾‹1: [How To Create Datasets for Finetuning From Multiple Sources! Improving Finetunes With Embeddings.](https://youtu.be/fYyZiRi6yNE?si=mYgoyAeCMkkKQUr1)   
    * ç°¡å–®ä»»å‹™çš„ç¯„ä¾‹2: [How I created an instruction dataset using GPT 3.5 to fine-tune Llama 2 for news classification](https://medium.com/@kshitiz.sahay26/how-i-created-an-instruction-dataset-using-gpt-3-5-to-fine-tune-llama-2-for-news-classification-ed02fe41c81f)
* **é€²éšæŠ€å·§**: äº†è§£å¦‚ä½•ä½¿ç”¨[Evol-Instruct](https://arxiv.org/abs/2304.12244)æ”¹é€²ç¾æœ‰è³‡æ–™é›†ï¼Œå¦‚ä½•ç”¢ç”Ÿå’Œ[Orca](https://arxiv.org/abs/2306.02707) å’Œ [phi-1](https://arxiv.org/abs/2306.11644) è«–æ–‡ä¸­é¡ä¼¼çš„é«˜å“è³ªåˆæˆè³‡æ–™.
* **è³‡æ–™éæ¿¾**: æ¶‰åŠæ­£è¦è¡¨ç¤ºå¼ã€åˆªé™¤è¿‘ä¼¼é‡è¤‡é …ã€é—œæ³¨å…·æœ‰å¤§é‡æ¨™è¨˜çš„ç­”æ¡ˆç­‰çš„å‚³çµ±æŠ€å·§.
* **æç¤ºè©æ¨¡æ¿**: ç›®å‰é‚„æ²’æœ‰çœŸæ­£çš„æ¨™æº–æ–¹æ³•ä¾†æ ¼å¼ä¾†æ¨™æº–åŒ–èªªæ˜ç¯„æœ¬å’Œç­”æ¡ˆï¼Œé€™å°±æ˜¯ç‚ºä»€éº¼äº†è§£ä¸åŒçš„èŠå¤©ç¯„æœ¬å¾ˆé‡è¦, åƒæ˜¯ [ChatML](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/chatgpt?tabs=python&pivots=programming-language-chat-ml), [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) ç­‰.

ğŸ“š **åƒè€ƒè³‡æ–™**:
* [Preparing a Dataset for Instruction tuning](https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-1-Preparing-a-Dataset-for-Instruction-Tuning--Vmlldzo1NTcxNzE2) by Thomas Capelle: Alpaca å’Œ Alpaca-GPT4 è³‡æ–™é›†çš„æ¢ç´¢ä»¥åŠå¦‚ä½•æ¨™æº–åŒ–è³‡æ–™.
* [Generating a Clinical Instruction Dataset](https://medium.com/mlearning-ai/generating-a-clinical-instruction-dataset-in-portuguese-with-langchain-and-gpt-4-6ee9abfa41ae) by Solano Todeschini: æœ‰é—œå¦‚ä½•ä½¿ç”¨ GPT-4 å»ºç«‹ç¶œåˆæŒ‡å°è³‡æ–™é›†çš„æ•™å­¸. 
* [GPT 3.5 for news classification](https://medium.com/@kshitiz.sahay26/how-i-created-an-instruction-dataset-using-gpt-3-5-to-fine-tune-llama-2-for-news-classification-ed02fe41c81f) by Kshitiz Sahay: ä½¿ç”¨ GPT 3.5 å»ºç«‹ç¯„ä¾‹è³‡æ–™é›†ä¾†å¾®èª¿ Llama 2 çš„æ–°èåˆ†é¡.
* [Dataset creation for fine-tuning LLM](https://colab.research.google.com/drive/1GH8PW9-zAe4cXEZyOIE-T9uHXblIldAg?usp=sharing): åŒ…å«ä¸€äº›éæ¿¾è³‡æ–™é›†å’Œä¸Šå‚³çµæœæŠ€è¡“çš„ Notebook .
* [Chat Template](https://huggingface.co/blog/chat-templates) by Matthew Carrigan: é—œæ–¼æç¤ºæ¨¡æ¿çš„ Hugging Face é é¢

---
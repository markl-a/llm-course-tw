2. ğŸ§‘â€ğŸ”¬ **LLM æ¨¡å‹å·¥ç¨‹** å°ˆæ³¨åœ¨ä½¿ç”¨æœ€æ–°çš„æŠ€å·§,æŠ€è¡“æ­å»ºç¾éšæ®µå€‹äººå¯å¯¦ç¾æœ€å¥½çš„LLMs.

## ğŸ“ Notebooks

èˆ‡å¤§å‹èªè¨€æ¨¡å‹ç›¸é—œçš„ colab notebook å’Œæ–‡ç« æ¸…å–®ã€‚

### Tools

| åç¨± | æ•˜è¿° | é€£çµ |
|----------|-------------|----------|
| ğŸ§ [LLM AutoEval](https://github.com/mlabonne/llm-autoeval) | ä½¿ç”¨ RunPod è‡ªå‹•è©•ä¼°æ‚¨çš„LLMs | <a href="https://colab.research.google.com/drive/1Igs3WZuXAIv9X0vwqiE90QlEPys8e8Oa?usp=sharing"><img src="../img/colab.svg" alt="Open In Colab"></a> |
| ğŸ¥± LazyMergekit | ä½¿ç”¨ mergekit ä¸€éµè¼•é¬†åˆä½µæ¨¡å‹. | <a href="https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing"><img src="../img/colab.svg" alt="Open In Colab"></a> |
| âš¡ AutoGGUF | ä¸€éµé‡åŒ– GGUF æ ¼å¼çš„ LLM. | <a href="https://colab.research.google.com/drive/1P646NEg33BZy4BfLDNpTz0V0lwIU3CHu?usp=sharing"><img src="../img/colab.svg" alt="Open In Colab"></a> |
| ğŸŒ³ Model Family Tree | å¯è¦–åŒ–åˆä½µæ¨¡å‹çš„æ¨¹ç‹€çµæ§‹åœ–. | <a href="https://colab.research.google.com/drive/1s2eQlolcI1VGgDhqWIANfkfKvcKrMyNr?usp=sharing"><img src="../img/colab.svg" alt="Open In Colab"></a> |

### Fine-tuning (å¾®èª¿)

| åç¨± | æ•˜è¿° | æ–‡ç«  | é€£çµ |
|---------------------------------------|-------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|
| Fine-tune Llama 2 in Google Colab | å¾®èª¿æ‚¨çš„ç¬¬ä¸€å€‹ Llama 2 æ¨¡å‹çš„é€æ­¥æŒ‡å—ã€‚ | [Article](https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html) | <a href="https://colab.research.google.com/drive/1PEQyJO1-f6j0S_XJ8DV50NkpzasXkrzd?usp=sharing"><img src="../img/colab.svg" alt="Open In Colab"></a> |
| Fine-tune LLMs with Axolotl | æœ€å…ˆé€²å¾®èª¿å·¥å…·çš„ç«¯åˆ°ç«¯æŒ‡å—ã€‚| [Article](https://mlabonne.github.io/blog/posts/A_Beginners_Guide_to_LLM_Finetuning.html) | <a href="https://colab.research.google.com/drive/1Xu0BrCB7IShwSWKVcfAfhehwjDrDMH5m?usp=sharing"><img src="../img/colab.svg" alt="Open In Colab"></a> |
| Fine-tune Mistral-7b with DPO | ä½¿ç”¨ DPO æå‡ç›£ç£å¾®èª¿æ¨¡å‹çš„æ€§èƒ½ | [Article](https://medium.com/towards-data-science/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac) | <a href="https://colab.research.google.com/drive/15iFBr1xWgztXvhrj5I9fBv20c7CFOPBE?usp=sharing"><img src="../img/colab.svg" alt="Open In Colab"></a> |

### Quantization(é‡åŒ–)

| åç¨± | æ•˜è¿° | æ–‡ç«  | é€£çµ |
|---------------------------------------|-------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|
| 1. Introduction to Quantization | ä½¿ç”¨ 8 ä½å…ƒé‡åŒ–çš„å¤§å‹èªè¨€æ¨¡å‹æœ€ä½³åŒ–ã€‚ | [Article](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html) | <a href="https://colab.research.google.com/drive/1DPr4mUQ92Cc-xf4GgAaB6dFcFnWIvqYi?usp=sharing"><img src="../img/colab.svg" alt="Open In Colab"></a> |
| 2. 4-bit Quantization using GPTQ | é‡åŒ–æ‚¨è‡ªå·±çš„é–‹æº LLM ä»¥åœ¨æ¶ˆè²»æ€§ç¡¬é«”ä¸Šé‹è¡Œå®ƒå€‘ã€‚ | [Article](https://mlabonne.github.io/blog/4bit_quantization/) | <a href="https://colab.research.google.com/drive/1lSvVDaRgqQp_mWK_jC9gydz6_-y6Aq4A?usp=sharing"><img src="../img/colab.svg" alt="Open In Colab"></a> |
| 3. Quantization with GGUF and llama.cpp | ä½¿ç”¨ llama.cpp é‡åŒ– Llama 2 æ¨¡å‹ä¸¦å°‡ GGUF ç‰ˆæœ¬ä¸Šå‚³åˆ° HF Hubã€‚ | [Article](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html) | <a href="https://colab.research.google.com/drive/1pL8k7m04mgE5jo2NrjGi8atB0j_37aDD?usp=sharing"><img src="../img/colab.svg" alt="Open In Colab"></a> |
| 4. ExLlamaV2: The Fastest Library to RunÂ LLMs | é‡åŒ–ä¸¦åŸ·è¡Œ EXL2 æ¨¡å‹ä¸¦å°‡å…¶ä¸Šå‚³è‡³ HF Hubã€‚| [Article](https://mlabonne.github.io/blog/posts/ExLlamaV2_The_Fastest_Library_to_Run%C2%A0LLMs.html) | <a href="https://colab.research.google.com/drive/1yrq4XBlxiA0fALtMoT2dwiACVc77PHou?usp=sharing"><img src="../img/colab.svg" alt="Open In Colab"></a> |

### å…¶ä»–

| åç¨± | æ•˜è¿° | æ–‡ç«  | é€£çµ |
|---------------------------------------|-------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|
| Decoding Strategies in Large Language Models | å¾æ³¢æŸæœå°‹(beam search)åˆ°æ ¸æ¡æ¨£(nucleus sampling)çš„æ–‡æœ¬ç”ŸæˆæŒ‡å—| [Article](https://mlabonne.github.io/blog/posts/2022-06-07-Decoding_strategies.html) | <a href="https://colab.research.google.com/drive/19CJlOS5lI29g-B3dziNn93Enez1yiHk2?usp=sharing"><img src="../img/colab.svg" alt="Open In Colab"></a> |
| Visualizing GPT-2's Loss Landscape | åŸºæ–¼æ¬Šé‡æ“¾å‹•çš„æå¤±æ™¯è§€ä¸‰ç¶­åœ–(3D plot of the loss landscape based on weight perturbations.)| [Tweet](https://twitter.com/maximelabonne/status/1667618081844219904) | <a href="https://colab.research.google.com/drive/1Fu1jikJzFxnSPzR_V2JJyDVWWJNXssaL?usp=sharing"><img src="../img/colab.svg" alt="Open In Colab"></a> |
| Improve ChatGPT with Knowledge Graphs | ç”¨çŸ¥è­˜åœ–è­œå¢å¼· ChatGPT çš„ç­”æ¡ˆ | [Article](https://mlabonne.github.io/blog/posts/Article_Improve_ChatGPT_with_Knowledge_Graphs.html) | <a href="https://colab.research.google.com/drive/1mwhOSw9Y9bgEaIFKT4CLi0n18pXRM4cj?usp=sharing"><img src="../img/colab.svg" alt="Open In Colab"></a> |
| Merge LLMs with mergekit | è¼•é¬†å‰µå»ºæ‚¨è‡ªå·±çš„æ¨¡å‹ï¼Œç„¡éœ€ GPUï¼| [Article](https://towardsdatascience.com/merge-large-language-models-with-mergekit-2118fb392b54) | <a href="https://colab.research.google.com/drive/1_JS7JKJAQozD48-LhYdegcuuZ2ddgXfr?usp=sharing"><img src="../img/colab.svg" alt="Open In Colab"></a> |


## ğŸ§‘â€ğŸ”¬ LLM æ¨¡å‹å·¥ç¨‹

æœ¬èª²ç¨‹çš„é€™ä¸€éƒ¨åˆ†é‡é»åœ¨æ–¼å­¸ç¿’å¦‚ä½•ä½¿ç”¨æœ€æ–°æŠ€è¡“ä¾†å»ºç«‹æœ€å¥½çš„ LLMsã€‚

![](../img/roadmap_scientist.png)

### 1. The LLM æ¶æ§‹

é›–ç„¶ä¸éœ€è¦æ·±å…¥äº†è§£ Transformer æ¶æ§‹ï¼Œä½†æ·±å…¥äº†è§£å…¶è¼¸å…¥ï¼ˆtokens ä»¤ç‰Œï¼‰å’Œè¼¸å‡ºï¼ˆlogitsï¼‰éå¸¸é‡è¦ã€‚æ™®é€šçš„æ³¨æ„åŠ›æ©Ÿåˆ¶æ˜¯å¦ä¸€å€‹éœ€è¦æŒæ¡çš„é—œéµçµ„æˆéƒ¨åˆ†ï¼Œç¨å¾Œæœƒä»‹ç´¹å®ƒçš„æ”¹é€²ç‰ˆæœ¬ã€‚

* **é«˜éšè¦–åœ– High-level view**: é‡æ–°å¯©è¦–ç·¨ç¢¼å™¨-è§£ç¢¼å™¨ Transformer æ¶æ§‹ï¼Œæ›´å…·é«”åœ°èªªï¼Œæ˜¯åƒ…è§£ç¢¼å™¨çš„ GPT æ¶æ§‹ï¼Œè©²æ¶æ§‹åœ¨æ¯å€‹æœ€è¿‘çš„ LLM ä¸­åŸºæœ¬éƒ½æœ‰ä½¿ç”¨ã€‚

  * [LLM Foundations](https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/llm-foundations/): å½±ç‰‡æ­é…æ•™æçš„è¬›è§£ã€‚

  * [Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY): å½±ç‰‡æ­é…æ•™æçš„è¬›è§£ã€‚

  * [LLM-from-scratch.ipynb](https://colab.research.google.com/gist/iamaziz/171170dce60d9cd07fab221507fd1d52): å¯¦éš›ç°¡å–®ç‰ˆçš„ç¨‹å¼ç¢¼ã€‚

  * [[1hr Talk] Intro to Large Language Models](https://youtu.be/zjkBMFhNj_g?si=VnNOE1gggtAhxTDn): å½±ç‰‡æ­é…æ•™æçš„è¬›è§£ã€‚


* **æ¨™è¨˜åŒ– Tokenization**: äº†è§£å¦‚ä½•å°‡åŸå§‹æ–‡å­—è³‡æ–™è½‰æ›ç‚ºæ¨¡å‹å¯ä»¥ç†è§£çš„æ ¼å¼ï¼Œé€™æ¶‰åŠå°‡æ–‡å­—æ‹†åˆ†ç‚ºæ¨™è¨˜ï¼ˆé€šå¸¸æ˜¯å–®å­—æˆ–å­å–®å­—ï¼‰ã€‚
    * [Let's build the GPT Tokenizer](https://youtu.be/zjkBMFhNj_g?si=VnNOE1gggtAhxTDn): å½±ç‰‡æ­é…æ•™æçš„è¬›è§£ã€‚

* **æ³¨æ„åŠ›æ©Ÿåˆ¶**: æŒæ¡æ³¨æ„åŠ›æ©Ÿåˆ¶èƒŒå¾Œçš„ç†è«–ï¼ŒåŒ…æ‹¬è‡ªè¨»æ„åŠ›å’Œç¸®æ”¾é»ç©æ³¨æ„åŠ›ï¼Œé€™ä½¿å¾—æ¨¡å‹åœ¨ç”¢ç”Ÿè¼¸å‡ºæ™‚èƒ½å¤ å°ˆæ³¨æ–¼è¼¸å…¥çš„ä¸åŒéƒ¨åˆ†ã€‚
    * [å‹•æ‰‹æ·±åº¦å­¸ç¿’-æ³¨æ„åŠ›æ©Ÿåˆ¶](https://zh.d2l.ai/chapter_attention-mechanisms/index.html) ( å…¶å¯¦ä¸Šé¢çš„å½±ç‰‡æœ‰è¦†è“‹åˆ° )
* **æ–‡å­—ç”Ÿæˆ**: äº†è§£æ¨¡å‹ç”¢ç”Ÿè¼¸å‡ºåºåˆ—çš„ä¸åŒæ–¹å¼ã€‚å¸¸è¦‹çš„ç­–ç•¥åŒ…æ‹¬è²ªå©ªè§£ç¢¼(greedy decoding)ã€æ³¢æŸæœå°‹(beam searc)ã€top-k æ¡æ¨£(top-k sampling)å’Œæ ¸æ¡æ¨£(nucleus sampling)ã€‚
    * [å¦‚ä½•ç”Ÿæˆæ–‡æœ¬: é€šè¿‡ Transformers ç”¨ä¸åŒçš„è§£ç æ–¹æ³•ç”Ÿæˆæ–‡æœ¬](https://huggingface.co/blog/zh/how-to-generate)

- [HuggingFaceçš„ Transformeræ•™å­¸](https://huggingface.co/docs/transformers/quicktour):å»ºè­°ä»¥å»ºç«‹æ‡‰ç”¨ç‚ºç›®æ¨™ï¼Œå¾å°å…¥,å¾®èª¿,éƒ¨ç½²æ•´å€‹æµç¨‹è·‘ä¸€éã€‚
- å²ä¸¹ä½›çš„Transformersèª²ï¼Œå¾æ¶æ§‹åˆ°æ‡‰ç”¨éƒ½æœ‰ [CS25: Transformers United V3](https://web.stanford.edu/class/cs25/)
- Youtube Transformers United ä¸Šèª²éŒ„å½±[Stanford CS25: V1 I Transformers United: DL Models that have revolutionized NLP, CV, RL](https://www.youtube.com/watch?v=P127jhj-8-Y&list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM)

ğŸ“š **åƒè€ƒè³‡æ–™**:
- [Building LLMs from Scratch](https://youtu.be/UU1WVnMk4E8?si=Vn1IbHE5p5LUQmKi) å¾é›¶é–‹å§‹ build LLMsã€‚
- [Transformer æ’åœ–](https://jalammar.github.io/illustrated-transformer/) by Jay Alammar: Transformer æ¨¡å‹çš„ç›´è§€è§£é‡‹ã€‚
- [GPT-2åœ–è§£](https://jalammar.github.io/illustrated-gpt2/) by Jay Alammar: æ­¤æ–‡æ¯”ä¸Šä¸€ç¯‡æ–‡ç« æƒ³å°æ›´é‡è¦äº›ï¼Œå®ƒå°ˆæ³¨æ–¼å’Œ Llama éå¸¸ç›¸ä¼¼çš„ GPT æ¶æ§‹ã€‚
- [LLM Visualization](https://bbycroft.net/llm) by Brendan Bycroft: ä»¥ 3D è¦–è¦ºåŒ–æ–¹å¼å‘ˆç¾ LLM å…§éƒ¨ç™¼ç”Ÿçš„æƒ…æ³ã€‚
* [nanoGPT](https://www.youtube.com/watch?v=kCc8FmEb1nY) by Andrej Karpathy:ä¸€æ®µ 2 å°æ™‚é•·çš„ YouTube å½±ç‰‡ï¼Œç”¨æ–¼å¾é ­é–‹å§‹é‡æ–°å¯¦ç¾ GPTï¼ˆä»¥ç¨‹å¼è¨­è¨ˆå¸«çš„è¦–è§’ï¼‰ã€‚
* [Attention? Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/) by Lilian Weng: ä»¥æ›´æ­£å¼çš„æ–¹å¼ä»‹ç´¹æ³¨æ„åŠ›çš„å¿…è¦æ€§ã€‚
* [Decoding Strategies in LLMs](https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html): æä¾›å°ç”Ÿæˆæ–‡æœ¬çš„ä¸åŒè§£ç¢¼ç­–ç•¥çš„åœ–åƒåŒ–ä»‹ç´¹ä»¥åŠç¨‹å¼ç¢¼ã€‚


---
### 2. æ§‹å»ºä¸€å€‹ç¯„ä¾‹(æˆ–æŒ‡ä»¤)è³‡æ–™é›† instruction dataset

é›–ç„¶å¾ç¶­åŸºç™¾ç§‘å’Œå…¶ä»–ç¶²ç«™æ‰¾åˆ°åŸå§‹è³‡æ–™å¾ˆå®¹æ˜“ï¼Œä½†åœ¨å¾ˆå¤šçš„ç’°å¢ƒä¸­æ”¶é›†æˆå°çš„æŒ‡ç¤º,ç¯„ä¾‹å’Œç­”æ¡ˆå»å¾ˆå›°é›£ã€‚èˆ‡å‚³çµ±æ©Ÿå™¨å­¸ç¿’ä¸€æ¨£ï¼Œè³‡æ–™é›†çš„å“è³ªå°‡ç›´æ¥å½±éŸ¿æ¨¡å‹çš„å“è³ªï¼Œé€™å°±æ˜¯ç‚ºä»€éº¼å®ƒå¯èƒ½æ˜¯å¾®èª¿éç¨‹ä¸­æœ€é‡è¦çš„çµ„æˆéƒ¨åˆ†ã€‚

* **[é¡ä¼¼ Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html)-çš„è³‡æ–™é›†**: ä½¿ç”¨ OpenAI API (GPT) å¾é ­é–‹å§‹ç”¢ç”Ÿåˆæˆè³‡æ–™ã€‚æ‚¨å¯ä»¥æŒ‡å®šç¨®å­å’Œç³»çµ±æç¤ºä¾†å»ºç«‹å¤šæ¨£åŒ–çš„è³‡æ–™é›†ã€‚
    * ç°¡å–®ä»»å‹™çš„ç¯„ä¾‹1: [How To Create Datasets for Finetuning From Multiple Sources! Improving Finetunes With Embeddings.](https://youtu.be/fYyZiRi6yNE?si=mYgoyAeCMkkKQUr1)   
    * ç°¡å–®ä»»å‹™çš„ç¯„ä¾‹2: [How I created an instruction dataset using GPT 3.5 to fine-tune Llama 2 for news classification](https://medium.com/@kshitiz.sahay26/how-i-created-an-instruction-dataset-using-gpt-3-5-to-fine-tune-llama-2-for-news-classification-ed02fe41c81f)
* **é€²éšæŠ€å·§**: äº†è§£å¦‚ä½•ä½¿ç”¨[Evol-Instruct](https://arxiv.org/abs/2304.12244)æ”¹é€²ç¾æœ‰è³‡æ–™é›†ï¼Œå¦‚ä½•ç”¢ç”Ÿå’Œ[Orca](https://arxiv.org/abs/2306.02707) å’Œ [phi-1](https://arxiv.org/abs/2306.11644) è«–æ–‡ä¸­é¡ä¼¼çš„é«˜å“è³ªåˆæˆè³‡æ–™.
    * [Evol-Instructä¸­æ–‡æ¦‚ç•¥çš„è§£é‡‹](https://zhuanlan.zhihu.com/p/668755469)
* **è³‡æ–™éæ¿¾**: æ¶‰åŠæ­£è¦è¡¨ç¤ºå¼ã€åˆªé™¤è¿‘ä¼¼é‡è¤‡é …ã€é—œæ³¨å…·æœ‰å¤§é‡æ¨™è¨˜çš„ç­”æ¡ˆç­‰çš„å‚³çµ±æŠ€å·§.
    * ç›¸é—œå…§å®¹å¦‚åƒè€ƒè³‡æ–™
* **æç¤ºè©æ¨¡æ¿**: ç›®å‰é‚„æ²’æœ‰çœŸæ­£çš„æ¨™æº–æ–¹æ³•ä¾†æ ¼å¼ä¾†æ¨™æº–åŒ–èªªæ˜ç¯„æœ¬å’Œç­”æ¡ˆï¼Œé€™å°±æ˜¯ç‚ºä»€éº¼äº†è§£ä¸åŒçš„èŠå¤©ç¯„æœ¬å¾ˆé‡è¦, åƒæ˜¯ [ChatML](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/chatgpt?tabs=python&pivots=programming-language-chat-ml), [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) ç­‰.
    * ç›¸é—œå…§å®¹å¦‚åƒè€ƒè³‡æ–™

ğŸ“š **åƒè€ƒè³‡æ–™**:
* [Preparing a Dataset for Instruction tuning](https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-1-Preparing-a-Dataset-for-Instruction-Tuning--Vmlldzo1NTcxNzE2) by Thomas Capelle: Alpaca å’Œ Alpaca-GPT4 è³‡æ–™é›†çš„æ¢ç´¢ä»¥åŠå¦‚ä½•æ¨™æº–åŒ–è³‡æ–™.
* [Generating a Clinical Instruction Dataset](https://medium.com/mlearning-ai/generating-a-clinical-instruction-dataset-in-portuguese-with-langchain-and-gpt-4-6ee9abfa41ae) by Solano Todeschini: æœ‰é—œå¦‚ä½•ä½¿ç”¨ GPT-4 å»ºç«‹ç¶œåˆæŒ‡å°è³‡æ–™é›†çš„æ•™å­¸. 
* [GPT 3.5 for news classification](https://medium.com/@kshitiz.sahay26/how-i-created-an-instruction-dataset-using-gpt-3-5-to-fine-tune-llama-2-for-news-classification-ed02fe41c81f) by Kshitiz Sahay: ä½¿ç”¨ GPT 3.5 å»ºç«‹ç¯„ä¾‹è³‡æ–™é›†ä¾†å¾®èª¿ Llama 2 çš„æ–°èåˆ†é¡.
* [Dataset creation for fine-tuning LLM](https://colab.research.google.com/drive/1GH8PW9-zAe4cXEZyOIE-T9uHXblIldAg?usp=sharing): åŒ…å«ä¸€äº›éæ¿¾è³‡æ–™é›†å’Œä¸Šå‚³çµæœæŠ€è¡“çš„ Notebook .
* [Chat Template](https://huggingface.co/blog/chat-templates) by Matthew Carrigan: é—œæ–¼æç¤ºæ¨¡æ¿çš„ Hugging Face é é¢

---
### 3. é è¨“ç·´æ¨¡å‹

é è¨“ç·´æ˜¯ä¸€å€‹éå¸¸æ¼«é•·ä¸”æˆæœ¬é«˜æ˜‚çš„éç¨‹ï¼Œé€™å°±æ˜¯ç‚ºä»€éº¼é€™ä¸æ˜¯æœ¬èª²ç¨‹çš„é‡é»ã€‚ä½†æ˜¯å°é è¨“ç·´æœŸé–“ç™¼ç”Ÿçš„æƒ…æ³æœ‰ä¸€å®šç¨‹åº¦çš„äº†è§£æ˜¯å¾ˆå¥½çš„ã€‚ç°¡å–®ä¾†èªªï¼Œäº†è§£å¯ä»¥ï¼Œå¯¦ç¾æ²’å››åƒè¬åˆ¥åšï¼Œç›®å‰å·²çŸ¥èŠ±æœ€å°‘éŒ¢è¨“ç·´çš„æ˜¯[10è¬ç¾å…ƒ](https://www.ithome.com.tw/news/158779)ï¼Œå¦å¤–ï¼Œ[æˆ‘å¾æ–°èå¾—çŸ¥é€šå¸¸çš„è¨“ç·´æˆæœ¬æ˜¯1200è¬ç¾å…ƒå·¦å³](https://tw.news.yahoo.com/chatgpt%E6%9C%89%E5%A4%9A%E7%87%92%E9%8C%A2-%E5%88%9D%E5%A7%8B%E6%99%B6%E7%89%87%E9%9C%808%E5%84%84-%E6%AC%A1%E8%A8%93%E7%B7%B4%E6%88%90%E6%9C%AC%E4%B8%8A%E7%9C%8B1200%E8%90%AC%E7%BE%8E%E5%85%83-014000496.html)ã€‚

* ç›®å‰è™Ÿç¨±å®Œå…¨é–‹æºçš„é è¨“ç·´æ¨¡å‹ï¼š
    * 1.[olmo](https://allenai.org/olmo):[è™Ÿç¨±çœŸæ­£é–‹æºï¼AI2é‡‹å‡ºOLMoèªè¨€æ¨¡å‹å’Œæ‰€æœ‰ç›¸é—œè³‡æ–™](https://www.ithome.com.tw/news/161199)
        * [å²ä¸Šé¦–ä¸ª100%å¼€æºå¤§æ¨¡å‹é‡ç£…ç™»åœºï¼Œç ´çºªå½•å…¬å¼€ä»£ç /æƒé‡/æ•°æ®é›†/è®­ç»ƒå…¨è¿‡ç¨‹ï¼ŒAMDéƒ½èƒ½è®­](https://36kr.com/p/2632336993616134)
    * 2.[LLM 360](https://www.llm360.ai/): é–‹æºå¤§èªè¨€æ¨¡å‹æ¡†æ¶ï¼ŒåŒ…å«è¨“ç·´å’Œè³‡æ–™æº–å‚™ä»£ç¢¼ã€è³‡æ–™ã€æŒ‡æ¨™å’Œæ¨¡å‹ã€‚
    * 3.[å…¨çƒé¦–å€‹å®Œå…¨é–‹æºçš„å¤§èªè¨€æ¨¡å‹Dollyï¼Œæ€§èƒ½å ªæ¯” GPT3.5ï¼](https://github.com/databrickslabs/dolly)

* **è³‡æ–™è™•ç†æµç¨‹**: é è¨“ç·´éœ€è¦é¾å¤§çš„è³‡æ–™é›† (ä¾‹å¦‚ï¼š [Llama 2](https://arxiv.org/abs/2307.09288) ä½¿ç”¨ 2 å…†å€‹tokensé€²è¡Œè¨“ç·´) ï¼Œéœ€è¦å°‡é€™äº›è³‡æ–™é›†éæ¿¾ã€æ¨™è¨˜åŒ–ä¸¦èˆ‡é å…ˆå®šç¾©çš„è©å½™é€²è¡Œæ•´ç†ã€‚
    * [LLMDataHub](https://github.com/Zjh-819/LLMDataHub) by Junhao Zhao: ç”¨æ–¼é è¨“ç·´ã€å¾®èª¿å’Œ RLHF çš„ç²¾é¸è³‡æ–™é›†æ¸…å–®ã€‚
    * [Training a causal language model from scratch](https://huggingface.co/learn/nlp-course/chapter7/6?fw=pt) by Hugging Face: ä½¿ç”¨ Transformers åº«å¾é ­é–‹å§‹é å…ˆè¨“ç·´ GPT-2 æ¨¡å‹ã€‚
    * [TinyLlama](https://github.com/jzhang38/TinyLlama) by Zhang et al.: å¯åœ¨æ­¤é …ç›®å¾ˆå¥½åœ°äº†è§£ Llama æ¨¡å‹æ˜¯å¦‚ä½•å¾é ­é–‹å§‹è¨“ç·´çš„ã€‚
    * [ä½¿ç”¨pytorchï¼Œç”¨æ­ç§¯æœ¨çš„æ–¹å¼å®ç°å®Œæ•´çš„Transformeræ¨¡å‹](https://zhuanlan.zhihu.com/p/682451065) 
    
* **å› æœèªè¨€å»ºæ¨¡(Causal language modeling)**: äº†è§£å› æœèªè¨€å»ºæ¨¡å’Œæ©ç¢¼èªè¨€å»ºæ¨¡(causal and masked language modeling)ä¹‹é–“çš„å€åˆ¥,ä»¥åŠæœ¬ä¾‹ä¸­ä½¿ç”¨çš„æå¤±å‡½æ•¸ã€‚æ›´å¤šé«˜æ•ˆç‡çš„é è¨“ç·´çŸ¥è­˜å¯å‰å¾€ [Megatron-LM](https://github.com/NVIDIA/Megatron-LM) æˆ–[gpt-neox](https://github.com/EleutherAI/gpt-neox)äº†è§£ã€‚
    * [Causal language modeling](https://huggingface.co/docs/transformers/tasks/language_modeling) by Hugging Face: è§£é‡‹å› æœèªè¨€å»ºæ¨¡å’Œå±è”½èªè¨€å»ºæ¨¡ä¹‹é–“çš„å·®ç•°ä»¥åŠå¦‚ä½•å¿«é€Ÿå¾®èª¿ DistilGPT-2 æ¨¡å‹ã€‚
* **ç¸®æ”¾çš„è¦å¾‹**: [ç¸®æ”¾çš„è¦å¾‹](https://arxiv.org/pdf/2001.08361.pdf) æ ¹æ“šæ¨¡å‹å¤§å°ã€è³‡æ–™é›†å¤§å°å’Œç”¨æ–¼è¨“ç·´çš„è¨ˆç®—é‡æè¿°é æœŸçš„æ¨¡å‹æ€§èƒ½ã€‚
    * [Chinchilla's wild implications](https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications) by nostalgebraist: è¨è«–ç¸®æ”¾å®šå¾‹ä¸¦è§£é‡‹å®ƒå€‘å°å¤§èªè¨€æ¨¡å‹çš„æ„ç¾©ã€‚
* **é«˜æ•ˆèƒ½é‹ç®—**: é€™æœ‰é»è¶…å‡ºäº†æœ¬æ–‡çš„ç¯„åœï¼Œä½†å¦‚æœæ‚¨æ‰“ç®—å¾é ­é–‹å§‹å‰µå»ºè‡ªå·±çš„LLMs(å¤§èªè¨€æ¨¡å‹)ï¼ˆç¡¬é«”ã€åˆ†æ•£å¼å·¥ä½œè² è¼‰ç­‰ï¼‰ï¼Œé‚£éº¼æ›´å¤šæœ‰é—œ HPC çš„çŸ¥è­˜æ˜¯å°ä½ è€Œè¨€æ˜¯å¿…è¦çš„ã€‚
  
ğŸ“š **åƒè€ƒè³‡æ–™**:
* [BLOOM](https://bigscience.notion.site/BLOOM-BigScience-176B-Model-ad073ca07cdf479398d5f95d88e218c4) by BigScience: æè¿°å¦‚ä½•å»ºç«‹ BLOOM æ¨¡å‹çš„ Notion é é¢ï¼Œå…¶ä¸­åŒ…å«å¤§é‡æœ‰é—œå·¥ç¨‹éƒ¨åˆ†å’Œé‡åˆ°å•é¡Œçš„æœ‰ç”¨è³‡è¨Šã€‚
* [OPT-175 Logbook](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/OPT175B_Logbook.pdf) by Meta: ç ”ç©¶æ—¥èªŒé¡¯ç¤ºå‡ºäº†ä»€éº¼éŒ¯çš„ä»¥åŠä»€éº¼æ˜¯æ­£ç¢ºçš„ã€‚å¦‚æœæ‚¨è¨ˆåŠƒé å…ˆè¨“ç·´éå¸¸å¤§çš„èªè¨€æ¨¡å‹ï¼ˆåœ¨æœ¬ä¾‹ä¸­ç‚º 175B åƒæ•¸ï¼‰ï¼Œå‰‡éå¸¸æœ‰ç”¨ã€‚

---
### 4. ç›£ç£å¾®èª¿ (Supervised Fine-Tuning)

é è¨“ç·´æ¨¡å‹åƒ…é‡å°ä¸‹ä¸€å€‹æ¨™è¨˜(next-token)é æ¸¬ä»»å‹™é€²è¡Œè¨“ç·´ï¼Œé€™å°±æ˜¯ç‚ºä»€éº¼å®ƒå€‘ä¸æ˜¯æœ‰ç”¨çš„åŠ©æ‰‹ã€‚SFT å…è¨±æ‚¨èª¿æ•´å®ƒå€‘ä»¥å›æ‡‰æŒ‡ä»¤ã€‚æ­¤å¤–ï¼Œå®ƒå…è¨±æ‚¨æ ¹æ“šä»»ä½•è³‡æ–™ï¼ˆç§äººè³‡æ–™ã€GPT-4 ç„¡æ³•çœ‹åˆ°çš„è³‡æ–™ç­‰ï¼‰å¾®èª¿æ‚¨çš„æ¨¡å‹ä¸¦ä½¿ç”¨å®ƒï¼Œè€Œç„¡éœ€æ”¯ä»˜ OpenAI ç­‰ API çš„è²»ç”¨ã€‚
* ç°¡ä»‹è·Ÿæ•™å­¸ï¼š
    * [ç”¨äººè©±è¬›è§£å¾®èª¿æŠ€è¡“](https://www.zhihu.com/zvideo/1723348624463994881)ï¼šè«‹åªå°ˆæ³¨æŠ€è¡“ã€‚
    * [Finetuning Large Language Models](https://www.deeplearning.ai/short-courses/finetuning-large-language-models/): Deeplearning.ai çš„å¾®èª¿æŠ€è¡“çŸ­èª²ç¨‹ï¼Œé©åˆå¿«é€Ÿå…¥é–€ã€‚
* **å…¨å¾®èª¿**: å…¨å¾®èª¿æ˜¯æŒ‡è¨“ç·´æ¨¡å‹ä¸­çš„æ‰€æœ‰åƒæ•¸ ( å°±æ˜¯æ¨¡å‹è¨“ç·´ï¼Œåªæ˜¯è³‡æ–™é‡ä¸å¤šï¼Œä¸¦ä¸”è³‡æ–™é€šå¸¸æ˜¯ç‰¹å®šä»»å‹™æˆ–å­é ˜åŸŸä¸Šçš„ )ã€‚é€™ä¸æ˜¯ä¸€ç¨®æœ‰æ•ˆçš„æŠ€è¡“ï¼Œä½†å®ƒæœƒç”¢ç”Ÿç¨å¾®å¥½ä¸€é»çš„çµæœ.
    * [The Novice's LLM Training Guide](https://rentry.org/llm-training) by Alpin: æ¦‚è¿°å¾®èª¿ LLM æ™‚è¦è€ƒæ…®çš„ä¸»è¦æ¦‚å¿µå’Œåƒæ•¸.
* [**LoRA**](https://arxiv.org/abs/2106.09685): ä¸€ç¨®åŸºæ–¼ä½éšé©é…å™¨(low-rank adapters)çš„é«˜æ•ˆåƒæ•¸å¾®èª¿æŠ€è¡“ï¼ˆPEFTï¼‰ã€‚æˆ‘å€‘ä¸è¨“ç·´æ‰€æœ‰åƒæ•¸ï¼Œè€Œæ˜¯åªè¨“ç·´é€™äº›é©é…å™¨(adapters)ã€‚
    * [LoRA insights](https://lightning.ai/pages/community/lora-insights/) by Sebastian Raschka: æœ‰é—œ LoRA ä»¥åŠå¦‚ä½•é¸æ“‡æœ€ä½³åƒæ•¸çš„å¯¦ç”¨è¦‹è§£.
    * [Fine-Tune Your Own Llama 2 Model](https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html): æœ‰é—œå¦‚ä½•ä½¿ç”¨ Hugging Face åº«å¾®èª¿ Llama 2 æ¨¡å‹çš„å¯¦ä½œæ•™å­¸.
    * [Padding Large Language Models](https://towardsdatascience.com/padding-large-language-models-examples-with-llama-2-199fb10df8ff) by Benjamin Marie: ç‚ºå› æœLLMs(causal LLMs)å¡«å……è¨“ç·´ç¯„ä¾‹çš„æœ€ä½³å¯¦è¸ 
* [**QLoRA**](https://arxiv.org/abs/2305.14314): å¦ä¸€å€‹åŸºæ–¼ LoRA çš„ PEFTï¼Œå®ƒé‚„å°‡æ¨¡å‹çš„æ¬Šé‡é‡åŒ–ç‚º 4 bitsï¼Œä¸¦å¼•å…¥åˆ†é å„ªåŒ–å™¨ä¾†ç®¡ç†è¨˜æ†¶é«”å³°å€¼ã€‚å°‡å…¶èˆ‡[Unsloth](https://github.com/unslothai/unsloth)çµåˆä½¿ç”¨ï¼Œå¯ä»¥åœ¨å…è²»çš„ Colab ç­†è¨˜æœ¬ä¸Šé‹è¡Œã€‚
* **[Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)**: ä¸€ç¨®ç”¨æˆ¶å‹å¥½ä¸”åŠŸèƒ½å¼·å¤§çš„å¾®èª¿å·¥å…·ï¼Œç”¨æ–¼è¨±å¤šæœ€å…ˆé€²çš„é–‹æºæ¨¡å‹ã€‚
    * [A Beginner's Guide to LLM Fine-Tuning](https://mlabonne.github.io/blog/posts/A_Beginners_Guide_to_LLM_Finetuning.html): æœ‰é—œå¦‚ä½•ä½¿ç”¨ Axolotl å¾®èª¿ CodeLlama æ¨¡å‹çš„æ•™å­¸.
* [**DeepSpeed**](https://www.deepspeed.ai/): é‡å°å¤š GPU å’Œå¤šç¯€é»è¨­å®šçš„ LLM çš„é«˜æ•ˆé è¨“ç·´å’Œå¾®èª¿ï¼ˆåœ¨ Axolotl ä¸­å¯¦ç¾ï¼‰ã€‚
    
ğŸ“š **åƒè€ƒè³‡æ–™**:
* [ä¸‡å­—é•¿æ–‡ä¹‹æç¤ºå­¦ä¹ å’Œå¾®è°ƒå¤§æ¨¡å‹ï¼ˆPrompt Learning & Prompt Tuningï¼‰](https://zhuanlan.zhihu.com/p/670039833)
* [å¤§æ¨¡å‹å¾®è°ƒæ€»ç»“](https://www.zhihu.com/tardis/zm/art/627642632?source_id=1003)
* [Finetuning Large Language Models çš„èª²ç¨‹ç­†è¨˜](https://hackmd.io/@YungHuiHsu/HJ6AT8XG6)

---
### 5. Reinforcement Learning from Human Feedback RLHF (æ ¹æ“šäººé¡å›é¥‹é€²è¡Œå¼·åŒ–å­¸ç¿’)

ç¶“éç›£ç£å¾®èª¿å¾Œï¼ŒRLHF æ˜¯ç”¨ä¾†ä½¿ LLM çš„ç­”æ¡ˆèˆ‡äººé¡æœŸæœ›ä¿æŒä¸€è‡´çš„ä¸€å€‹æ­¥é©Ÿã€‚é€™å€‹æƒ³æ³•æ˜¯å¾äººé¡ï¼ˆæˆ–äººå·¥ï¼‰å›é¥‹ä¸­å­¸ç¿’åå¥½ï¼Œé€™å¯ç”¨æ–¼æ¸›å°‘åè¦‹ã€å¯©æŸ¥æ¨¡å‹æˆ–ä½¿å®ƒå€‘ä»¥æ›´æœ‰ç”¨çš„æ–¹å¼è¡Œäº‹ã€‚å®ƒæ¯” SFT æ›´è¤‡é›œï¼Œä¸¦ä¸”é€šå¸¸è¢«è¦–ç‚ºå¯é¸é …ä¹‹ä¸€ã€‚
* ç°¡ä»‹èˆ‡å…¥é–€ï¼š
    * [An Introduction to Training LLMs using RLHF](https://wandb.ai/ayush-thakur/Intro-RLAIF/reports/An-Introduction-to-Training-LLMs-Using-Reinforcement-Learning-From-Human-Feedback-RLHF---VmlldzozMzYyNjcy) by Ayush Thakur: é€™è§£é‡‹äº†ç‚ºä»€éº¼ RLHF å°æ–¼æ¸›å°‘å¤§èªè¨€æ¨¡å‹çš„åè¦‹å’Œæé«˜ç¸¾æ•ˆæ˜¯å¯å–çš„ã€‚
    * [Illustration RLHF](https://huggingface.co/blog/rlhf) by Hugging Face: RLHF ç°¡ä»‹ï¼ŒåŒ…æ‹¬çå‹µæ¨¡å‹è¨“ç·´å’Œå¼·åŒ–å­¸ç¿’å¾®èª¿.
    * [RLHF from Deeplearning.ai](https://www.deeplearning.ai/short-courses/finetuning-large-language-models/):  Deeplearning.ai çš„RLHFçŸ­èª²ç¨‹ï¼Œé©åˆå¿«é€Ÿå…¥é–€ã€‚

* **åå¥½è³‡æ–™é›†**: é€™äº›è³‡æ–™é›†é€šå¸¸åŒ…å«å…·æœ‰æŸç¨®æ’åçš„å¤šå€‹ç­”æ¡ˆï¼Œé€™ä½¿å¾—å®ƒå€‘æ¯”æŒ‡ä»¤è³‡æ–™é›†æ›´é›£ç”¢ç”Ÿ.
    * [StackLLaMA](https://huggingface.co/blog/stackllama) by Hugging Face: ä½¿ç”¨ Transformer å‡½å¼åº«æœ‰æ•ˆåœ°å°‡ LLaMA æ¨¡å‹èˆ‡ RLHF å°é½Šçš„æ•™å­¸.
* [**è¿‘ç«¯ç­–ç•¥æœ€ä½³åŒ–**](https://arxiv.org/abs/1707.06347): æ­¤æ¼”ç®—æ³•åˆ©ç”¨çå‹µæ¨¡å‹ä¾†é æ¸¬çµ¦å®šæ–‡å­—æ˜¯å¦è¢«äººé¡æ’åè¼ƒé«˜ã€‚ç„¶å¾Œä½¿ç”¨è©²é æ¸¬ä¾†æœ€ä½³åŒ– SFT æ¨¡å‹ï¼Œä¸¦æ ¹æ“š KL æ•£åº¦é€²è¡Œçæ‡²ã€‚
* **[ç›´æ¥åå¥½å„ªåŒ–](https://arxiv.org/abs/2305.18290)**: DPO é€éå°‡å…¶é‡æ–°å®šç¾©ç‚ºåˆ†é¡å•é¡Œä¾†ç°¡åŒ–æµç¨‹ã€‚å®ƒä½¿ç”¨åƒè€ƒæ¨¡å‹è€Œä¸æ˜¯çå‹µæ¨¡å‹ï¼ˆç„¡éœ€è¨“ç·´ï¼‰ï¼Œä¸¦ä¸”åªéœ€è¦ä¸€å€‹è¶…åƒæ•¸ï¼Œä½¿å…¶æ›´åŠ ç©©å®šå’Œé«˜æ•ˆã€‚

ğŸ“š **åƒè€ƒè³‡æ–™**:
* [LLM Training: RLHF and Its Alternatives](https://substack.com/profile/27393275-sebastian-raschka-phd) by Sebastian Rashcka: RLHF æµç¨‹å’Œ RLAIF ç­‰æ›¿ä»£æ–¹æ¡ˆçš„æ¦‚è¿°.
* [Fine-tune Mistral-7b with DPO](https://huggingface.co/blog/dpo-trl):ä½¿ç”¨ DPO å¾®èª¿ Mistral-7b æ¨¡å‹ä¸¦é‡ç¾[NeuralHermes-2.5](https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B) çš„æ•™å­¸.
* [[RL] Fine-Tuning Language Models from Human Preferences (RLHF) è«–æ–‡ç­†è¨˜-ChatGPTéŠæˆè¡“](https://hackmd.io/@YungHuiHsu/Sy5Ug7iV6)
* [è¯¦è§£å¤§æ¨¡å‹RLHFè¿‡ç¨‹ï¼ˆé…ä»£ç è§£è¯»ï¼‰](https://zhuanlan.zhihu.com/p/624589622)
* [LLMåŸºçŸ³ï¼šRLHFåŠå…¶æ›¿ä»£æŠ€æœ¯](https://zhuanlan.zhihu.com/p/682683518)
* [å›¾è§£å¤§æ¨¡å‹RLHFç³»åˆ—ä¹‹ï¼šäººäººéƒ½èƒ½çœ‹æ‡‚çš„PPOåŸç†ä¸æºç è§£è¯»](https://zhuanlan.zhihu.com/p/677607581)
* [RLAIFç»†èŠ‚åˆ†äº«&ä¸ªäººæƒ³æ³•](https://zhuanlan.zhihu.com/p/657436655)
---
### 6. è©•ä¼° Evaluation

è©•ä¼°å¤§å‹èªè¨€æ¨¡å‹(LLMs)æ˜¯æµç¨‹ä¸­ä¸€å€‹è¢«ä½ä¼°çš„éƒ¨åˆ†ï¼Œå› ç‚ºè©•ä¼°é€™ä¸€å€‹éç¨‹è€—æ™‚ä¸”ç›¸å°å¯é æ€§è¼ƒä½ã€‚ä½ çš„ä¸‹æ¸¸ä»»å‹™æ‡‰è©²æŒ‡æ˜ä½ æƒ³è¦è©•ä¼°çš„å…§å®¹ï¼Œä½†è¨˜å¾—å¤å¾·å“ˆç‰¹å®šå¾‹(Goodhart's law)æåˆ°çš„ï¼šâ€œç•¶ä¸€å€‹è¡¡é‡æŒ‡æ¨™è®Šæˆäº†ç›®æ¨™ï¼Œå®ƒå°±ä¸å†æ˜¯ä¸€å€‹å¥½çš„è¡¡é‡æŒ‡æ¨™ã€‚â€
* ç°¡ä»‹èˆ‡å…¥é–€ï¼š
    * [Evaluating and Debugging Generative AI Models Using Weights and Biases](https://www.deeplearning.ai/short-courses/evaluating-debugging-generative-ai//): Deeplearning AI çš„çŸ­èª²ç¨‹ï¼Œé©åˆå¿«é€Ÿå…¥é–€ã€‚
    * [LLM Evaluation å¦‚ä½•è¯„ä¼°ä¸€ä¸ªå¤§æ¨¡å‹ï¼Ÿ](https://zhuanlan.zhihu.com/p/644373658): åˆ¥äººçš„å¿ƒå¾—ï¼Œå¯ä»¥åƒè€ƒä¸‹ã€‚

* **å‚³çµ±æŒ‡æ¨™ Traditional metrics**: åƒå›°æƒ‘åº¦(perplexity)å’ŒBLEUåˆ†æ•¸é€™æ¨£çš„æŒ‡æ¨™ä¸å†åƒä»¥å‰é‚£æ¨£å—æ­¡è¿ï¼Œå› ç‚ºåœ¨å¤§å¤šæ•¸æƒ…æ³ä¸‹å®ƒå€‘æ˜¯æœ‰ç¼ºé™·çš„ã€‚ä½†äº†è§£å®ƒå€‘ä»¥åŠå®ƒå€‘é©ç”¨çš„æƒ…å¢ƒä»ç„¶å¾ˆé‡è¦ã€‚
    * [å›ºå®šé•·åº¦è¼¸å…¥(æœ‰æœ€å¤§è¼¸å…¥é™åˆ¶)æ¨¡å‹çš„å›°æƒ‘åº¦](https://huggingface.co/docs/transformers/perplexity) by Hugging Face: å›°æƒ‘åº¦(perplexity)çš„æ¦‚è¿°ï¼Œä¸¦ä½¿ç”¨ Transformer åº«å¯¦ç¾äº†å®ƒçš„ç¨‹å¼ç¢¼ã€‚
    * [BLEU ä½¿ç”¨é¢¨éšª](https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213) by Rachael Tatman: BLEU åˆ†æ•¸åŠå…¶è¨±å¤šå•é¡Œçš„æ¦‚è¿°ï¼Œä¸¦æä¾›äº†ç¤ºä¾‹ã€‚
* **é€šç”¨åŸºæº– General benchmarks**: åŸºæ–¼èªè¨€æ¨¡å‹è©•ä¼°å·¥å…·ç®± [Language Model Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness)ï¼ŒOpen LLMæ’è¡Œæ¦œ [Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) æ˜¯ç”¨æ–¼é€šç”¨å¤§å‹èªè¨€æ¨¡å‹ï¼ˆå¦‚ChatGPTï¼‰çš„ä¸»è¦åŸºæº–ã€‚é‚„æœ‰å…¶ä»–å—æ­¡è¿çš„åŸºæº–ï¼Œå¦‚[BigBench](https://github.com/google/BIG-bench), [MT-Bench](https://arxiv.org/abs/2306.05685)ç­‰ã€‚
    * [å¤§å‹èªè¨€æ¨¡å‹è©•ä¼°èª¿æŸ¥](https://arxiv.org/abs/2307.03109) by Chang et al.: é—œæ–¼è©•ä¼°ä»€éº¼ã€åœ¨å“ªè£¡è©•ä¼°ä»¥åŠå¦‚ä½•è©•ä¼°çš„ç¶œåˆæ€§è«–æ–‡ã€‚
* **ä»»å‹™ç‰¹å®šåŸºæº– Task-specific benchmarks**: å¦‚æ‘˜è¦ã€ç¿»è­¯å’Œå•ç­”ç­‰ä»»å‹™æœ‰å°ˆé–€çš„åŸºæº–ã€æŒ‡æ¨™ç”šè‡³å­é ˜åŸŸï¼ˆé†«ç™‚ã€é‡‘èç­‰ï¼‰ï¼Œä¾‹å¦‚ç”¨æ–¼ç”Ÿç‰©é†«å­¸å•ç­” [PubMedQA](https://pubmedqa.github.io/)ã€‚
* **äººé¡è©•ä¼° Human evaluation**: æœ€å¯é çš„è©•ä¼°æ˜¯ç”¨æˆ¶çš„æ¥å—åº¦æˆ–ç”±äººé¡æ‰€åšçš„æ¯”è¼ƒã€‚å¦‚æœä½ æƒ³çŸ¥é“ä¸€å€‹æ¨¡å‹è¡¨ç¾å¾—å¦‚ä½•ï¼Œæœ€ç°¡å–®ä½†æœ€ç¢ºå®šçš„æ–¹å¼å°±æ˜¯è‡ªå·±ä½¿ç”¨å®ƒã€‚

ğŸ“š **åƒè€ƒæ–‡ç»**:
* [èŠå¤©æ©Ÿå™¨äººæ’è¡Œæ¦œ](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard) by lmsys: åŸºæ–¼äººé¡æ¯”è¼ƒçš„é€šç”¨å¤§å‹èªè¨€æ¨¡å‹çš„Eloè©•åˆ†ã€‚

---
### 7. é‡åŒ–

é‡åŒ–æ˜¯å°‡æ¨¡å‹çš„æ¬Šé‡ï¼ˆå’Œå•Ÿå‹•å€¼ï¼‰è½‰æ›æˆæ›´ä½ç²¾åº¦è¡¨ç¤ºçš„éç¨‹ã€‚ä¾‹å¦‚ï¼ŒåŸæœ¬ä½¿ç”¨16ä½å…ƒå„²å­˜çš„æ¬Šé‡å¯ä»¥è½‰æ›æˆ4ä½å…ƒè¡¨ç¤ºã€‚é€™ç¨®æŠ€è¡“æ„ˆä¾†æ„ˆé‡è¦ï¼Œç”¨ä¾†æ¸›å°‘èˆ‡å¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç›¸é—œçš„è¨ˆç®—èˆ‡è¨˜æ†¶é«”æˆæœ¬ã€‚

* **ç°¡ä»‹**: 
    * [é‡åŒ–ç°¡ä»‹](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html): é‡åŒ–æ¦‚è¿°ï¼Œabsmaxèˆ‡é›¶é»é‡åŒ–ï¼Œä»¥åŠä½¿ç”¨ LLM.int8()åœ¨ç¨‹å¼ç¢¼ä¸Šã€‚
    * [ç›®å‰é’ˆå¯¹å¤§æ¨¡å‹è¿›è¡Œé‡åŒ–çš„æ–¹æ³•æœ‰å“ªäº›ï¼Ÿ](https://www.zhihu.com/question/627484732)
    * [å¤§è¯­è¨€æ¨¡å‹é‡åŒ–ç›¸å…³æŠ€æœ¯](https://zhuanlan.zhihu.com/p/664054739)

* **åŸºç¤æŠ€è¡“**: ç­è§£ä¸åŒçš„ç²¾ç¢ºåº¦å±¤ç´šï¼ˆFP32ã€FP16ã€INT8ç­‰ï¼‰ä»¥åŠå¦‚ä½•ä½¿ç”¨absmaxèˆ‡é›¶é»æŠ€è¡“(zero-point techniques)é€²è¡Œç°¡å–®çš„é‡åŒ–ã€‚
* **GGUFå’Œllama.cpp**: æœ€åˆè¨­è¨ˆç”¨æ–¼åœ¨CPUä¸Šé‹è¡Œï¼Œ[llama.cpp](https://github.com/ggerganov/llama.cpp) å’ŒGGUFæ ¼å¼å·²æˆç‚ºåœ¨æ¶ˆè²»ç´šç¡¬é«”ä¸Šé‹è¡ŒLLMsçš„æœ€å—æ­¡è¿çš„å·¥å…·ã€‚
    * [ä½¿ç”¨llama.cppé‡åŒ–Llamaæ¨¡å‹](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html): é—œæ–¼å¦‚ä½•ä½¿ç”¨llama.cppå’ŒGGUFæ ¼å¼é‡åŒ–Llama 2æ¨¡å‹çš„æ•™å­¸ã€‚
* **GPTQå’ŒEXL2**: [GPTQ](https://arxiv.org/abs/2210.17323) ï¼Œç‰¹åˆ¥æ˜¯ [EXL2](https://github.com/turboderp/exllamav2) ï¼Œæä¾›äº†è¼ƒå¿«çš„é€Ÿåº¦ï¼Œä½†åªèƒ½åœ¨GPUä¸Šé‹è¡Œã€‚æ¨¡å‹é‡åŒ–ä¹Ÿéœ€è¦å¾ˆé•·æ™‚é–“ã€‚
    * [ä½¿ç”¨GPTQé€²è¡Œ4ä½å…ƒLLMé‡åŒ–](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html):é—œæ–¼å¦‚ä½•ä½¿ç”¨GPTQæ¼”ç®—æ³•å’ŒAutoGPTQé‡åŒ–LLMçš„æ•™å­¸ã€‚
    * [ExLlamaV2: é‹è¡ŒLLMsçš„æœ€å¿«ç¨‹å¼åº«](https://mlabonne.github.io/blog/posts/ExLlamaV2_The_Fastest_Library_to_Run%C2%A0LLMs.html): æŒ‡å—ï¼›é—œæ–¼å¦‚ä½•ä½¿ç”¨EXL2æ ¼å¼é‡åŒ–Mistralæ¨¡å‹ï¼Œä¸¦ä½¿ç”¨ExLlamaV2ç¨‹å¼åº«é‹è¡Œã€‚
* **AWQ**: é€™ç¨®æ–°æ ¼å¼æ¯”GPTQæ›´æº–ç¢ºï¼ˆå›°æƒ‘åº¦æ›´ä½ï¼‰ï¼Œä½†ä½¿ç”¨çš„é¡¯å­˜æ›´å¤šï¼Œé€Ÿåº¦ä¹Ÿä¸ä¸€å®šæ›´å¿«ã€‚
    * [äº†è§£å•Ÿå‹•æ„ŸçŸ¥æ¬Šé‡é‡åŒ–](https://medium.com/friendliai/understanding-activation-aware-weight-quantization-awq-boosting-inference-serving-efficiency-in-10bb0faf63a8) by FriendliAI: AWQæŠ€è¡“åŠå…¶å„ªå‹¢çš„æ¦‚è¿°ã€‚

ğŸ“š **åƒè€ƒæ–‡ç»**:

* [LLM Note Day 14 - é‡åŒ– Quantization](https://ithelp.ithome.com.tw/articles/10330372) :ç­†è¨˜å¼ç‰ˆæœ¬çš„é‡åŒ–ä»‹ç´¹ã€‚
    
---
### 8. æ–°è¶¨å‹¢

* **ä½ç½®åµŒå…¥ Positional embeddings**: äº†è§£å¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¦‚ä½•ç·¨ç¢¼ä½ç½®ï¼Œç‰¹åˆ¥æ˜¯åƒ [RoPE](https://arxiv.org/abs/2104.09864) é€™æ¨£çš„ç›¸å°ä½ç½®ç·¨ç¢¼æ–¹æ¡ˆã€‚å¯¦ç¾ [YaRN](https://arxiv.org/abs/2309.00071) (é€šéæº«åº¦å› å­ä¹˜ä»¥æ³¨æ„åŠ›çŸ©é™£) or [ALiBi](https://arxiv.org/abs/2108.12409) (åŸºæ–¼tokenè·é›¢çš„æ³¨æ„åŠ›çæ‡²) ä¾†æ“´å±•ä¸Šä¸‹æ–‡é•·åº¦ã€‚
    * [Extending the RoPE](https://blog.eleuther.ai/yarn/) by EleutherAI: ç¸½çµä¸åŒä½ç½®ç·¨ç¢¼æŠ€è¡“çš„æ–‡ç« .
    * [Understanding YaRN](https://medium.com/@rcrajatchawla/understanding-yarn-extending-context-window-of-llms-3f21e3522465) by Rajat Chawla: å°YaRNçš„ä»‹ç´¹.
* **æ¨¡å‹èåˆ Model merging**: å°‡è¨“ç·´å¥½çš„æ¨¡å‹åˆä½µå·²æˆç‚ºä¸€ç¨®æµè¡Œçš„æ–¹å¼ï¼Œç”¨æ–¼å‰µå»ºè¡¨ç¾å„ªç•°çš„æ¨¡å‹ï¼Œç„¡éœ€ä»»ä½•å¾®èª¿ã€‚æµè¡Œçš„ [mergekit](https://github.com/cg123/mergekit) åº«å¯¦ç¾äº†æœ€å—æ­¡è¿çš„èåˆæ–¹æ³•ï¼Œå¦‚e SLERP, [DARE](https://arxiv.org/abs/2311.03099), å’Œ [TIES](https://arxiv.org/abs/2311.03099)ã€‚æ¨¡å‹èåˆé€šå¸¸æŒ‡çš„æ˜¯å°‡å¤šå€‹å·²è¨“ç·´çš„æ¨¡å‹åˆä½µæˆä¸€å€‹å–®ä¸€æ¨¡å‹çš„éç¨‹ã€‚é€™ä¸åƒ…åƒ…æ˜¯å¹³å‡æˆ–æŠ•ç¥¨æ±ºå®šè¼¸å‡ºï¼Œè€Œæ˜¯åœ¨æ¨¡å‹çš„æ¬Šé‡å’Œçµæ§‹å±¤é¢ä¸Šé€²è¡Œåˆä½µã€‚é€™å€‹éç¨‹ä¸éœ€è¦å†æ¬¡è¨“ç·´ï¼Œå¯ä»¥é€šéæ•¸å­¸æ“ä½œï¼ˆå¦‚çƒé¢ç·šæ€§å…§æ’ï¼ˆSLERPï¼‰æˆ–å…¶ä»–èåˆæŠ€è¡“ï¼‰å°‡ä¸åŒæ¨¡å‹çš„çŸ¥è­˜æ•´åˆèµ·ä¾†ã€‚æ¨¡å‹èåˆç”¨æ–¼å‰µå»ºä¸€å€‹è¡¨ç¾æ›´ä½³ã€æ›´å¼·å¤§çš„æ¨¡å‹ï¼Œé€šå¸¸æ˜¯å°‡å¤šå€‹æ¨¡å‹åœ¨ç‰¹å®šä»»å‹™ä¸Šçš„å„ªå‹¢çµåˆèµ·ä¾†ã€‚
    * [Merge LLMs with mergekit](https://mlabonne.github.io/blog/posts/2024-01-08_Merge_LLMs_with_mergekit.html): é—œæ–¼ä½¿ç”¨mergekité€²è¡Œæ¨¡å‹èåˆçš„æ•™ç¨‹.
* **å°ˆå®¶æ··åˆ Mixture of Experts**: [Mixtral](https://arxiv.org/abs/2401.04088) å› å…¶å“è¶Šçš„æ€§èƒ½è€Œé‡æ–°ä½¿MoEæ¶æ§‹æµè¡Œèµ·ä¾†ã€‚ èˆ‡æ­¤åŒæ™‚ï¼ŒOSSç¤¾å€å‡ºç¾äº†ä¸€ç¨®frankenMoEï¼Œé€šéèåˆåƒ [Phixtral](https://huggingface.co/mlabonne/phixtral-2x2_8)é€™æ¨£çš„æ¨¡å‹ï¼Œé€™æ˜¯ä¸€å€‹æ›´ç¶“æ¿Ÿä¸”æ€§èƒ½è‰¯å¥½çš„é¸é …ã€‚MoEæ˜¯ä¸€ç¨®çµæ§‹ï¼Œå®ƒåŒ…å«å¤šå€‹å­æ¨¡å‹æˆ–â€œå°ˆå®¶â€ï¼Œæ¯å€‹å°ˆå®¶å°ˆé–€è™•ç†ä¸åŒçš„ä»»å‹™æˆ–æ•¸æ“šå­é›†ã€‚åœ¨MoEæ¶æ§‹ä¸­ï¼Œä¸€å€‹â€œgateâ€æˆ–èª¿åº¦å™¨æ±ºå®šå°æ–¼çµ¦å®šçš„è¼¸å…¥ï¼Œå“ªå€‹å°ˆå®¶è¢«ä½¿ç”¨ã€‚é€™æ˜¯ä¸€ç¨®ç¨€ç–å•Ÿå‹•æ–¹æ³•ï¼Œå¯ä»¥å¤§å¹…æå‡æ¨¡å‹çš„å®¹é‡å’Œæ•ˆç‡ï¼Œå› ç‚ºä¸æ˜¯æ‰€æœ‰çš„å°ˆå®¶éƒ½æœƒå°æ¯å€‹è¼¸å…¥é€²è¡ŒéŸ¿æ‡‰ã€‚
    * [Mixture of Experts Explained](https://huggingface.co/blog/moe) by Hugging Face: é—œæ–¼MoEåŠå…¶å·¥ä½œæ–¹å¼çš„è©³ç›¡æŒ‡å—.
* **å¤šæ¨¡æ…‹æ¨¡å‹ Multimodal models**: é€™äº›æ¨¡å‹ï¼ˆ [CLIP](https://openai.com/research/clip), [Stable Diffusion](https://stability.ai/stable-image), æˆ– [LLaVA](https://llava-vl.github.io/)) è™•ç†å¤šç¨®é¡å‹çš„è¼¸å…¥ï¼ˆæ–‡æœ¬ã€åœ–åƒã€éŸ³é »ç­‰ï¼‰èˆ‡çµ±ä¸€çš„åµŒå…¥ç©ºé–“ï¼Œå¾è€Œè§£é–äº†å¼·å¤§çš„æ‡‰ç”¨ï¼Œå¦‚æ–‡æœ¬åˆ°åœ–åƒã€‚
    * [Large Multimodal Models](https://huyenchip.com/2023/10/10/multimodal.html) by Chip Huyen: å°å¤šæ¨¡æ…‹ç³»çµ±åŠå…¶è¿‘æœŸç™¼å±•æ­·å²çš„æ¦‚è¿°.

ğŸ“š **åƒè€ƒæ–‡ç»**:
 * [æ¨¡å‹èåˆã€æ··åˆä¸“å®¶ã€æ›´å°çš„LLMï¼Œå‡ ç¯‡è®ºæ–‡çœ‹æ‡‚2024å¹´LLMå‘å±•æ–¹å‘](https://www.jiqizhixin.com/articles/2024-02-22)ï¼šå¯ä»¥åƒè€ƒä¸‹ï¼Œå¯«å¾—ç®—å…¨é¢çš„ã€‚
 * [Ten Noteworthy AI Research Papers of 2023](https://magazine.sebastianraschka.com/p/10-ai-research-papers-2023?utm_source=profile&utm_medium=reader2): é€™ç¯‡æ–‡ç« å›é¡§äº†2023å¹´åç¯‡å€¼å¾—æ³¨æ„çš„AIç ”ç©¶è«–æ–‡ã€‚æ¶µè“‹äº†å¾å¤§è¦æ¨¡è¨“ç·´é‹è¡Œçš„æ´å¯Ÿã€é–‹æ”¾åŸºç¤å’Œå¾®èª¿èŠå¤©æ¨¡å‹åˆ°é‡åŒ–LLMçš„é«˜æ•ˆå¾®èª¿ç­‰å¤šå€‹æ–¹é¢ã€‚ä½œè€…å¼·èª¿äº†é€™äº›ç ”ç©¶åœ¨é€æ˜åº¦ã€æ–¹æ³•å‰µæ–°å’Œç‰¹å®šé ˜åŸŸæ‡‰ç”¨æ–¹é¢çš„é‡è¦æ€§ï¼Œä¸¦æœŸå¾…æœªä¾†çš„ç ”ç©¶èƒ½å¤ å¸¶ä¾†æ›´å¤šé¡ä¼¼çš„é«˜è³ªé‡è«–æ–‡ã€‚é€™ç¯‡è·Ÿä¸Šä¸€ç¯‡æ˜¯åŒä¸€å€‹ä½œè€…ã€‚
